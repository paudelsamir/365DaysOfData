# -*- coding: utf-8 -*-
"""rag_app_ucl_2026.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QLLbGinCEYax4C-2R99eBAIxgRnspnwX
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install langchain_community wikipedia

# Commented out IPython magic to ensure Python compatibility.
# %pip install langchain_huggingface

# Commented out IPython magic to ensure Python compatibility.
# %pip install llama_cpp

from langchain_community.llms import LlamaCpp

import langchain
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import WikipediaLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFaceHub
from dotenv import load_dotenv
from huggingface_hub import HfApi, InferenceClient
from langchain_huggingface.llms import HuggingFaceEndpoint
from langchain.prompts import PromptTemplate

from langchain.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# huggingface token
HF_TOKEN = "yours_one"

loader = WikipediaLoader("2025â€“26_UEFA_Champions_League")
data = loader.load()

len(data)

splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(data)
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
db = FAISS.from_documents(chunks, embeddings)

retriever = db.as_retriever(search_kwargs={"k": 2})

model_path = "/content/tiny-llama"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    torch_dtype="auto"
)

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=200)
llm = HuggingFacePipeline(pipeline=pipe)

prompt_template = """Answer the question **ONLY** using the following context.
Do not invent anything, do not ask additional questions.
Context:
{context}
Question: {question}
Answer:"""

template = PromptTemplate(
    input_variables=["context", "question"],
    template=prompt_template
)

qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    return_source_documents=False,
    chain_type_kwargs={"prompt": template}
)

query = "Which team is in Pot 1 this season?"
answer = qa.run(query)
print(answer)

query = "what's the ranking of England in association ranking"
answer = qa.run(query)
print(answer)