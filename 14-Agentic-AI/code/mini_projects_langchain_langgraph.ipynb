{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "410808d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/Github/365DaysOfData/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.tools import tool\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.tools import DuckDuckGoSearchResults, YouTubeSearchTool, AzureAiServicesDocumentIntelligenceTool, WikipediaQueryRun\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing import TypedDict\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6630e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! It's nice to meet you. I'm here to help answer any questions or chat with you about any topic that interests you. How's your day going so far?\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OllamaLLM(model= 'llama3.2')\n",
    "\n",
    "llm.invoke(\"Hello, Ollama!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05a1d05",
   "metadata": {},
   "source": [
    "# 1. contextual q&a with memory\n",
    "build a chatbot that not only answers questions about a dataset (like pdfs or notes) but also remembers past queries using langgraph’s state nodes.\n",
    "bonus: add a “forget” command that clears memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13b5f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "doc = PyPDFLoader(\"/home/sam/Github/365DaysOfData/14-Agentic-AI/code/extras/geeta.pdf\")\n",
    "docs = doc.load()\n",
    "parser = StrOutputParser()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ce715fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "vectorstore = Chroma.from_documents(docs, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d23dd3e",
   "metadata": {},
   "source": [
    "### Using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3ccbda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided text from the Bhagavad Gita, I can say that lust (or kama) is considered the greatest enemy of living entities, and it induces them to remain entangled in the material world. Lust is described as a mode of passion that, if not elevated to the mode of goodness through spiritual attachment, leads to degradation into wrath.\\n\\nLust is said to be the symbol of ignorance, keeping the living entity within the material world. When living entities indulge in lustful activities, they may experience temporary happiness, but ultimately, it is an enemy that hinders their spiritual growth and understanding of their real position.\\n\\nThe text also mentions that lust cannot be satisfied by sense enjoyment, just like fire cannot be extinguished by a constant supply of fuel. In the material world, sex is considered the center of all activities, and this leads to the shackles of sex life, which are similar to prison bars for those who disobey the laws of the Lord.\\n\\nIn essence, lust is seen as a fundamental flaw in human nature that prevents living entities from realizing their true purpose and connection with the Supreme Being. When transformed into love for the Supreme (Krsna consciousness), lust can be spiritualized, becoming a positive force rather than an enemy.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a Bhagavad Gita expert. answer the question based on the context provided. If the context does not help, say 'I don't know'\"),\n",
    "    (\"human\", \"query: {query} context: {context}\"),\n",
    "])\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough() \n",
    "    | (lambda query: {\"query\": query, \"context\": \"\\n\".join([doc.page_content for doc in vectorstore.similarity_search(query, k=3)])})\n",
    "    | chat_prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"What is lust, how lust destroy humans??\")\n",
    "\n",
    "\n",
    "# there's no memeory implemented yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9206b28c",
   "metadata": {},
   "source": [
    "### Using Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2423dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(state):\n",
    "    query = state[\"messages\"][-1].content if state[\"messages\"] else \"\"\n",
    "    docs = vectorstore.similarity_search(query, k=3)\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    return {\"context\": context}\n",
    "\n",
    "def response_generation(state):\n",
    "    context = state.get(\"context\", \"\")\n",
    "    query = state[\"messages\"][-1].content if state[\"messages\"] else \"\"\n",
    "\n",
    "    # Get the LLM's response\n",
    "    response = llm.invoke(chat_prompt.format_messages(query=query, context=context, chat_history=state[\"messages\"][:-1]))\n",
    "    \n",
    "    # Check the type of response and handle accordingly\n",
    "    if hasattr(response, 'content'):\n",
    "        response_text = response.content\n",
    "    else:\n",
    "        response_text = str(response)\n",
    "        \n",
    "    # Create and return AIMessage with the response text\n",
    "    return {\"messages\": state[\"messages\"] + [AIMessage(content=response_text)]}\n",
    "\n",
    "# Handle commands like 'forget'\n",
    "def handle_commands(state):\n",
    "    last_message = state[\"messages\"][-1].content.lower()\n",
    "    \n",
    "    if last_message.startswith(\"/forget\"):\n",
    "        # Clear the history except for this command\n",
    "        return {\"messages\": [state[\"messages\"][-1]], \"command_executed\": True}\n",
    "    return {\"command_executed\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cd1dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete implementation example\n",
    "class State(TypedDict):\n",
    "    messages: list\n",
    "    context: str\n",
    "    command_executed: bool\n",
    "\n",
    "graph = StateGraph(State)\n",
    "\n",
    "# Add all nodes with consistent names\n",
    "graph.add_node(\"retrieve\", retrieve_context)\n",
    "graph.add_node(\"command_handler\", handle_commands)\n",
    "graph.add_node(\"respond\", response_generation)\n",
    "\n",
    "# Connect START to first node\n",
    "graph.add_edge(START, \"command_handler\")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"command_handler\",\n",
    "    lambda state: state[\"command_executed\"],\n",
    "    {False: \"retrieve\", True: END}\n",
    ")\n",
    "\n",
    "graph.add_edge(\"retrieve\", \"respond\")\n",
    "graph.add_edge(\"respond\", END)\n",
    "\n",
    "workflow = graph.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02cce5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with Bhagavad Gita Expert (type '/forget' to clear history, '/exit' to quit)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mChat with Bhagavad Gita Expert (type \u001b[39m\u001b[33m'\u001b[39m\u001b[33m/forget\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to clear history, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m/exit\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to quit)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     user_input = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mYou: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m user_input.lower() == \u001b[33m\"\u001b[39m\u001b[33m/exit\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      9\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/365DaysOfData/venv/lib/python3.13/site-packages/ipykernel/kernelbase.py:1275\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1273\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/365DaysOfData/venv/lib/python3.13/site-packages/ipykernel/kernelbase.py:1320\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1318\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1319\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "state = {\"messages\": [], \"context\": \"\", \"command_executed\": False}\n",
    "\n",
    "print(\"Chat with Bhagavad Gita Expert (type '/forget' to clear history, '/exit' to quit)\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nYou: \")\n",
    "    \n",
    "    if user_input.lower() == \"/exit\":\n",
    "        break\n",
    "        \n",
    "    state[\"messages\"] = add_messages(state[\"messages\"], [HumanMessage(content=user_input)])\n",
    "    \n",
    "    state = workflow.invoke(\n",
    "        state,\n",
    "        config={\n",
    "            \"configurable\": {\n",
    "                \"thread_id\": \"user_conversation_1\",  \n",
    "                \"checkpoint_ns\": \"bhagavad_gita_chat\" \n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nHuman: {state['messages'][-2].content}\")\n",
    "    print(f\"AI: {state['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b67f1",
   "metadata": {},
   "source": [
    "# 2. multi-agent debate\n",
    "\n",
    "create 2 agents (one optimistic, one pessimistic) and wire them in langgraph so they debate a user’s query. final node summarizes the debate for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f11c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d8fba78",
   "metadata": {},
   "source": [
    "# 3. workflow assistant\n",
    "input: user describes a task (e.g., “plan a study routine”).\n",
    "graph:\n",
    "\n",
    "- node 1 extracts subtasks\n",
    "\n",
    "- node 2 searches web (or dummy data)\n",
    "\n",
    "- node 3 synthesizes final plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15983d1d",
   "metadata": {},
   "source": [
    "# 4. self-healing coder\n",
    "a mini system where you feed code to one node, another node checks errors, and a third node proposes fixes. graph manages looping until code is “ok”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536e755b",
   "metadata": {},
   "source": [
    "# 5. daily journal summarizer\n",
    "user dumps daily notes. one node organizes events, another node extracts key emotions, last node writes “insight of the day”. over time, graph keeps a memory chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511bdd2c",
   "metadata": {},
   "source": [
    "# 6. task router\n",
    "user says something, graph decides:\n",
    "\n",
    "if it’s a factual q → send to retrieval node\n",
    "\n",
    "if it’s casual chat → send to smalltalk node\n",
    "\n",
    "if it’s a todo item → log into a simple json file.\n",
    "perfect to practice conditional edges in langgraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f28ca1e",
   "metadata": {},
   "source": [
    "# 7. multi-step reasoning tutor\n",
    "user asks a math/logic q.\n",
    "graph:\n",
    "\n",
    "step 1 → rephrase problem\n",
    "\n",
    "step 2 → propose reasoning steps\n",
    "\n",
    "step 3 → verify with another node\n",
    "\n",
    "step 4 → deliver clean final solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
