{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "410808d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/Github/365DaysOfData/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.tools import tool\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.tools import DuckDuckGoSearchResults, YouTubeSearchTool, AzureAiServicesDocumentIntelligenceTool, WikipediaQueryRun\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing import TypedDict\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6630e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! It's nice to meet you. I'm here to help with any questions or topics you'd like to discuss. How can I assist you today?\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OllamaLLM(model= 'llama3.2', temperature=0.7)\n",
    "\n",
    "llm.invoke(\"Hello, Ollama!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05a1d05",
   "metadata": {},
   "source": [
    "# 1. contextual q&a with memory\n",
    "build a chatbot that not only answers questions about a dataset (like pdfs or notes) but also remembers past queries using langgraph’s state nodes.\n",
    "bonus: add a “forget” command that clears memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13b5f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "doc = PyPDFLoader(\"/home/sam/Github/365DaysOfData/14-Agentic-AI/code/extras/geeta.pdf\")\n",
    "docs = doc.load()\n",
    "parser = StrOutputParser()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ce715fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "vectorstore = Chroma.from_documents(docs, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d23dd3e",
   "metadata": {},
   "source": [
    "### Using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3ccbda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context from the Bhagavad Gita, lust is described as the greatest enemy of living entities and the primary cause of their entanglement in the material world.\\n\\nLust is said to be the manifestation of the mode of passion (raja-guña) within an individual. When a living entity comes into contact with the material creation, their eternal love for Kåñëa (the Supreme Personality of Godhead) is transformed into lust, driven by their senses (indriyāëi), mind (manaù), and intelligence (buddhiù).\\n\\nLust is characterized as an all-devouring sinful enemy that can never be satisfied by material sense gratification. It is said to \"cover\" or \"overwhelm\" the embodied soul (dehinam), preventing them from attaining true knowledge (jïänam) and spiritual realization.\\n\\nThe Bhagavad Gita teaches that when lust is not transformed into love for the Supreme, it can lead to a cycle of craving, dissatisfaction, and ultimately, wrath. However, when an individual transforms their lustful propensities into a love for Kåñëa, or desires everything for Kåñëa, both lust and wrath can be spiritualized.\\n\\nIn this context, the Bhagavad Gita suggests that by cultivating a sense of devotion (bhakti) to Kåñëa, one can transcend the limitations of lust and attain liberation from the material world.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a Bhagavad Gita expert. answer the question based on the context provided. If the context does not help, say 'I don't know'\"),\n",
    "    (\"human\", \"query: {query} context: {context}\"),\n",
    "])\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough() \n",
    "    | (lambda query: {\"query\": query, \"context\": \"\\n\".join([doc.page_content for doc in vectorstore.similarity_search(query, k=3)])})\n",
    "    | chat_prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"What is lust, how lust destroy humans??\")\n",
    "\n",
    "\n",
    "# there's no memeory implemented yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9206b28c",
   "metadata": {},
   "source": [
    "### Using Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2423dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(state):\n",
    "    query = state[\"messages\"][-1].content if state[\"messages\"] else \"\"\n",
    "    docs = vectorstore.similarity_search(query, k=3)\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    return {\"context\": context}\n",
    "\n",
    "def response_generation(state):\n",
    "    context = state.get(\"context\", \"\")\n",
    "    query = state[\"messages\"][-1].content if state[\"messages\"] else \"\"\n",
    "\n",
    "    # Get the LLM's response\n",
    "    response = llm.invoke(chat_prompt.format_messages(query=query, context=context, chat_history=state[\"messages\"][:-1]))\n",
    "    \n",
    "    # Check the type of response and handle accordingly\n",
    "    if hasattr(response, 'content'):\n",
    "        response_text = response.content\n",
    "    else:\n",
    "        response_text = str(response)\n",
    "        \n",
    "    # Create and return AIMessage with the response text\n",
    "    return {\"messages\": state[\"messages\"] + [AIMessage(content=response_text)]}\n",
    "\n",
    "# Handle commands like 'forget'\n",
    "def handle_commands(state):\n",
    "    last_message = state[\"messages\"][-1].content.lower()\n",
    "    \n",
    "    if last_message.startswith(\"/forget\"):\n",
    "        # Clear the history except for this command\n",
    "        return {\"messages\": [state[\"messages\"][-1]], \"command_executed\": True}\n",
    "    return {\"command_executed\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cd1dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete implementation example\n",
    "class State(TypedDict):\n",
    "    messages: list\n",
    "    context: str\n",
    "    command_executed: bool\n",
    "\n",
    "graph = StateGraph(State)\n",
    "\n",
    "# Add all nodes with consistent names\n",
    "graph.add_node(\"retrieve\", retrieve_context)\n",
    "graph.add_node(\"command_handler\", handle_commands)\n",
    "graph.add_node(\"respond\", response_generation)\n",
    "\n",
    "# Connect START to first node\n",
    "graph.add_edge(START, \"command_handler\")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"command_handler\",\n",
    "    lambda state: state[\"command_executed\"],\n",
    "    {False: \"retrieve\", True: END}\n",
    ")\n",
    "\n",
    "graph.add_edge(\"retrieve\", \"respond\")\n",
    "graph.add_edge(\"respond\", END)\n",
    "\n",
    "workflow = graph.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02cce5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with Bhagavad Gita Expert (type '/forget' to clear history, '/exit' to quit)\n",
      "\n",
      "Human: what do you think aobut how this world is going to destroy?\n",
      "AI: Based on the provided context from the Bhagavad Gita, I believe that this world will be destroyed by those who are driven by demoniac tendencies and engage in activities that are detrimental to the well-being of others. These individuals are described as having \"no intelligence\" and being devoid of all sense, and they prioritize their own sense gratification over the happiness and prosperity of others.\n",
      "\n",
      "In particular, the verse mentions the invention of nuclear weapons, which is a modern example of such destructive technology. The Bhagavad Gita suggests that such creations are not meant for peace and prosperity but rather for destruction.\n",
      "\n",
      "The text also highlights the importance of education in developing the mode of goodness, which can lead to sobriety, full knowledge, and happiness. It warns against the dangers of animal killing and the consequences of being unaware of the interconnectedness of life.\n",
      "\n",
      "Ultimately, the verse suggests that if a certain percentage of the population develops Kåñëa consciousness and becomes situated in the mode of goodness, there is the possibility for peace and prosperity throughout the world. However, without such awareness and development, the world will continue to be plagued by destructive tendencies and chaos.\n"
     ]
    }
   ],
   "source": [
    "state = {\"messages\": [], \"context\": \"\", \"command_executed\": False}\n",
    "\n",
    "print(\"Chat with Bhagavad Gita Expert (type '/forget' to clear history, '/exit' to quit)\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nYou: \")\n",
    "    \n",
    "    if user_input.lower() == \"/exit\":\n",
    "        break\n",
    "        \n",
    "    state[\"messages\"] = add_messages(state[\"messages\"], [HumanMessage(content=user_input)])\n",
    "    \n",
    "    state = workflow.invoke(\n",
    "        state,\n",
    "        config={\n",
    "            \"configurable\": {\n",
    "                \"thread_id\": \"user_conversation_1\",  \n",
    "                \"checkpoint_ns\": \"bhagavad_gita_chat\" \n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nHuman: {state['messages'][-2].content}\")\n",
    "    print(f\"AI: {state['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b67f1",
   "metadata": {},
   "source": [
    "# 2. multi-agent pattern\n",
    "\n",
    "Create different specialized agent and extract output from different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9765623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "END = \"END\"\n",
    "\n",
    "# --- debate state ---\n",
    "class DebateState(TypedDict):\n",
    "    messages: list          # full transcript\n",
    "    next: str               # which agent speaks next\n",
    "    round: int              # current round\n",
    "    max_rounds: int         # stop after N rounds\n",
    "    topic: str              # the debate topic\n",
    "    sides: dict             # which side each agent defends\n",
    "    scores: dict            # track scores per agent\n",
    "    highlights: dict        # best one-liner or argument from each\n",
    "\n",
    "# --- workflow engine ---\n",
    "class StateGraph:\n",
    "    def __init__(self):\n",
    "        self.nodes = {}\n",
    "        self.entry_point = None\n",
    "    \n",
    "    def add_node(self, name, func):\n",
    "        self.nodes[name] = func\n",
    "    \n",
    "    def set_entry_point(self, name):\n",
    "        self.entry_point = name\n",
    "    \n",
    "    def get_node(self, name):\n",
    "        return self.nodes.get(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56dae55",
   "metadata": {},
   "source": [
    "    Agent that argues based on science, logic, and evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f11c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def science_agent(state: DebateState):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", f\"You are a science debater. Defend the side: {state['sides']['science']}. \"\n",
    "                   \"Respond with one short, sharp paragraph using scientific reasoning and facts. \"\n",
    "                   \"Be persuasive but concise.\"),\n",
    "        (\"user\", \"{messages}\")\n",
    "    ])\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"messages\": state[\"messages\"]})\n",
    "    \n",
    "    # log message\n",
    "    state[\"messages\"].append(f\"Science: {response}\")\n",
    "    \n",
    "    # update next turn\n",
    "    state[\"next\"] = \"religion\"\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5847f5b2",
   "metadata": {},
   "source": [
    "    Agent that argues from religious/God perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61923c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def religion_agent(state: DebateState):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", f\"You are a religious debater. Defend the side: {state['sides']['religion']}. \"\n",
    "                   \"Respond with one short, sharp paragraph using religious, moral, or spiritual reasoning. \"\n",
    "                   \"Be persuasive but concise.\"),\n",
    "        (\"user\", \"{messages}\")\n",
    "    ])\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"messages\": state[\"messages\"]})\n",
    "    \n",
    "    # log message\n",
    "    state[\"messages\"].append(f\"Religion: {response}\")\n",
    "    \n",
    "    # update next turn\n",
    "    state[\"next\"] = \"evaluator\"\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c956542",
   "metadata": {},
   "source": [
    "    Harsh evaluator that scores both sides and picks best highlights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744dac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluator_agent(state: DebateState):    \n",
    "    # grab last two debater messages\n",
    "    last_science = [m for m in state[\"messages\"] if m.startswith(\"Science:\")][-1]\n",
    "    last_religion = [m for m in state[\"messages\"] if m.startswith(\"Religion:\")][-1]\n",
    "    \n",
    "    # dummy scoring: could replace with LLM judgment, but here random for demo\n",
    "    science_score = random.randint(5, 10)\n",
    "    religion_score = random.randint(5, 10)\n",
    "    \n",
    "    state[\"scores\"][\"science\"] += science_score\n",
    "    state[\"scores\"][\"religion\"] += religion_score\n",
    "    \n",
    "    # pick \"highlight\" (just choose whichever scored higher this round)\n",
    "    if science_score >= religion_score:\n",
    "        state[\"highlights\"][\"science\"] = last_science\n",
    "        highlight = f\"Best this round: Science side → {last_science}\"\n",
    "    else:\n",
    "        state[\"highlights\"][\"religion\"] = last_religion\n",
    "        highlight = f\"Best this round: Religion side → {last_religion}\"\n",
    "    \n",
    "    # log judge decision\n",
    "    decision = (f\"Evaluator: Science scored {science_score}, \"\n",
    "                f\"Religion scored {religion_score}. {highlight}\")\n",
    "    state[\"messages\"].append(decision)\n",
    "    \n",
    "    # advance round or end debate\n",
    "    if state[\"round\"] >= state[\"max_rounds\"]:\n",
    "        state[\"next\"] = END\n",
    "    else:\n",
    "        state[\"round\"] += 1\n",
    "        state[\"next\"] = \"science\"\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83dc845",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "workflow = StateGraph()\n",
    "workflow.add_node(\"science\", science_agent)\n",
    "workflow.add_node(\"religion\", religion_agent)\n",
    "workflow.add_node(\"evaluator\", evaluator_agent)\n",
    "workflow.set_entry_point(\"science\")\n",
    "\n",
    "topic = input(\"Enter the debate topic (e.g., Education vs Money): \")\n",
    "\n",
    "sides = {\n",
    "    \"science\": topic.split(\" vs \")[0] if \"vs\" in topic else \"Education\",\n",
    "    \"religion\": topic.split(\" vs \")[1] if \"vs\" in topic else \"Money\"\n",
    "}\n",
    "\n",
    "state = DebateState(\n",
    "    messages=[],\n",
    "    next=workflow.entry_point,\n",
    "    round=1,\n",
    "    max_rounds=5,\n",
    "    topic=topic,\n",
    "    sides=sides,\n",
    "    scores={\"science\": 0, \"religion\": 0},\n",
    "    highlights={\"science\": \"\", \"religion\": \"\"}\n",
    ")\n",
    "\n",
    "while state[\"next\"] != END:\n",
    "    agent_func = workflow.get_node(state[\"next\"])\n",
    "    state = agent_func(state)\n",
    "    print(state[\"messages\"][-1])   \n",
    "\n",
    "print(\"\\n--- FINAL RESULTS ---\")\n",
    "print(f\"Science total: {state['scores']['science']} points\")\n",
    "print(f\"Religion total: {state['scores']['religion']} points\")\n",
    "\n",
    "if state[\"scores\"][\"science\"] > state[\"scores\"][\"religion\"]:\n",
    "    print(\"WINNER: Science side\")\n",
    "elif state[\"scores\"][\"science\"] < state[\"scores\"][\"religion\"]:\n",
    "    print(\"WINNER: Religion side\")\n",
    "else:\n",
    "    print(\"DRAW!\")\n",
    "\n",
    "print(\"\\n--- BEST HIGHLIGHTS ---\")\n",
    "print(f\"Science: {state['highlights']['science']}\")\n",
    "print(f\"Religion: {state['highlights']['religion']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8fba78",
   "metadata": {},
   "source": [
    "# 3. workflow assistant\n",
    "input: user describes a task (e.g., “plan a study routine”).\n",
    "graph:\n",
    "\n",
    "- node 1 extracts subtasks\n",
    "\n",
    "- node 2 searches web (or dummy data)\n",
    "\n",
    "- node 3 synthesizes final plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15983d1d",
   "metadata": {},
   "source": [
    "# 4. self-healing coder\n",
    "a mini system where you feed code to one node, another node checks errors, and a third node proposes fixes. graph manages looping until code is “ok”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536e755b",
   "metadata": {},
   "source": [
    "# 5. daily journal summarizer\n",
    "user dumps daily notes. one node organizes events, another node extracts key emotions, last node writes “insight of the day”. over time, graph keeps a memory chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511bdd2c",
   "metadata": {},
   "source": [
    "# 6. task router\n",
    "user says something, graph decides:\n",
    "\n",
    "if it’s a factual q → send to retrieval node\n",
    "\n",
    "if it’s casual chat → send to smalltalk node\n",
    "\n",
    "if it’s a todo item → log into a simple json file.\n",
    "perfect to practice conditional edges in langgraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f28ca1e",
   "metadata": {},
   "source": [
    "# 7. multi-step reasoning tutor\n",
    "user asks a math/logic q.\n",
    "graph:\n",
    "\n",
    "step 1 → rephrase problem\n",
    "\n",
    "step 2 → propose reasoning steps\n",
    "\n",
    "step 3 → verify with another node\n",
    "\n",
    "step 4 → deliver clean final solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
