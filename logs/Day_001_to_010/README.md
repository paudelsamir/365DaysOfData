# [2024/12/14 - 2024/12/24]


Before starting this challenge, I already had [**intermediate proficiency**](https://github.com/paudelsamir/python-mastery) in **Python** and had worked on several basic to advanced [**EDA projects**](https://github.com/paudelsamir/EDA-Projects). Iâ€™m also familiar with **Statistics** and **Mathematics**, so now my goal is to revisit these topics with a **practical, data-driven focus**.



# Day 01: Setting Up + Basics of Linear Algebra

![Importance of Linear Algebra](./img/Linear%20Algebra%20and%20Calculus/importance_of_linear_algebra.png)

### Topics Covered:
- **Scalars, Vectors, Matrices, Tensors**: Basic data structures for ML.
    ![](img/Linear%20Algebra%20and%20Calculus/example_of_tensor.png)
  
- **Linear Combination and Span**: Representing data points as weighted sums. Used in **Linear Regression** and neural networks.
    ![](img/Linear%20Algebra%20and%20Calculus/3dlinear_transformation.png)
  
- **Determinants**: Matrix invertibility, unique solutions in **linear regression**.
  
- **Dot and Cross Product**: Similarity (e.g., in **SVMs**) and vector transformations.
    ![](img/Linear%20Algebra%20and%20Calculus/dot%20product.png)

---

# Day 02: Decomposition, Derivation, Integration, and Gradient Descent

### Topics Covered:
- **Identity and Inverse Matrices**: Solving equations (e.g., **linear regression**) and optimization (e.g., **gradient descent**).
  
- **Eigenvalues and Eigenvectors**: **PCA**, **SVD**, feature extraction; eigenvalues capture variance.
    ![](img/eigenvalue_eigenvector.png)
  
- **Singular Value Decomposition (SVD)**: **PCA**, image compression, and **collaborative filtering**.

[Notes Here](./data/Linear%20Algebra%20for%20ML.pdf)

### Calculus Overview:
- **Functions & Graphs**: Relationship between input (e.g., house size) and output (e.g., house price).
  
- **Derivatives**: Adjust model parameters to minimize error in predictions (e.g., house price).
    ![](img/Linear%20Algebra%20and%20Calculus/area_of_circle.png)
  
- **Partial Derivatives**: Measure change with respect to one variable, used in **neural networks** for weight updates.
  
- **Gradient Descent**: Optimization to minimize the cost function (error).
  
- **Optimization**: Finding the best values (minima/maxima) of a function to improve predictions.
  
- **Integrals**: Calculate area under a curve, used in **probabilistic models** (e.g., Naive Bayes).
    ![](img/Linear%20Algebra%20and%20Calculus/integration.png)

*Revised statistics and probability concepts. Ready for the **ML Specialization** course!*

---

# Day 03: 
