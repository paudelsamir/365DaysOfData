I followed the CampusX RNN playlist during my exam days and couldn't log every day due to exam pressure. I haven't done any projects yet, but I focused on learning through these resources:

2. **RNN Architectures**: Covered vanilla RNNs, LSTM, and GRU via the CampusX playlist:
    - [Recurrent Neural Network | Forward Propagation | Architecture](https://www.youtube.com/playlist?list=PLGP2q2bIgaNzBBpxxNUf126chLsQ20dfG)
    - [Types of RNN | Many to Many | One to Many | Many to One RNNs](https://www.youtube.com/playlist?list=PLGP2q2bIgaNzBBpxxNUf126chLsQ20dfG)
    - [LSTM | Long Short Term Memory | Part 1](https://www.youtube.com/playlist?list=PLGP2q2bIgaNzBBpxxNUf126chLsQ20dfG)
    - [LSTM Architecture | Part 2](https://www.youtube.com/playlist?list=PLGP2q2bIgaNzBBpxxNUf126chLsQ20dfG)
    - [Gated Recurrent Unit | Deep Learning | GRU](https://www.youtube.com/playlist?list=PLGP2q2bIgaNzBBpxxNUf126chLsQ20dfG)
3. **Forward and Backward Pass**: Learned about backpropagation through time:
    - [How Backpropagation works in RNN | Backpropagation Through Time](https://www.youtube.com/playlist?list=PLGP2q2bIgaNzBBpxxNUf126chLsQ20dfG)
4. **Problems with RNNs**: Understood limitations and challenges:
    - [Problems with RNN | 100 Days of Deep Learning](https://www.youtube.com/playlist?list=PLGP2q2bIgaNzBBpxxNUf126chLsQ20dfG)
5. **Advanced Topics**: Explored deep and bidirectional RNNs:
    - [Deep RNNs | Stacked RNNs | Stacked LSTMs | Stacked GRUs](https://www.youtube.com/playlist?list=PLGP2q2bIgaNzBBpxxNUf126chLsQ20dfG)
    - [Bidirectional RNN | BiLSTM | Bidirectional LSTM | Bidirectional GRU](https://www.youtube.com/playlist?list=PLGP2q2bIgaNzBBpxxNUf126chLsQ20dfG)


This chapter serves as a transition because it marks the shift from studying the fundamentals and architectures of Recurrent Neural Networks (RNNs) to exploring Natural Language Processing (NLP). Here, I consolidated my understanding of RNNs, their variants, and their limitations, which are foundational for tackling more advanced NLP topics. The next folders will build upon this knowledge, moving towards state-of-the-art deep learning and large language models.
NOW, i have Entered to NLP. will explore most of the NLP in next folders:

[Folder: STATE OF THE ART](../09-State-of-the-Art-DL)

[Folder: Foundations of LLMS](../11-Foundations-Of-LLMS)