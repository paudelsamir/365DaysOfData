> [!Warning]
> This content is AI-generated and is being refined to suit my needs. The separate folder exists because it contains everything you need to learn to stay up-to-date with the Foundation Models chapter. SOTA (State of the Art) refers to the latest models and techniques. This folder may not include implementations yet, but once I finish the foundations (both building and using), I plan to implement these models and review the relevant papers. Before building anything from scratch, I'll explore both the usage and construction of LLMs, then return to implementing these concepts.



| Category                | Models / Techniques                                                                                                                                                                                                                                                                                                                                                  |
|-------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Computer Vision (CV)** | **Image Classification / General CV**<br>- [ResNet (2015)](https://arxiv.org/abs/1512.03385): Residual connections to address vanishing gradients.<br>- [DenseNet (2017)](https://arxiv.org/abs/1608.06993): Dense connectivity for feature reuse.<br>- [EfficientNet (2019)](https://arxiv.org/abs/1905.11946): Compound scaling of depth, width, and resolution.<br>- [ConvNeXt (2022)](https://arxiv.org/abs/2201.03545): Modernized convolutional network with transformer-inspired design.<br><br>**Object Detection**<br>- [Faster R-CNN (2015)](https://arxiv.org/abs/1506.01497): Two-stage detection, research baseline.<br>- [YOLOv7](https://arxiv.org/abs/2207.02696) / [YOLOv8](https://github.com/ultralytics/ultralytics): Fast, single-stage, real-time detectors.<br>- [DETR (2020)](https://arxiv.org/abs/2005.12872): Transformer-based, end-to-end detection without anchors.<br><br>**Segmentation**<br>- [U-Net (2015)](https://arxiv.org/abs/1505.04597): Encoder-decoder with skip connections, widely used in biomedical segmentation.<br>- [Mask R-CNN (2017)](https://arxiv.org/abs/1703.06870): Instance segmentation, extension of Faster R-CNN.<br>- [Swin Transformer (2021)](https://arxiv.org/abs/2103.14030): Hierarchical transformer for segmentation and detection.<br><br>**Generative CV**<br>- [StyleGAN](https://arxiv.org/abs/1812.04948) / [StyleGAN3 (2021)](https://arxiv.org/abs/2106.12423): High-quality image generation.<br>- [Diffusion Models (DDPM)](https://arxiv.org/abs/2006.11239), [Stable Diffusion](https://arxiv.org/abs/2112.10752), [Imagen](https://arxiv.org/abs/2205.11487): Probabilistic image generation.<br>- [VQ-VAE](https://arxiv.org/abs/1711.00937) / [VQ-GAN](https://arxiv.org/abs/2012.09841): Discrete latent-space generative models. |
| **Natural Language Processing (NLP)** | **Transformer-Based LLMs**<br>- [BERT (2018)](https://arxiv.org/abs/1810.04805): Bidirectional encoder, masked language modeling.<br>- [RoBERTa (2019)](https://arxiv.org/abs/1907.11692): Improved BERT pretraining.<br>- [GPT-2](https://cdn.openai.com/better-language-models/paper.pdf), [GPT-3](https://arxiv.org/abs/2005.14165), [GPT-4](https://cdn.openai.com/papers/gpt-4.pdf): Autoregressive LLMs, zero/few-shot learning.<br>- [T5 (2019)](https://arxiv.org/abs/1910.10683) / [mT5 (2021)](https://arxiv.org/abs/2010.11934): Text-to-text framework for diverse NLP tasks.<br>- [LLaMA (2023)](https://arxiv.org/abs/2302.13971) / [LLaMA 2](https://arxiv.org/abs/2307.09288): Lightweight, research-friendly LLMs.<br>- [BLOOM](https://arxiv.org/abs/2211.05100) / [Falcon](https://arxiv.org/abs/2306.01116): Open-access multilingual LLMs.<br><br>**Sequence-to-Sequence / Translation**<br>- [Transformer (2017)](https://arxiv.org/abs/1706.03762): Base architecture for modern NLP.<br>- [mBART](https://arxiv.org/abs/2001.08210) / [MarianMT](https://arxiv.org/abs/2007.13802): Multilingual translation models.<br><br>**Instruction-Tuned Models**<br>- [ChatGPT / GPT-4](https://cdn.openai.com/papers/gpt-4.pdf): RLHF fine-tuned for instruction following.<br>- [FLAN-T5](https://arxiv.org/abs/2210.11416): Instruction-tuned T5, improved zero-shot performance.<br><br>**Retrieval-Augmented / Knowledge-Grounded**<br>- [RAG (2020)](https://arxiv.org/abs/2005.11401): Combines retrieval with transformers for long-context QA.<br>- [Atlas](https://arxiv.org/abs/2212.08061) / [RETRO](https://arxiv.org/abs/2110.07875): LLMs augmented with external retrieval. |
| **Multimodal / Cross-Modal** | **Text ↔ Image**<br>- [CLIP (2021)](https://arxiv.org/abs/2103.00020): Contrastive pretraining for image-text alignment.<br>- [DALL·E 2 (2022)](https://arxiv.org/abs/2204.06125): Text-to-image generation using CLIP guidance.<br>- [Stable Diffusion (2022)](https://arxiv.org/abs/2112.10752): Open-source text-to-image diffusion model.<br><br>**Video / Vision-Language**<br>- [VideoMAE](https://arxiv.org/abs/2203.12602) / [TimeSformer](https://arxiv.org/abs/2102.05095): Transformer-based video representation learning.<br>- [Florence](https://arxiv.org/abs/2111.11432) / [Flamingo](https://arxiv.org/abs/2204.14198): Multimodal transformers for text, image, and video. |
| **Reinforcement Learning / Control** | - [DQN (2015)](https://www.nature.com/articles/nature14236): Deep Q-Network for Atari games.<br>- [A3C](https://arxiv.org/abs/1602.01783) / [PPO](https://arxiv.org/abs/1707.06347) / [SAC](https://arxiv.org/abs/1801.01290): Scalable and stable actor-critic methods.<br>- [MuZero (2019)](https://arxiv.org/abs/1911.08265): Model-based RL without explicit environment models.<br>- [RLAIF](https://arxiv.org/abs/2309.16647) / [RLHF](https://arxiv.org/abs/2203.02155): Aligns LLMs using reinforcement learning and human feedback. |
| **Generative Models / Diffusion / Hybrid** | - [VAE (2013)](https://arxiv.org/abs/1312.6114): Probabilistic latent-space generative model.<br>- [GAN (2014)](https://arxiv.org/abs/1406.2661): Adversarial learning for realistic generation.<br>- [StyleGAN3 (2021)](https://arxiv.org/abs/2106.12423): High-fidelity image generation.<br>- [DDPM](https://arxiv.org/abs/2006.11239) / [Stable Diffusion](https://arxiv.org/abs/2112.10752) / [Imagen](https://arxiv.org/abs/2205.11487) / [MidJourney](https://docs.midjourney.com/docs/research): Diffusion-based generative models.<br>- [Score-based Models](https://arxiv.org/abs/2011.13456): Continuous-time generative modeling.<br>- Hybrid [GAN + Diffusion](https://arxiv.org/abs/2302.03069) / [Autoregressive](https://arxiv.org/abs/1706.03762): Cutting-edge multimodal research. |
| **Foundations / Backbone Architectures** | - [Transformer](https://arxiv.org/abs/1706.03762) / [ViT (Vision Transformer)](https://arxiv.org/abs/2010.11929): Attention-based architectures for vision.<br>- [Swin Transformer](https://arxiv.org/abs/2103.14030): Hierarchical attention for vision tasks.<br>- [MLP-Mixer](https://arxiv.org/abs/2105.01601) / [gMLP](https://arxiv.org/abs/2105.08050): Token mixing architectures.<br>- [EfficientFormer](https://arxiv.org/abs/2206.01191) / [ConvNeXt](https://arxiv.org/abs/2201.03545): Efficient architectures for deployment. |
