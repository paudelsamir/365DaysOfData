
<div id="top"></div>
<div align="left">
  <!-- Left Badges (Last Updated & Repo Size) -->
  <img src="https://img.shields.io/github/last-commit/paudelsamir/365DaysOfData" alt="Last Updated" />
  <img src="https://img.shields.io/github/repo-size/paudelsamir/365DaysOfData" alt="Repo Size" />
  
</div>

![cover](./resources/images/cover.png)

I‚Äôll be sharing updates regularly on [**LinkedIn**](https://www.linkedin.com/in/paudelsamir/).

<div align="right">
  <a href="#bottom" target="_blank">
    <img src="https://img.shields.io/badge/Go%20to%20Bottom%20ü°ª-orange?style=for-the-badge&logo=arrow-down-right&logoColor=white" style="height: 25px; width: auto;" />
  </a>
</div>

## Projects Completed
| Projects | Description | Live Demo |
|----------|-------------|-----------|
|[Football Players Market Value Prediction](https://github.com/paudelsamir/365DaysOfData/tree/main/04-ML-Based-Football-Players-Market-Value-Prediction)| A 10 day end-to-end machine learning capstone project involving data scraping, cleaning, feature engineering, model training, and deployment to a cloud server. |[Link üëÜüèΩ](https://paudelsamir.streamlit.app/)|
|[Movie Recommender System]()|A content Based end to end Movie Recommender System based on 5000 movies from [Kaggle dataset](https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata?select=tmdb_5000_movies.csv)| [Link üëÜüèΩ](https://movie-recommender-samir.streamlit.app/)|

 

## Resources
| Books & Courses  | Completion Status |
|--------------------|-------------------|
| [Machine Learning Specialization @Coursera](https://www.coursera.org/specializations/machine-learning-introduction) | ‚úÖ |
| [100 Days ML @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH) | ‚úÖ |
| [Deep Learning @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) | ‚åõ |
| [Intro to Deep Learning @MIT](https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI) | ‚åõ |
| [Grokking Deep Learning @Andrew W. Trask](https://edu.anarcho-copy.org/Algorithm/grokking-deep-learning.pdf) | ‚åõ |
<!-- | [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https://github.com/yanshengjia/ml-road/blob/master/resources/Hands%20On%20Machine%20Learning%20with%20Scikit%20Learn%20and%20TensorFlow.pdf)| üèä‚åõ|  -->


## Progress
| Days | Date               | Topics                      | Resources    |
|------|--------------------|-----------------------------|--------------|
| [Day1](#day-01-basics-of-linear-algebra) |2024·öë12·öë14 |Basics of Linear Algebra  | [3blue1brown](https://www.3blue1brown.com/topics/linear-algebra) |
| [Day2](#day-02-decomposition-derivation-integration-and-gradient-descent) | 2024-12-15 | Decomposition, Derivation, Integration, and Gradient Descent | [3blue1brown](https://www.3blue1brown.com/topics/calculus) |
| [Day3](#day-03-supervised-machine-learning-regression-and-classificaiton) | 2024-12-16 |Supervised Learning, Regression and classification|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) |
| [Day4](#day-04-unsupervised-learning-clustering-dimensionality-reduction) | 2024-12-17 |Unsupervised Learning: Clustering and dimensionality reduction|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) |
| [Day5](#day-05-univariate-linear-regression) | 2024-12-18|Univariate linear Regression|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) |
| [Day6](#day-06-cost-function) | 2024-12-19 |Cost Functions|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) |
| [Day7](#day-07-gradient-descent) | 2024-12-20 |Gradient Descent|[CampusX, ](https://www.youtube.com/watch?v=ORyfPJypKuU) [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) |
| [Day8](#day-08-effect-of-learning-rate-cost-function-and-data-on-gd) | 2024-12-21 |Effect of learning Rate, Cost function and Data on GD|[CampusX, ](https://www.youtube.com/watch?v=ORyfPJypKuU) [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) |
| [Day9](#day-09-linear-regression-with-multiple-features-vectorization) | 2024-12-22 |Linear Regression with multiple features, Vectorization| [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) |
| [Day10](#day10-feature-scaling) | 2024-12-23 |Feature Scaling, Visualization of Multiple Regression and Polynomial Regression| [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) |
| [Day11](#day-11-feature-engineering-and-polynomial-regression) | 2024-12-24 |Feature Engineering, Polynomial Regression| [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) |
| [Day12](#day-12-linear-regression-using-scikit-learn) | 2024-12-25 |Scikit-Learn revision, Linear Regression using Scikit Learn| [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) |
| [Day13](#day-13-classification) | 2024-12-26 |LR lab, Classification|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day14](#day-14-logistic-regression-sigmoid-function) | 2024-12-27 |Logistic Regression, Sigmoid Function|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) [, CampusX](https://www.youtube.com/watch?v=ABrrSwMYWSg&list=PLKnIA16_Rmvb-ZTsM1QS-tlwmlkeGSnru&index=6)|
| [Day15](#day-15-decision-boundary-cost-function) | 2024-12-28 |Decision Boundary, Cost Function|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) [, CampusX](https://www.youtube.com/watch?v=ABrrSwMYWSg&list=PLKnIA16_Rmvb-ZTsM1QS-tlwmlkeGSnru&index=6)|
| [Day16](#day-16-gradient-descent-for-logical-regression) | 2024-12-29 |Gradient Descent for logical regression|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) [, CampusX](https://www.youtube.com/watch?v=ABrrSwMYWSg&list=PLKnIA16_Rmvb-ZTsM1QS-tlwmlkeGSnru&index=6)|
| [Day17](#day-17-underfitting-overfitting) | 2024-12-30 |Underfitting, Overfitting, Regularization Polynomial Features, Hyperparameters|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day18](#day-18-neurons-layer-neural-netowrk-forward-propagation) | 2024-12-31 |Neurons, Neural Netowrk, Forward Propagation|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day19](#day-19-forward-propagation) | 2025-01-01 |Forward Propagation, Tensorflow implementations|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day20](#day-20-python-implementation-from-scratch) | 2025-01-02 |Building and comparing models (Binary Classification)|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day21](#day-21-vectorization-model-training) | 2025-01-03 |Vectorization, Model training using Tensoflow|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day22](#day-22-activation-functions-softmax) | 2025-01-04 |Activation Functions, Softmax Intution|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day23](#day-23-implementation-of-softmax-regression) | 2025-01-05 |Implementing Softmax|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day24](#day-24-backpropagation---what-and-how-) | 2025-01-06 |Backpropagaton, What and how??|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day25](#day-25-backpropagation---why-advices-for-applying-machine-learning) | 2025-01-07 |Backpropagation - Why? Advices for applying machine Learning|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day26](#day-26-model-selection-and-trainingcross-validationtest-sets-bias-and-variance) | 2025-01-08 |Model selection, training test, cross validation, Bias and Variance, Learning curves|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day27](#day-27-machine-learning-development-process-ml-workflow) | 2025-01-09 |Machine Learning Development Process, ML workflow|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day28](#day-28-machine-learning-model-error-analysis-and-transfer-learning) | 2025-01-10 |Implementing ML model: Error Analysis and Transfer Learning|[Notebook: Implementation, ](02-Advanced-Learning-Algorithms/code/day28_implementation.ipynb)[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day29](#day-29-error-metrices-encoding-of-categorical-data-transoformers) | 2025-01-11 |Error Metrices, Encoding of Categorical Data, Transoformers|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) [, CampusX](https://www.youtube.com/watch?v=8osKeShYVRQ&list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH&index=65)|
| [Day30](#day-30-scikit-learn-pipelines--ridge-regression-l2-regularization) | 2025-01-12 | Scikit-Learn Pipelines & Ridge Regression (L2 Regularization) |[Documentation: Scikit-Learn](https://scikit-learn.org/1.5/modules/linear_model.html)  [, CampusX](https://www.youtube.com/watch?v=8osKeShYVRQ&list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH&index=65)|
| [Day31](#day-31-lasso-regression-l1-regularization-elastic-net-regularization) | 2025-01-13 | Lasso Regression (L1 Regularization), Elastic Net Regularization |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day32](#day-32-decision-tree-entropy-and-information-gain) | 2025-01-14 | Decision Tree Emtropy and Information Gain |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day33](#day-33-hyperparameters-of-dt-with-sclearn-regression-trees) | 2025-01-15 | Hyperparameters of Decision Tree with Scikit Learn, Regression Trees |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH) [, Visualize Yourself>>](https://decisiontreeclassifier.streamlit.app/)|
| [Day34](#day-34-visualization-using-dtreeviz-ensemble-learning-overview) | 2025-01-16 | Visualization Using DtreeViz(), Ensemble Learning |[Github Repo: Dtreeviz, ](https://github.com/parrt/dtreeviz) [ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day35](#day-35-voting-ensemble--classification-and-regression) | 2025-01-17 | Voting Ensemble >> Classification and Regression |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH) [, Visualize Yourself](https://votingclassifier.streamlit.app/)|
| [Day36](#day-36-bagging-ensemble--classification-and-regression) | 2025-01-18 |  Bagging Ensemble > Classification and Regression |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day37](#day-37-random-forest-intution-working-and-difference-with-bagging-random-forest-hyperparameters) | 2025-01-19 | Random Forest: Intution, Working and difference with bagging, Random Forest Hyperparameters |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day38](#day-38-boosting-ensemble-adaboost-boosting) | 2025-01-20 |  Boosting Ensemble: Adaboost Boosting |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day39](#day-39-understanding-gradientboosting-with-regression) | 2025-01-21 |  Understanding GradientBoosting with Regression |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day40](#day-40-gradient-boosting-with-classification) | 2025-01-22 | Gradient Boosting with Classification|[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH) [, Vlog Link](https://towardsdatascience.com/all-you-need-to-know-about-gradient-boosting-algorithm-part-2-classification-d3ed8f56541e)|
| [Day41](#day-41-variations-of-gradient-boosting-xgboost---introduction) | 2025-01-23 | XGboost Introduction |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day42](#day-42-xgboost-for-regression-and-classification-catboost-vs-xgboost-vs-lightgbm) | 2025-01-24 | XGBoost for Regression and Classification, Catboost Vs XGboost Vs LightGBM |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH) [,Research Paper](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf)|
| [Day43](#day-43-stacking-ensemble-understanding-blending-and-k-fold-methods) | 2025-01-25 | Stacking Ensemble, Understanding Blending and K fold |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day44](#day-44-k-nearest-neighbor-coding-knn-from-scratch-and-applying-on-different-datasets) | 2025-01-26 | K-Nearest Neighbor, Coding KNN from Scratch |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day45](#day-45-support-vector-machines) | 2025-01-27 | Support Vector Machine  |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day46](#day-46-k-means-clustering-dbscan) | 2025-01-28 | K-Means Clustering, DBSCAN | [Notebook: K-Means clustering Demo](02-Advanced-Learning-Algorithms/code/day46_kmeans-clustering-demo.ipynb) [, Notebook: DBSCAN demo](02-Advanced-Learning-Algorithms/code/day46_dbscan_demo.ipynb) |
| [Day47](#day-47-hierarchical-clustering-silhouette-score) | 2025-01-29 | Hierarchical Clustering, Silhouette Score | [Kaggle, ](https://www.kaggle.com/) [ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH) |
| [Day49](#day-49-pca-principle-component-analysis-implementing-with-mnist-dataset) | 2025-01-30 | PCA (Principle Component Analysis), Implementing with MNIST dataset | [Notebook: Applying PCA on MNIST dataset](03-Unsupervised-Learning-And-Reinforcement-Learning/code/day49_pca_on_mnist.ipynb) |
| [Day50](#day-50-visualizing-and-comparing-pca-t-sne-umap-and-lda--revision-with-the-course-ml-specialization) | 2025-02-01 | Visualizing and Comparing PCA, t-SNE, UMAP, and LDA + Revision with the course ML specialization | [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day51](#day-51-anomaly-detection) | 2025-02-02 | Anomaly Detection |[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)[, Notebook: Anomaly Detection](03-Unsupervised-Learning-And-Reinforcement-Learning/code/day51_anomaly_detection.ipynb) |
| [Day52](#day-52-collaborative-filtering) | 2025-02-03 | Collaborative Filtering |[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day53](#day-53-project--football-players-market-value-prediction---introduction-and-planning) | 2025-02-04 | Project @ Football Players Market Value Prediction - Introduction and Planning | [Project Plan](04-ML-Based-Football-Players-Market-Value-Prediction/images/project_plan.png) |
| [Day54](#day-54-project--football-players-market-value-prediction---collecting-data-scraping) | 2025-02-05 | Project @ Football Players Market Value Prediction - Collecting Data (Scraping) | [Notebook](04-ML-Based-Football-Players-Market-Value-Prediction/notebooks/scraping.ipynb) |
| [Day55](#day-55-project--football-players-market-value-prediction---cleaning-data) | 2025-02-06 | Project @ Football Players Market Value Prediction - Cleaning Data | [Notebook](04-ML-Based-Football-Players-Market-Value-Prediction/notebooks/cleaning.ipynb) |
| [Day56](#day-56-project--football-players-market-value-prediction---eda) | 2025-02-07 | Project @ Football Players Market Value Prediction - EDA | [Notebook](04-ML-Based-Football-Players-Market-Value-Prediction/notebooks/eda.ipynb) |
| [Day57](#day-57-project--football-players-market-value-prediction---feature-engineering-creating-features-transforming-features) | 2025-02-08 | Project @ Football Players Market Value Prediction - Feature Engineering: (Creating features, Transforming Features) | [Notebook](04-ML-Based-Football-Players-Market-Value-Prediction/notebooks/feature_engineering.ipynb) |
| [Day58](#day-58-project--football-players-market-value-prediction---ml--linear-regression-with-refined-features-and-deploying-with-streamlit) | 2025-02-09 | Project @ Football Players Market Value Prediction - ML: (Linear Regression with Refined Features and deploying with Streamlit) | [Notebook](04-ML-Based-Football-Players-Market-Value-Prediction/notebooks/feature_engineering_2.ipynb.ipynb) |
| [Day59](#day-59-project--football-players-market-value-prediction---complete-streamlit-setup-for-our-first-model---linear-regression) | 2025-02-10 |Project @ Complete Streamlit setup for Linear Regression | [Streamlit Documentation](https://docs.streamlit.io/) |
| [Day60](#day-60-project--football-players-market-value-prediction---testing-ridge-lasso-and-decision-trees) | 2025-02-11 |Project @  Testing Ridge, Lasso, and Decision Trees | [Project @ Football Players Market Value Prediction](https://paudelsamir.streamlit.app/) |
| [Day61](#day-61-project--football-players-market-value-prediction--had-to-hit-reset-from-feature-engineering) | 2025-02-12 |Project @ Had to hit reset from Feature Engineering | [Project @ Football Players Market Value Prediction](https://paudelsamir.streamlit.app/) |
| [Day62](#day-62-project--football-players-market-value-prediction---finalizing-project-and-deploying-it) | 2025-02-13 | Project @ Finalizing Project and Deploying it | [Project @ Football Players Market Value Prediction](https://paudelsamir.streamlit.app/) |
| [Day63](#day-63-content-based-movie-recommender-system---preprocessing) | 2025-02-14 | Content-Based Movie Recommender System - Preprocessing | [Notebook](05-Content-Based-Movie-Recommender-System/notebooks/preprocessing.ipynb) |
| [Day64](#day-64-content-based-movie-recommender-system---building-and-deployment) | 2025-02-15 | Content-Based Movie Recommender System - Building and Deployment | [Live Demo](https://movie-recommender-samir.streamlit.app/) |
| [Day65](#day-65-diving-into-deep-learning) | 2025-02-16 | Diving into Deep Learning | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day66](#day-66-perceptrons) | 2025-02-17 | Perceptrons | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day67]() | 2025-02-18 | ---------------- | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day68]() | 2025-02-19 | ---------------- | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day69]() | 2025-02-20 | ---------------- | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day70]() | 2025-02-21 | ---------------- | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day71]() | 2025-02-22 | ---------------- | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day72]() | 2025-02-23 | ---------------- | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day73]() | 2025-02-24 | ---------------- | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day74]() | 2025-02-25 | ---------------- | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day75]() | 2025-02-26 | ---------------- | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day76]() | 2025-02-27 | ---------------- | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day77]() | 2025-02-28 | ---------------- | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day78]() | 2025-03-01 | ---------------- | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day79]() | 2025-03-02 | ---------------- | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day80]() | 2025-03-03 | ---------------- | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |









---
<br>
<br>



# Day 01: Basics of Linear Algebra 
<!-- <img src="01-Supervised-Learning/images/importance_of_linear_algebra.png" width="400" /> -->

<!-- 
![Importance of Linear Algebra](./01-Supervised-Learning/images/importance_of_linear_algebra.png) -->
linear algebra is used to represent data, perform matrix operations, and solve equations in algorithms like regression, pca, and neural networks.
- Scalars, Vectors, Matrices, Tensors: Basic data structures for ML.
    ![](./01-Supervised-Learning/images/example_of_tensor.png)

- Linear Combination and Span: Representing data points as weighted sums. Used in Linear Regression and neural networks.
    ![](./01-Supervised-Learning/images/3dlinear_transformation.png)

- Determinants: Matrix invertibility, unique solutions in linear regression.

- Dot and Cross Product: Similarity (e.g., in SVMs) and vector transformations.
    ![](./01-Supervised-Learning/images/dot_product.png)

*Slow progress right?? but consistent wins the race!*

---
# Day 02: Decomposition, Derivation, Integration, and Gradient Descent

- Identity and Inverse Matrices: Solving equations (e.g., linear regression) and optimization (e.g., gradient descent).

- Eigenvalues and Eigenvectors: PCA, SVD, feature extraction; eigenvalues capture variance.
    ![](./01-Supervised-Learning/images/eigenvalue_eigenvector.png)

- Singular Value Decomposition (SVD): PCA, image compression, and collaborative filtering.

[Notes Here]()

### Calculus Overview:
- Functions & Graphs: Relationship between input (e.g., house size) and output (e.g., house price).

- Derivatives: Adjust model parameters to minimize error in predictions (e.g., house price).
    ![](./01-Supervised-Learning/images/area_of_circle.png)

- Partial Derivatives: Measure change with respect to one variable, used in neural networks for weight updates.

- Gradient Descent: Optimization to minimize the cost function (error).

- Optimization: Finding the best values (minima/maxima) of a function to improve predictions.

- Integrals: Calculate area under a curve, used in probabilistic models (e.g., Naive Bayes).
    ![](./01-Supervised-Learning/images/integration.png)

Revised statistics and probability concepts. Ready for the ML Specialization course!

---
# Day 03: Supervised Machine Learning: Regression and Classificaiton
<!-- [Notes credit](https://drive.google.com/file/d/1SO3WJZGSPx2jypBUugJkkwO8LZozBK7B/view?usp=sharing) -->

- Supervised Learning: <br>
![Supervised Learning](01-Supervised-Learning/images/day1_supervisedlearning.gif)
![](./01-Supervised-Learning/images/Supervised%20Learning.png)
- Regression:<br>
![](./01-Supervised-Learning/images/Regression_model.png)
- Classification:<br>
![](./01-Supervised-Learning/images/classification_model.png)
![](./01-Supervised-Learning/images/classification_model2.png)

---
# Day 04: Unsupervised Learning: Clustering, dimensionality reduction

data only comes with input x, but not output labels y. Algorithm has to find structure in data.
![Unsupervised Learinging](01-Supervised-Learning/images/day2_supervisedvsunsupervised.gif)
- Clustering: group similar data points together <br>
![alt text](./01-Supervised-Learning/images/clustering.png)
- dimensionality reduction: compress data using fewer numbers eg image compression<br> <img src="./01-Supervised-Learning/images/dimensionality_reduction.png" width = "300">
<!-- ![alt text](./01-Supervised-Learning/images/dimensionality_reduction.png) -->
- anomaly detection: find unusual data points eg fraud detection<br>

---
# Day 05: Univariate Linear Regression:
- Learned univariate linear regression and practiced building a model to predict house prices using size as input, including defining the hypothesis function, making predictions, and visualizing results.

[Notebook: Model Representation](./01-Supervised-Learning/code/day04_model_representation.ipynb)

<!-- ![](./01-Supervised-Learning/images/notes_univariate_linear_regression.jpg) -->

<img src="./01-Supervised-Learning/images/notes_univariate_linear_regression.jpg" width = "400">

![](./01-Supervised-Learning/images/notations_summary.png) - Univariate Linear Regression Quiz

![](./01-Supervised-Learning/images/univariate_linear_regression_quiz.png)

---
# Day 06: Cost Function:
![alt text](./01-Supervised-Learning/images/costfunction.jpg)
Visualization of cost function:
![Visualization of cost function](./01-Supervised-Learning/images/visualization_costfunction.png)

- manually reading these contour plot is not effective or correct, as the complexity increases, we need an algorithm which figures out the values w, b (parameters) to get the best fit time, minimizing cost function

[Notebook: Model Representation](./01-Supervised-Learning/code/day04_model_representation.ipynb)

*Gradient descent is an algorithm which does this task*

---
# Day 07: Gradient Descent
[Notebook: Gradient descent](./01-Supervised-Learning/code/day07_gradient-descent-code-from-scratch.ipynb)

![gradient descent](01-Supervised-Learning/images/day4_gradient_descent_parameter_a.gif)
learned the basics by assuming slope constant and with only the vertical shift.
later learned GD with both the parameters w and b.
![alt text](./01-Supervised-Learning/images/gradientdescent.png)
<!-- 
![alt text](./01-Supervised-Learning/images/implementation_of_gradient_descent.png) -->
 ![alt text](01-Supervised-Learning/images/gdnote.jpg) 

---
# Day 08: Effect of learning Rate, Cost function and Data on GD
- learning rate on GD:Affects the step size; too high can overshoot, too low can slow convergence
<img src="./01-Supervised-Learning/images/learningrate_eg1.png" width = "400">
<!-- ![alt text](./01-Supervised-Learning/images/learningrate_eg1.png) -->
<img src="./01-Supervised-Learning/images/learningrate_eg2.png" width = "400">
<!-- ![alt text](./01-Supervised-Learning/images/learningrate_eg2.png) -->
- cost function on GD:Smooth, convex functions help faster convergence; complex ones may trap in local minima

- Data on GD:Quality and scaling affect stability; more data improves gradient estimates

[Notebook: gradient descent animation 3d](./01-Supervised-Learning/code/day08_gradient-descent-animation(both-m-and-b).ipynb)

---
# Day 09: Linear Regression with multiple features, Vectorization

Predicts target using multiple features, minimizing error.
- Vectorization: Matrix operations replace loops for faster calculations.

![alt text\](image.png](01-Supervised-Learning/images/multifeatureLR.png)

[Lab1: Vectorization](./01-Supervised-Learning/code/day09_Python_Numpy_Vectorization_Soln.ipynb)
<br>

---
# Day10: Feature Scaling
[Lab2: Multiple Variable](./01-Supervised-Learning/code/day09_Lab02_Multiple_Variable_Soln.ipynb)

Today, I learned about feature scaling and how it helps improve predictions. There are multiple methods for feature scaling, including
- Min-Max Scaling
- Mean Normalization
- Z-Score Normalization
![alt text](./01-Supervised-Learning/images/notesfeature_scaling.png
)
To ensure proper convergence:
check the learning curve to confirm the loss is decreasing.
Start with a small learning rate and gradually increase to find the optimal value.

![alt text](./01-Supervised-Learning/images/gdforconvergence.png) 
![alt text](01-Supervised-Learning/images/choosinglearningrate.png)

---
# Day 11: Feature engineering and Polynomial Regression
feature engineering improves features to better predict the target.

eg If we need to predict the cost of flooring and have length and breadth of the room as features, we can use feature engineering to create a new feature, area (length √ó breadth), which directly impacts the flooring cost.

<img src="./01-Supervised-Learning/images/notes_featureengineering.jpg" width = "400">
![alt text](01-Supervised-Learning/images/notes_featureengineering.jpg)

explored polynomial regression that models the relationship between variables as a polynomial curve instead of a straight line

**Equation:**  
y = b‚ÇÄ + b‚ÇÅx + b‚ÇÇx¬≤ + ... + b‚Çôx‚Åø 
It is useful for capturing nonlinear relationships in data.
![alt text](01-Supervised-Learning/images/visualizationof_polynomialfunctions.png)

![alt text](01-Supervised-Learning/images/scalingfeatures_complexfunctions.png)


[Lab1: Feature Scaling and Learning Rate](01-Supervised-Learning/code/day11_Feature_Scaling_and_Learning_Rate_Soln.ipynb) <br>
[Lab2: Feature Engineering and PolyRegression](01-Supervised-Learning/code/day11_FeatEng_PolyReg_Soln.ipynb)

---
# Day 12: Linear Regression using Scikit Learn
Had a productive session with linear regression in scikit learn. The lab helped me get a better grasp of the process, though I need more practice with tuning models. Also revisited the Scikit-Learn models ,more comfortable with them now

- Scikit-learn is an open-source Python library used for machine learning that provides simple and efficient tools for data analysis, including algorithms for classification, regression, clustering, and dimensionality reduction.

![](https://github.com/paudelsamir/365DaysOfData/blob/main/01-Supervised-Learning/images/LRusingscikitlearn.png)
![alt text](01-Supervised-Learning/images/LRusingscikitlearn.png)
![alt text](01-Supervised-Learning/images/scikitlearn_cleansheet1.png) 
![alt text](01-Supervised-Learning/images/scikitlearn_cleansheet2.png) <br>
[Notebook: ScikitLearn GD](01-Supervised-Learning/code/day12_Sklearn_GD_Soln.ipynb)

---
# Day 13: Classification

[Notebook: Graded Lab](./01-Supervised-Learning/code/day13_Linear_Regression_Lab.ipynb) 
![alt text](01-Supervised-Learning/images/code-snapshot_GD_LR.png)
<br>
[Notebook: Classification solution](./01-Supervised-Learning/code/day13_Classification_Soln.ipynb)

- Classification is the process of categorizing items into different groups based on shared characteristics, like classifying tumors into benign (non-cancerous) and malignant (cancerous) based on their growth behavior and potential to spread.
![](./01-Supervised-Learning/images/example_of_lr_on_categoricaldata.png)
The example above demonstrates that the linear model is insufficient to model categorical data. The model can be extended as described in the following lab.

---
# Day 14: Logistic Regression, Sigmoid Function

- Logistic Regression: A classification algorithm used to predict probabilities of binary outcomes.
![Logistic regression on categorical data](01-Supervised-Learning/images/day14_egoflogisticrregression.png)
- Sigmoid Function: A mathematical function that maps any input to a value between 0 and 1, used in logistic regression to model probabilities.
 ![sigmoid-function](01-Supervised-Learning/images/sigmoid_function.png)

[Notebook: Sigmoid Function](01-Supervised-Learning/code/day14_Sigmoid_function_Soln.ipynb)

---
# Day 15: Decision Boundary, Cost Function
[Notebook: Cost Function ](01-Supervised-Learning/code/day16_Cost_Function_Soln.ipynb)
- Decision Boundary: A line or surface that separates different classes in a classification problem based on the model‚Äôs predictions.
- cost function:
![formula cost function](01-Supervised-Learning/images/formula_costfunction.png)
![notes](01-Supervised-Learning/images/day14_15notes.jpg)
 
[Notebook: Decision boundary](01-Supervised-Learning/code/day15_Decision_Boundary_Soln.ipynb)

[Notebook Logistic Loss ](01-Supervised-Learning/code/day16_LogisticLoss_Soln.ipynb)

---
# Day 16: Gradient Descent for Logical Regression

[Notebook: Gradient Descent Model implementation](01-Supervised-Learning/code/day16_Gradient_Descent_Soln.ipynb)

[Notebook: GD with Scikit-learn](01-Supervised-Learning/code/day16_Scikit_Learn_Soln.ipynb)

Learned logistic regression cost, gradient descent, and sigmoid derivatives through step-by-step derivations and comparisons with linear regression.

![notes_day16](01-Supervised-Learning/images/day16_notes_GD.jpg)
![alt text](01-Supervised-Learning/images/gdoflogisticregression.png)

---
# Day 17: Underfitting, Overfitting


Today, explored teh concepts, overfitting (high variance), underfitting (high bias) and generalization(just right). 
Regularization to reduce Overfitting. Explored Regularized logistic regression.
- If the data is in non linear behaviour then we have to appply Ml algos like decision tree, random forest and svm.

Explored hypermeters of logistic regression, and gained some knowledge.
![text](01-Supervised-Learning/images/day17_notes2.jpg) 
![text](01-Supervised-Learning/images/day17_notes1.jpg)

[Notebook:Overfitting Solution ](01-Supervised-Learning/code/day17_Overfitting_Soln.ipynb)

![text](01-Supervised-Learning/images/day17_overfittingexample.png)


[Notebook:Regularization ](01-Supervised-Learning/code/day17_regularization.ipynb)
![text](01-Supervised-Learning/images/day17_regularizatiom.png) 


---
# Day 18: Neurons, Layer, Neural netowrk, forward propagation
- neural network: 
neural networks are machine learning algorithms that model complex patterns using multiple hidden layers and non-linear activation functions. they take inputs, pass them through hidden layers of neurons, and output a prediction.
![alt text](02-Advanced-Learning-Algorithms/images/day18_NeuralNetowkr.png) 

- Neurons:
a neuron takes weighted inputs, applies an activation function, and outputs a result. inputs can be features or outputs from previous neurons, with weights adjusting their influence.
![alt text](02-Advanced-Learning-Algorithms/images/day18_single_neuroninaction.gif)
fig: single neuron in action

- Synapse: 
synapses connect neurons and carry the weighted inputs. each connection has a weight that adjusts during training.

- weights: 
weights control the strength of connections between neurons. they are multiplied by inputs to influence the output, and are adjusted during training.

Popular activation functions include relu and sigmoid.

- Bias: 
bias is a constant added to the weighted input before applying the activation function, helping the model represent patterns that don‚Äôt pass through the origin.

- Layers: 
![alt text](02-Advanced-Learning-Algorithms/images/day18_Layers.png) 
  - **input layer**: holds the data for the model, with each neuron representing an attribute.
  - **hidden layer**: applies activation functions to the inputs and passes results to the next layer.
  - **output layer**: receives input from the last hidden layer and returns the model‚Äôs prediction.


---
# Day 19: Forward Propagation
- Forward Propagation: Input data is ‚Äúforward propagated‚Äù through the network layer by layer to the final layer which outputs a prediction.


Notes for today:
![alt text](02-Advanced-Learning-Algorithms/images/day19_notes1.jpg) 
![alt text](02-Advanced-Learning-Algorithms/images/day19_notes2.jpg)
 

Matrix Representation:
![text](02-Advanced-Learning-Algorithms/images/day19_matrixrepn.png) 
How forward Prop works for digit classification??
![text](02-Advanced-Learning-Algorithms/images/day19_UnderstandingNN.gif)
[Notebook: Neurons and Layers](02-Advanced-Learning-Algorithms/code/day19_Neurons_and_Layers.ipynb) 
[Notebook: A small Neural Netowrk using tensoflow](02-Advanced-Learning-Algorithms/code/day19_NN_CoffeeRoasting_TF.ipynb)


- **tensorflow basics**:
    - **representation of data**:numpy arrays used for input (e.g., 2D arrays).
        
        ```python
        x = np.array([[1, 2, 3], [4, 5, 6]])
        
        ```
        
    - **building a neural network**:
        1. define layers:
            
            ```python
            layer1 = dense(units=25, activation='sigmoid')
            layer2 = dense(units=15, activation='sigmoid')
            layer3 = dense(units=1, activation='sigmoid')
            
            ```
            
        2. stack layers in a model:
            
            ```python
            model = sequential([layer1, layer2, layer3])
            
            ```
            
        3. compile and train:
            
            ```python
            model.compile(optimizer='adam', loss='binary_crossentropy')
            model.fit(x, y, epochs=10)
            
            ```
            
    - **visualization**:neurons connect layer by layer, with weights and biases computed at each step (refer to attached gif).

---
# Day 20: Python Implementation from Scratch

Implemented forward propagation to compute predictions and backpropagation to optimize weights for binary classification.
- AGI: An advanced AI capable of generalizing across tasks like humans.

![loss graph](02-Advanced-Learning-Algorithms/images/day20_modelaccuracies.png)

![model accuracy](02-Advanced-Learning-Algorithms/images/day_20_comparisonforwardvsbackward.png)

[Notebook: Building Models](02-Advanced-Learning-Algorithms/code/day20_BuildingForwardPropagation.ipynb)

---
# Day 21: Vectorization, Model Training
Exploredd Vectorization for efficient computation
- Tensorflow: model.compile, binary_crossentropy, model.fit
trained a binary classification model and tested its accuracy

![alt text](02-Advanced-Learning-Algorithms/images/day21_testaccuracy.png) 
Training Model with tensorflow:
![alt text](02-Advanced-Learning-Algorithms/images/day21_trainingNNwithtenserflow.png)
Notes for today:
![Notes for today](02-Advanced-Learning-Algorithms/images/day21_notes.jpg)


---
# Day 22: Activation Functions, Softmax
*the universal approximation theorem explains that a neural network with enough hidden neurons and non-linear activations like sigmoid or relu can approximate almost any function, even complex patterns like wavy graphs.*

#### Activation Functions:
commonly used activation functions include:
- sigmoid: squashes values between 0 and 1, often used for binary classification.
- relu: outputs 0 for negatives and the input itself for positives, commonly used in hidden layers.
- tanh: outputs between -1 and 1, useful for centered data.

![multiclass example](./02-Advanced-Learning-Algorithms/images/day22_eg_multiclassclassn.png)

for multiclass classification, softmax is ideal in the output layer as it converts logits into probabilities that sum to 1. during training, the model adjusts weights to maximize the correct class probability, using categorical cross-entropy loss. softmax generalizes logistic regression, which is typically used for binary classification. in both, activation and loss functions differ based on the output type.


Logistic Vs softmax:

![logistic vs softmax](./02-Advanced-Learning-Algorithms/images/day22_logisticvssoftmax.png)

Notes:

![Notes](02-Advanced-Learning-Algorithms/images/day22_notes1.jpg) 
![Notes](02-Advanced-Learning-Algorithms/images/day22_notes2.jpg)
---
# Day 23: Implementation of Softmax Regression

- Classification with multiple Outputs:
PS: there's difference between multiclass classificaiton and multilabel classification
[Notebook: Softmax/Multinomial in Iris dataset- Multilabel](02-Advanced-Learning-Algorithms/code/day23_softmax_demo.ipynb)

> *NOTE: softmax regression is a classification algorithm that calculates probabilities for multiple classes using a linear combination of inputs and the softmax function. the class with the highest probability is chosen as the prediction*

Improved Implementation of Softmax:
![Roundoff](02-Advanced-Learning-Algorithms/images/day23_roundofferrors.png)

Tensorflow implementation:
```python
model = Sequential(
    [ 
        Dense(25, activation = 'relu'),
        Dense(15, activation = 'relu'),
        Dense(4, activation = 'softmax')    # < softmax activation here

        ##         Dense(4, activation = 'linear')   #<-- Note
    ]
)
model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    ##     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #<-- Note ---- This is preferred model softmax and loss are combined for more accurate result.
    optimizer=tf.keras.optimizers.Adam(0.001),
)

model.fit(
    X_train,y_train,
    epochs=10
)
    
```

---
# Day 24: Backpropagation - what and how ?
- what? Backpropagation adjusts nneural network weights by propagating errors backward using the chain rule and optimizing them with gradient descent.
- how? Forward pass to compute outputs, Calculate errors, propagate them backward, and update weights iteratively. Take a look at document below.

Backpropagation example with Neural Network:
![Backpropagation example with Neural Network](02-Advanced-Learning-Algorithms/images/day24_backpropagation_example.png)

[Notebook: Backprop](02-Advanced-Learning-Algorithms/code/day25_Backprop.ipynb)

[pdf- How to implement??](02-Advanced-Learning-Algorithms/backpropagation_how.pdf)

*Notes:*

![alt text](02-Advanced-Learning-Algorithms/images/day24_notes.jpg) 
---
# Day 25: Backpropagation - Why? Advices for applying machine Learning

Today, I dived into the reasons behind backpropagation's effectiveness in training neural networks. It's not just about adjusting weights; it's the gradients that guide optimization, helping the model minimize error and improve predictions. The backpropagation process makes sure that the error gets distributed in a way that leads to better learning.

*How????*
- **Gradients**: they ensure efficient error minimization by guiding weight updates.
- **Optimization**: properly tuned gradients lead to smoother optimization and faster convergence.
- **Model Evaluation**: evaluating model performance becomes easier with backpropagation because of the systematic error propagation and weight adjustments.

*Notes:*

![alt text](02-Advanced-Learning-Algorithms/images/day25_notes.jpg)


---
# Day 26: Model selection and training/cross validation/test sets, Bias and Variance

mnist dataset: Label and our prediction after training
![alt text](02-Advanced-Learning-Algorithms/images/day26_predictionvslabel.png)
Errors in our prediction:
![alt text](02-Advanced-Learning-Algorithms/images/day26_errors_inMNIST.png) 
- the importance of splitting data into training, validation, and test sets 
cross-validation and its role in hyperparameter tuning.
- Diagnosing bias and variance with error trends: high bias = underfit, high variance = overfit.
- regulariaztion to handle the tradeoff between bias and variance.
- how learning curves reveal insights about model performance and whether gathering more data will help.
![summary of learning algorithm:](02-Advanced-Learning-Algorithms/images/day26_biasandvar_as_regularization.png) ![summary of learning algorithm:](02-Advanced-Learning-Algorithms/images/day26_debugging_learningalgo.png)

Notes: 
![notes](02-Advanced-Learning-Algorithms/images/day26_notes.jpg)

[Notebook: Practice Lab: Neural Networks for Handwritten Digit Recognition, Multiclass](02-Advanced-Learning-Algorithms/code/day25_NN_multiclass_mnist.ipynb)

[Notebook: Diagnosing Bias and Variance](02-Advanced-Learning-Algorithms/code/day26_Diagnosing_Bias_and_Variance.ipynb) 

[Notebook: Model Evaluation and selection](02-Advanced-Learning-Algorithms/code/day26_Model_Evaluation_and_Selection.ipynb)

---
# Day 27: Machine Learning Development Process, ML workflow
machine learning development process  

1. ml development is iterative, involving:  
   - choosing model and data architecture.  
   - training the model.  
   - diagnosing bias, variance, and errors.  

![alt text](02-Advanced-Learning-Algorithms/images/day27_fullMLworkflow.png)
2. error analysis: identify and fix patterns in model failures.  
3. adding data:  
   - data augmentation: modify existing data (e.g., distortions).  
   - data synthesis: create artificial data.  
4. transfer learning:  
   - reuse pre-trained models for similar tasks.  
   - fine-tune them with your own data.  
  

5. ml projects follow these steps:  
   - data collection.  
   - preprocessing.  
   - modeling.  
   - evaluation.  
   - deployment.  
   ![alt text](02-Advanced-Learning-Algorithms/images/day27_model_deployment.png)
   - monitoring.  


6. ethics and fairness -  
    ensure ethical use by:  
   - avoiding biased decisions in loans, jobs, etc.  
   - preventing harmful applications like deepfakes. 

Notes:
![Notes](02-Advanced-Learning-Algorithms/images/day27_notes.jpg)

---
# Day 28: Machine Learning Model: Error Analysis and Transfer Learning

[Notebook: Code Implementation from Scratch](02-Advanced-Learning-Algorithms/code/day28_implementation.ipynb)

- *Confusion Matrix Analysis:* the most frequent error is misclassifying 5 as 3. overall, the error rate is around 8%.
![alt text](02-Advanced-Learning-Algorithms/images/day28_confusion_matrix_mnist_first.png)

- *Iterations Insight:* after 200 iterations, the error rate does not decrease significantly, suggesting that 200 iterations are enough for the model to converge.
![alt text](02-Advanced-Learning-Algorithms/images/day28_error_reduction_with_tuning.png)

- *Data Augmentation Insight:* despite applying data augmentation, there was no improvement in accuracy. this is because the MNIST dataset is already preprocessed, with centered and normalized images, making the augmentation techniques less effective. in general, data augmentation works best when the dataset is smaller or images are not preprocessed.
![alt text](02-Advanced-Learning-Algorithms/images/day28_model_accuracy_beforeandafter.png)

- *Transfer Learning with MobileNetV2:* 
  - Training set accuracy improved from 56.95% to 73.02%.
  - Validation accuracy increased from 70.52% to 74.06%.
  - Loss decreased consistently for both training and validation sets, signaling better learning and generalization.
  - The model shows significant improvement over epochs. 
  - Using transfer learning, the model started with an initial accuracy of 74% (pre-trained on ImageNet). As training progressed, the model continued to adapt, improving with each epoch. transfer learning was effective, yielding good results with fewer epochs.
![alt text](02-Advanced-Learning-Algorithms/images/day28_transfer_learningimage.png)

---
# Day 29: Error Metrices, Encoding of Categorical Data, Transoformers

[Notebook: Lab week 3: Improving Model ](02-Advanced-Learning-Algorithms/code/day29_improvingML_models.ipynb) 

[Notebook: Error Metrics ](02-Advanced-Learning-Algorithms/code/day29_error_metrics.ipynb)
### Precision vs. Recall Trade-Off: 

- High Precision:
Only hire candidates you‚Äôre sure are good.
Result: Fewer bad hires, but you might miss some great ones.
Example: You hire 5 people, all are good, but you missed 10 other good ones.
- High Recall:
Hire as many as possible to ensure no good candidate is missed.
Result: You catch all great candidates but end up with some bad hires too.
Example: You hire 50 people, 20 are good, but 30 are bad.
- When to Focus on Each?
1. Precision: When mistakes (bad hires) are costly.
Example: Hiring a brain surgeon.

2. Recall: When missing good candidates is worse.
Example: Hiring for a customer service team.


#### Encoding of Categoriical Data:

| **Encoding Type** | **Use When** | **Example** |
| --- | --- | --- |
| **Label Encoding** | Small, unordered categories | Colors: `[Red, Blue]` |
| **Ordinal Encoding** | Ordered categories | Education: `[Low, High]` |
| **One-Hot Encoding** | Nominal data, fewer unique categories | Days: `[Mon, Tue, Wed]` |
<br>

#### Types of Transformers: 

| **Transformer** | **Purpose** | **Example Use Case** | **Input** | **Output** |
| --- | --- | --- | --- | --- |
| **Column Transformer** | Apply different transformations to different columns (e.g., scaling and encoding). | Scale age and one-hot encode city names. | `Age: [25, 35, 45]`, `City: [NY, LA, CHI]` | `[-1.22, 0, 0, 1]`, `[0, 1, 0, 0]`, `[1.22, 0, 1, 0]` |
| **Function Transformer** | Apply a mathematical function (e.g., log or sqrt) to all values. | Apply logarithmic transformation to data. | `[1, 10, 100]` | `[0.69, 2.39, 4.61]` |
| **Power Transformer** | Normalize and reduce skewness in data, making it more Gaussian-like. | Stabilize variance in highly skewed data. | `[1, 10, 100]` | `[-1.22, 0.0, 1.22]` |

---
# Day 30: Scikit-Learn Pipelines & Ridge Regression (L2 Regularization) 

I explored how to create pipelines in Scikit-learn to streamline the process of combining multiple steps (like preprocessing, model fitting, and regularization) into a single object. This simplifies workflows and ensures reproducibility.

There are three techniques of regularization:
- Ridge (L2)
- Lasso (L1)
- Elastic Net (Combination)

#### Key Understanding of Ridge Regression:
 1. How the coefficient Get affected?
 - Regularization shrinks the coefficients, preventing them from becoming too large, which reduces overfitting.
 ![alt text](02-Advanced-Learning-Algorithms/images/day30_howcoeff_effected.png)
 2. Higher Values are impacted more
 - The larger the regularization value (alpha), the more the coefficients are reduced.
 ![alt text](02-Advanced-Learning-Algorithms/images/day30_higher_coeff_effected_more.png)
 3. Impact on Bias Variance Tradeoff
 - Higher regularization increases bias but reduces variance, making the model more generalizable.
 ![alt text](02-Advanced-Learning-Algorithms/images/day30_impacton_biasvariance.png)
 4. Effect on Loss Function
 - adds a penalty term that limits the magnitude of the coefficients.
 ![alt text](02-Advanced-Learning-Algorithms/images/day30_effectonlossfunction.png)
 5. Why Ridge Regression is called so?
 - Named after the concept of creating a "ridge" or constraint on the model‚Äôs coefficients, preventing them from growing too large.

 <img src='https://explained.ai/regularization/images/lagrange-animation.gif'>

Notes:
![alt text](02-Advanced-Learning-Algorithms/images/day30_notes1.jpg)
![alt text](02-Advanced-Learning-Algorithms/images/day30_notes2.jpg) 

[Notebook: Key Understandings of ridge Regression](02-Advanced-Learning-Algorithms/code/day30_visualizing_ridgeregression_key_understanding.ipynb)

---
# Day 31: Lasso Regression (L1 Regularization), Elastic Net Regularization 

#### Lasso Regression:
1. **How are coefficients affected by Œª (alpha)?**
    As Œª increases, regularization strength grows, shrinking less important coefficients to **exactly zero**. Larger Œª values lead to feature selection by removing irrelevant features.
    ![alt text](02-Advanced-Learning-Algorithms/images/day31_howcoeffafected.png)
    
2. **Are higher coefficients affected more?**
    No. Lasso affects **smaller coefficients more**, shrinking them to zero first. Larger coefficients remain relatively unaffected if they contribute significantly to the model.
    ![alt text](02-Advanced-Learning-Algorithms/images/day31_higher_coeff.png)
    
3. **Impact of Œª on bias and variance**:
    - **Higher Œª** increases bias (simpler model) and reduces variance.
    - **Lower Œª** reduces bias (complex model) but increases variance.
    ![alt text](02-Advanced-Learning-Algorithms/images/day31_biasandvariance.png)

4. **Effect of Regularization on Loss Function**:
    - Lasso add Œª sum(w_i) to the loss, promoting sparsity by penalizing the absolute magnitude of coefficients. Unlike Ridge, it can remove features entirely, improving interpretability.
    ![alt text](02-Advanced-Learning-Algorithms/images/day31_effectofregularizationon_costfunction.png)

#### Elastic Net Regularization:
Elastic Net combines L1 (Lasso) and L2 (Ridge) penalties. It selects important features by shrinking some coefficients to zero (like Lasso) and handles correlated features by shrinking coefficients without setting them to zero (like Ridge). It‚Äôs useful when features are both highly correlated and some are irrelevant. Elastic Net is controlled by two parameters: 
Œ± (mix of Lasso and Ridge) and 
Œª (regularization strength).
![alt text](02-Advanced-Learning-Algorithms/images/day31elasticvsridgevslasso.png)

Notes:
![alt text](02-Advanced-Learning-Algorithms/images/day31_notes.jpg)



---
# Day 32: Decision Tree: Entropy and Information Gain
A decision tree is a flowchart-like structure used for classification or regression, where data is split into branches based on conditions until a final decision (leaf) is reached.
![alt text](02-Advanced-Learning-Algorithms/images/day32_decisiontree.gif)

- Decision Tree on Categorical Variables: Splits data based on categories like "Sunny" or "Rainy".
- Decision Tree on Numerical Variables: Splits data using thresholds like Age > 30.
- How Decision Tree Works: Repeatedly splits data into smaller groups based on conditions.
- Terminology: Root (start), Branch (path), Leaf (decision).
-  Pro - Simple to understand; Con - Can overfit.
- Entropy: Measures uncertainty; low entropy = purer data.
- Entropy Calculation: ```‚àë p(x) * log2(p(x)).```
- Information Gain: Reduction in entropy after splitting data. ``` IG=Entropy(before)‚àíWeighted¬†Entropy(after)``` 
- Gini Impurity: Measures group purity, faster than entropy.
- Why Use Gini Over Entropy: Simpler and computationally faster.

Notes;
![alt text](02-Advanced-Learning-Algorithms/images/day32_notes.jpg)


---
# Day 33: Hyperparameters of DT with sclearn, Regression Trees:

![alt text](02-Advanced-Learning-Algorithms/images/day33_notes.jpg)

Studied hyperparameters of Decision Trees in Scikit-learn and techniques to handle overfitting and underfitting.
- Criterion (gini, entropy, log loss): Determines the quality of a split.
- Splitter: Helps reduce overfitting with better random splits.
- Max Depth: Controls tree depth; too high causes overfitting, too low causes underfitting.
- Min Sample Split: Sets the minimum samples required to split a node.
- Min Sample Leaf: Sets the minimum samples per leaf.
- Max Features: Determines how many features to use for splits.
- Max Leaf Nodes: Limits the number of leaf nodes.
- Min Impurity Decrease: Controls splitting based on impurity reduction.

A Regression Tree predicts continuous variables by splitting data to minimize variance. The best split is determined by maximizing variance reduction, calculated as the variance of the root node minus the weighted average variance of the leaf nodes.


Code:
![alt text](02-Advanced-Learning-Algorithms/images/day33_dt_code.png) 
Output:
![alt text](02-Advanced-Learning-Algorithms/images/day33_dt_output.png)


---
# Day 34: Visualization Using dtreeviz, Ensemble Learning Overview

[Notebook: Visualization of Decision Tree Official](02-Advanced-Learning-Algorithms/code/day34_dtreeviz_sklearn_visualisations.ipynb) 
[Notebook: Visualization of Decision Tree ](02-Advanced-Learning-Algorithms/code/day34_dtreeviz_sklearn_pipeline_visualisations.ipynb)

- started learning about ensemble learning:
understood the concept of "wisdom of the crowd" in ensemble methods.
types of ensemble methods: voting, bagging, boosting, and stacking.
overview of random forest and bootstrapped aggregation (bagging).
learned how boosting adjusts predictions iteratively.

Visuals of Bagging, boosting and stacking:
![Notes](02-Advanced-Learning-Algorithms/images/day34_bagging-boosting-stacking.webp) 
Notes;
![Notes](02-Advanced-Learning-Algorithms/images/day34_notes.jpg)


---
# Day 35: Voting Ensemble > Classification and Regression

- Soft voting: logistic regression: 60% fraud, random forest: 80% fraud, svm: 40% fraud 
‚Üí final probability: (60% + 80% + 40%) / 3 = 60%. 
![alt text](02-Advanced-Learning-Algorithms/images/day35_Voting.jpg)

- Hard voting: majority wins, logistic regression predicts "fraud," random forest predicts "not fraud," and svm predicts "fraud" ‚Üí final prediction: "fraud."



Conclusions from [Notebook: voting Classifier](02-Advanced-Learning-Algorithms/code/day35_votingClassifier.ipynb)

- weighted voting: assigning weights to classifiers helps emphasize stronger models, further improving performance.

- same algorithm, different hyperparameters: tweaking hyperparameters (e.g., kernel degree in SVM) can lead to significant accuracy changes, highlighting the value of hyperparameter optimization.
#### Classification:
```python

voting_clf = VotingClassifier(
    estimators=[
        ('lr', log_reg),  # logistic regression: good for linear patterns
        ('rf', rand_forest),  # random forest: captures complex relationships
        ('svc', svm_clf)  # support vector machine: handles edge cases
    ],
    voting='soft',  # averages probabilities from all models for final prediction
    weights=[2, 1, 1],  # gives higher importance to logistic regression
    n_jobs=-1  # enables parallel processing for faster training
)


```
What to use? soft voting or hard voting, depends if possible use both and then try to find out:
![alt text](02-Advanced-Learning-Algorithms/images/day35_hard_voting.png) ![alt text](02-Advanced-Learning-Algorithms/images/day35_soft_voting.png)

#### Regression:
- voting regressor combines multiple regression models to predict continuous values.

similarly, in regression, soft voting averages continuous predictions, and weighted voting helps models with higher performance contribute more.

![Voting regressor:](02-Advanced-Learning-Algorithms/images/day35_voting_regressor.png)

Visualize voting regression here: [link](https://votingclassifier.streamlit.app/)

``` python
voting_reg = VotingRegressor(
    estimators=[
        ('lr', lin_reg),  # linear regression: good for linear relationships
        ('rf', rand_forest_reg),  # random forest regressor: handles non-linear patterns
        ('svr', svr_clf)  # support vector regressor: captures complex relationships
    ],
    weights=[2, 1, 1],  # assigns higher importance to linear regression
    n_jobs=-1  # enables parallel processing for faster training
)

```

---
# Day 36: Bagging Ensemble > Classification and Regression

- bagging (bootstrap aggregating) is an ensemble learning technique that combines predictions from multiple models trained on different subsets of the data (created via bootstrapping) to improve accuracy and reduce variance.

- Intution: 

    ![alt text](02-Advanced-Learning-Algorithms/images/day36_bagging.gif)
    - random sampling: create multiple datasets by sampling with replacement from the original dataset.
    - train independently: train a model (single preferred) on each bootstrapped dataset (e.g., decision trees).
    - combine predictions: aggregate their predictions by majority voting (classification) or averaging (regression).
    this reduces overfitting and increases stability, especially for high-variance models.

[Notebook: Bagging Intution](02-Advanced-Learning-Algorithms/code/day36_bagging_demo.ipynb)

#### Classification:
- Intution
![alt text](02-Advanced-Learning-Algorithms/images/day36_baggingclassifier.webp)
- Code Demo
``` python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

# initialize the bagging classifier
bagging_model = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(),  # base model for ensemble; here, decision trees
    n_estimators=10,                          # number of base models to train
    max_samples=1.0,                          # fraction of the dataset for each base model (1.0 = 100%)
    max_features=1.0,                         # fraction of features used in each bootstrap sample
    bootstrap=True,                           # sample datasets with replacement (True enables bootstrapping)
    bootstrap_features=False,                 # sample features with replacement (False = no feature bootstrapping)
    random_state=42                           # seed for reproducibility
)


```

#### Regression:
- Intution:
![alt text](02-Advanced-Learning-Algorithms/images/day36_baggingregressor.png)
- Code Demo:
``` python
bagging_model = BaggingRegressor(
    base_estimator=DecisionTreeRegressor(),  # base model for ensemble; here, decision trees
    n_estimators=10,                          # number of base models to train
    max_samples=1.0,                          # fraction of the dataset for each base model (1.0 = 100%)
    max_features=1.0,                         # fraction of features used in each bootstrap sample
    bootstrap=True,                           # sample datasets with replacement (True enables bootstrapping)
    bootstrap_features=False,                 # sample features with replacement (False = no feature bootstrapping)
    random_state=42                           # seed for reproducibility
)

```

---
# Day 37: Random Forest: Intution, Working and difference with bagging, Random Forest Hyperparameters

random forest is like a bunch of decision trees making a group decision. each tree gets a vote on the outcome, and the most votes win. it's like asking a bunch of experts for advice and going with the majority.

Sampling Techniques:
- row sampling
- column sampling
- combination

[Notebook Random Forest](02-Advanced-Learning-Algorithms/code/day37_RandomForest.ipynb)

#### Random Forest vs bagging:

- why random forest performs well: it reduces overfitting by averaging multiple decision trees trained on different random subsets of data and features, improving accuracy and robustness.

- random forest vs bagging: both use multiple trees, but random forest adds feature randomness at each split, making trees less correlated and boosting performance.

![alt text](02-Advanced-Learning-Algorithms/images/day37_notes.jpg)

[Notebook: Random forest Vs bagging](02-Advanced-Learning-Algorithms/code/day37_randomForestVsBagging.ipynb)

#### Random forest Hyperparameters:
``` python

model = RandomForestClassifier(
    # core hyperparameters
    n_estimators=100,         # number of decision trees in the forest
    max_features="sqrt",      # max features to consider at each split
    max_depth=None,           # max depth of each tree; None = grow fully
    min_samples_split=2,      # min samples needed to split a node
    min_samples_leaf=1,       # min samples required in a leaf node
    bootstrap=True,           # with replacement or without replacement

    # advanced hyperparameters
    max_leaf_nodes=None,      # max number of leaf nodes per tree; None = unlimited
    min_weight_fraction_leaf=0.0,  # min fraction of total weight for a leaf node
    class_weight=None,        # weights for handling class imbalance (e.g., 'balanced')
    ccp_alpha=0.0,            # complexity parameter for pruning; trade-off between size and accuracy
    criterion="gini",         # metric to evaluate splits: "gini" (default) or "entropy"
    warm_start=False,         # reuse previous trees for incremental training; False = train from scratch
    oob_score=False,          # whether to use out-of-bag samples to estimate generalization accuracy
    verbose=0,                # verbosity of output (0 = silent)
    n_jobs=-1,                # number of CPU cores for parallel processing; -1 = use all cores
    random_state=42           # seed for reproducibility
)

```
---
# Day 38: Boosting Ensemble: Adaboost Boosting

- boosting is a sequential ensemble learning method where models correct the errors of previous models to reduce bias.
combines weak learners (like decision stumps) to create a strong predictive model.
![Boosting](02-Advanced-Learning-Algorithms/images/day38_boosting.png)

Notes:
![alt text](02-Advanced-Learning-Algorithms/images/day38_notes.jpg)

#### AdaBoost Exploration:

step-by-step understanding of adaboost‚Äôs workflow, including:
- calculating sample weights and weak learner errors.
- updating model contributions based on performance.
- final prediction via weighted majority vote.
- visualized how adaboost focuses on difficult samples.

implemented adaboost from scratch without using sklearn.
![Adaboost from scratch](02-Advanced-Learning-Algorithms/images/day38_adaboostfromscratch.gif)

[Notebook: Adaboost Implementation](02-Advanced-Learning-Algorithms/code/day38_adaboost_from_scratch.ipynb)


---
# Day 39: Understanding GradientBoosting with Regression:

 "a model that learns step-by-step by fixing the mistakes of the previous model."
 ![Algorithm:](02-Advanced-Learning-Algorithms/images/day39_gradient_boost_algorithm.png)
 ![alt text](02-Advanced-Learning-Algorithms/images/day39_gradient_boosting.png)

- Comparision between Adaboost and Gradietn boost:
![alt text](02-Advanced-Learning-Algorithms/images/day39_adaboost_vs_gradientboost.jpg)


 > boosting + gradients (gradients = direction to minimize error).

``` 
pseudo-  residual = actual - predicted
new prediction = old prediction + (learning_rate √ó residual)
```


- Practice:  [Notebook: gradient boosting regressor using scikit learn](02-Advanced-Learning-Algorithms/code/day39_gradient_boosting_regressor.ipynb)

#### gradient boosting variations:
- xgboost: uses regularization and handles missing data efficiently.
- lightgbm: faster with large datasets.
- catboost: great for categorical data.

```python
    gb = GradientBoostingRegressor(
        n_estimators=100,      # number of trees
        learning_rate=0.1,     # step size for updates
        max_depth=3,           # depth of each tree
        min_samples_split=2,   # min samples to split a node
        min_samples_leaf=1,    # min samples per leaf
        subsample=1.0,         # fraction of samples per tree
        max_features=None,     # use all features
        random_state=42        # ensures reproducibility
    )
```

Notes:
![alt text](02-Advanced-Learning-Algorithms/images/day39_notes.jpg)


---
# Day 40: Gradient Boosting with Classification

## Overview
- Gradient boosting improves classification by minimizing log-loss iteratively.
- Each tree focuses on correcting errors made by previous trees.
- Uses gradients of the loss function to guide updates.

#### Algorithm
1. Initialize predictions: `F0(x)` (log-odds for classification).
2. For each iteration:
   - Compute pseudo-residuals:  
     `ri = - ‚àÇ(loss) / ‚àÇF(xi)`
   - Fit a weak learner (tree) to `ri`.
   - Update predictions:  
     `F(x) = F(x) + Œ∑ * h(x)`  
     where `Œ∑` is the learning rate.

#### Loss Function
- Binary classification: log-loss.  
  `Loss = -[y log(p) + (1 - y) log(1 - p)]`
- Multiclass classification: softmax loss.
```python
gb = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)
```

Notes:
![alt text](02-Advanced-Learning-Algorithms/images/day40_notes.jpg)

[Notebook: Gradient Boosting Classification](02-Advanced-Learning-Algorithms/code/day40_gradient_boosting_classification.ipynb)

[Blog Link](https://towardsdatascience.com/all-you-need-to-know-about-gradient-boosting-algorithm-part-2-classification-d3ed8f56541e)

Visuals:
![Visualizations](02-Advanced-Learning-Algorithms/images/day40_geometric_intution1.png)
![Visualizations](02-Advanced-Learning-Algorithms/images/day40_geometric_intution2.png) 


---
# Day 41: Variations of Gradient Boosting: XGBoost - Introduction.

- **xgboost** 
- **lightgbm** 
- **catboost**

#### XGBoost:
![alt text](02-Advanced-Learning-Algorithms/images/day41_xgboost_overview.png)
xgboost (eXtreme Gradient Boosting) is an advanced implementation of gradient boosting that addresses some key limitations in traditional gradient boosting and adaboost.
here's a quick rundown of what i've learned so far.
![Benchmark](02-Advanced-Learning-Algorithms/images/day41_benchmarkPerformance.png)
- Flexibility
    - cross-platform support means xgboost works on different operating systems without much hassle.
    - it supports multiple languages like python, c++, and r, which makes it easy to integrate with your preferred tech stack.
    - integrating with other libraries like scikit-learn or spark is a breeze.
    - it can handle all kinds of ml problems ‚Äî whether it's classification, regression, or ranking.

- Speed
    - parallel processing is key. it uses all your cores to speed things up, so you get faster results.
    - optimized data structures (like DMatrix) help reduce memory usage and make computations more efficient.
    - it‚Äôs cache-aware, so it knows how to make use of your cpu cache and speed up data retrieval.
    - xgboost handles datasets larger than memory through out-of-core computing.
    - distributed computing helps when your dataset is huge and you need to scale your work across multiple machines.
    - with gpu support, xgboost accelerates matrix calculations, making it much faster for large datasets.

- Performance (Why this is different from other algos??)
    - regularization is a big win. it prevents overfitting with L1 and L2 regularization, keeping your model generalizable.
    - it automatically handles missing values, learning the best way to fill them in.
    - sparsity-aware split finding is great for sparse data (think text data or categorical features).
    - finding efficient splits in trees is another xgboost feature that speeds up training without compromising accuracy.
    - tree pruning helps reduce tree size after training, improving the model‚Äôs performance on unseen data.


---
# Day 42: XGBoost for Regression and Classification, Catboost Vs XGboost Vs LightGBM

Notes on what i explored:
![Notes](02-Advanced-Learning-Algorithms/images/day42_notes1.jpg) 
![Notes](02-Advanced-Learning-Algorithms/images/day42_notes2.jpg) 
![Notes](02-Advanced-Learning-Algorithms/images/day42_notes3.jpg)


1. What if we have two or more than two features?
We can scan all features , and do all possible splits for all features, then we will calculate gain and similarity score , and select feature which has max gain, algo use greedy search

2. What if multiple feature and second feature is categorical (like binary: yes/no, male/female or muticlass: colors)?
You have to encode them using OHE or other. or use othre variants of GDboost. idenntify unique values in catagorical columns , and consider both as a potential split point ., then calculate gain and similarity scores ., and select with maximum gain .

2. What if the feature is binary categorical or multiclass categorical?
older versions: encode them (e.g., one-hot, label, or target encoding).
newer versions: native support for categorical features‚Äîno encoding needed.



#### Catboost Vs LightGBM Vs XGBoost:
![alt text](02-Advanced-Learning-Algorithms/images/day42_catboostVsLightGbmvsXGboost.png)
- **categorical features**
    - **catboost** handles them natively‚Äîno encoding needed.
    - **lightgbm/xgboost** require encoding, but xgboost supports label encoding in newer versions.
- **speed**
    - **lightgbm** is the fastest, especially for large datasets.
    - **catboost** is slower but optimized for small/mid datasets.
    - **xgboost** is slower than both due to its exhaustive computations.
- **memory usage**
    - **lightgbm** uses the least memory.
    - **catboost** and **xgboost** consume more, especially xgboost for large datasets.
- **overfitting prevention**
    - all three handle overfitting well, but **catboost** excels in datasets prone to overfitting due to ordered boosting.
- **use case**
    - **catboost**: small/mid datasets with many categorical features.
    - **lightgbm**: large datasets where speed and memory are critical.
    - **xgboost**: general-purpose, robust across various dataset types.

---
# Day 43: Stacking Ensemble: Understanding Blending and K-fold methods
~ blending: splits the data into a training set and a holdout set to train base models and then a meta-model on the predictions of the base models.~

~ k-fold stacking: uses cross-validation to generate predictions for the meta-model by training base models on different training folds and predicting on the validation fold.
~

![Notes:](02-Advanced-Learning-Algorithms/images/day43_notes.jpg) 
![Notes:](02-Advanced-Learning-Algorithms/images/day43_notes2.jpg)

#### !Hyperparameters Tuning:
- identify hyperparameters:
for decision trees: max_depth, min_samples_split, etc.
for neural networks: learning rate, batch size, number of layers, etc.
- choose a tuning method:
grid search: try every combination of parameters (computationally expensive).
random search: test random combinations (faster).
bayesian optimization: automatically find the best parameters using probabilistic methods.
automated tuning: libraries like optuna or hyperopt.
- cross-validate:
use k-fold cross-validation to test parameter combinations.
- pick the best:
finalize hyperparameters that minimize error metrics or maximize accuracy.
![alt text](02-Advanced-Learning-Algorithms/images/day43_hyperparameter_and_models.png)

#### 1. manual tuning

- tweak hyperparameters based on experience or trial-and-error.
- works for simple models but not scalable.

#### 2. grid search

- systematically tests all combinations of hyperparameter values.
- pros: exhaustive, finds the best combo (if time permits).
- cons: computationally expensive, impractical for large spaces.
- example:
    
    ```python
    from sklearn.model_selection import GridSearchCV
    param_grid = {'max_depth': [3, 5, 10], 'min_samples_split': [2, 5, 10]}
    grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)
    grid_search.fit(X_train, y_train)
    print(grid_search.best_params_)
    
    ```
    

#### 3. random search

- samples random combinations of hyperparameters.
- pros: faster, effective for large search spaces.
- cons: might miss optimal combinations.
- example:
    
    ```python
    from sklearn.model_selection import RandomizedSearchCV
    from scipy.stats import randint
    param_dist = {'max_depth': randint(3, 20), 'min_samples_split': randint(2, 20)}
    random_search = RandomizedSearchCV(DecisionTreeClassifier(), param_dist, n_iter=100, cv=5)
    random_search.fit(X_train, y_train)
    print(random_search.best_params_)
    
    ```

#### 4. bayesian optimization

- predicts the best hyperparameters using probabilistic models (e.g., Gaussian processes).
- pros: efficient in high-dimensional spaces.
- cons: complex implementation.
- libraries: `Optuna`, `Hyperopt`, `BayesSearchCV`.

#### 5. evolutionary algorithms

- optimizes hyperparameters using natural selection (e.g., genetic algorithms).
- library: `TPOT`.

#### 6. automated tools

- `auto-sklearn`: automates model selection + hyperparameter tuning.
- `H2O.ai`: distributed hyperparameter tuning.
- `Optuna`: fast, user-friendly library for hyperparameter search.

[Link to the blogpost](https://www.analytixlabs.co.in/blog/what-are-hyperparameters/#What_is_a_Hyperparameter)

---
# Day 44: K Nearest Neighbor, Coding KNN from scratch and applying on different datasets:
Predictions are based on the majority vote (classification) or average (regression) of the K closest data points in the training set.

#### **Step-by-Step Workflow**:

1. **Choose K**: Number of neighbors to consider.
2. **Calculate Distance**:
    - Common metrics:¬†**Euclidean**¬†(default), Manhattan, or Minkowski.
3. **Find K Nearest Neighbors**: Identify the K points closest to the query.
4. **Make Prediction**:
    - **Classification**: Majority class among neighbors.
    - **Regression**: Average value of neighbors.

#### **3. Choosing the Right K**

- **Small K**¬†(e.g., K=1): High variance, sensitive to noise (overfitting).
- **Large K**¬†(e.g., K=50): High bias, smoother boundaries (underfitting).
- **Rule of Thumb**: Start with¬†K=n*K*=*n*¬†(where¬†n*n*¬†= number of samples) or use¬†**cross-validation**.

Overfitting and Underfitting:
![alt text](02-Advanced-Learning-Algorithms/images/day44_fitting.png) 
![alt text](02-Advanced-Learning-Algorithms/images/day44_overfits.png)

Code of KNN using Python:
![alt text](02-Advanced-Learning-Algorithms/images/day44_KNNclass.png) 

Finally After Hyperparameter tuning, KNN models imporves for California House price prediction:
![alt text](02-Advanced-Learning-Algorithms/images/day44_california_house_prediction_hyperparam_tuuning.png) 

Notes:
![Notes:](02-Advanced-Learning-Algorithms/images/day44_notes.jpg)

---
# Day 45: Support Vector Machines:

1. What is SVM?
Goal: SVM finds the "best" hyperplane to separate data into classes.

Key Idea: Maximize the margin (distance between the hyperplane and the nearest data points, called support vectors).

- svm helps classify data by finding the best dividing line (hyperplane).
- **support vectors**: closest points to the line that influence its position. kind of like the apples and oranges closest to the ruler.
- **margin**: the gap between the line and the nearest points. svm tries to make this as wide as possible for better separation.
    ![Svm](02-Advanced-Learning-Algorithms/images/day45_svm.png)
- **hard margin svm**: works only when data is clean and separable. not great for noisy or messy data.
    - **example**: classifying perfectly labeled "cat" vs. "dog" images where there‚Äôs no overlap.
- **soft margin svm**: allows some mistakes for better flexibility with noisy/overlapping data.
    - **example**: separating spam and non-spam emails, where some emails are hard to classify.
- **kernel trick**: useful when the data isn‚Äôt linearly separable. it projects data into a higher dimension to make it separable.
    - **example**: in handwriting recognition, svm can map curvy letters into a higher space to separate them more easily.

More Notes like optimization regularization:
![Notes](02-Advanced-Learning-Algorithms/images/day45_notes.jpg)


---
# Day 46: K-Means Clustering, DBSCAN

## K-Means (centeroid based):

#### **Algorithm Steps**

1. Initialize Centroids: Randomly select¬†**K data points**¬†as initial centroids(Can lead to suboptimal clusters.) (or use¬†**k-means++**(Distributes initial centroids to improve stability and speed)¬†for smarter initialization).
2. Assign Points to Clusters: For each data point, compute the¬†**Euclidean distance- used by default**¬†to all centroids. Assign the point to the¬†**nearest centroid**.
3. Recalculate Centroids: Compute the¬†**mean**¬†of all points in each cluster to update centroids.
4. Repeat: Reassign points and update centroids until:
    - Centroids stabilize (change < tolerance threshold).
    - Maximum iterations are reached.

![Kmeans](02-Advanced-Learning-Algorithms/images/day46_Kmeans_algo.gif)

#### **Applications**
- Customer Segmentation
- Image Compression
- Document Clustering


``` python
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Preprocess data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Fit K-means
kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10)
kmeans.fit(X_scaled)

# Get labels and centroids
labels = kmeans.labels_
centroids = scaler.inverse_transform(kmeans.cluster_centers_)
```


#### Choosing the Optimal K

1. Elbow Method: Plot¬†inertia vs. K; the "elbow" (point where inertia decline slows) suggests optimal K.
2. Silhouette Score: Measures how similar a point is to its cluster vs others.¬†Higher score (closer to 1) = better clustering.
3. Domain Knowledge: Use prior understanding of the data to guide K selection (e.g., customer segments in marketing).

[Notebook: K-Means clustering Demo](02-Advanced-Learning-Algorithms/code/day46_kmeans-clustering-demo.ipynb)

*One limitation of K-means is that you must specify the number of clusters beforehand.*

## DBSCAN (density based clustering)
[Visualize: DBSCAN here](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)

![Why DBSCAN](./03-Unsupervised-Learning-And-Reinforcement-Learning/images/day46_whyDBSCAN.png)

[Notebook: DBSCAN demo](./03-Unsupervised-Learning-And-Reinforcement-Learning/code/day46_dbscan_demo.ipynb)

Groups data into¬†density-based clusters¬†(arbitrary shapes) and¬†flags outliers.

- Core Point: Has ‚â•¬†`min_samples`¬†neighbors within radius¬†`eps`.
- Border Point: In a core point‚Äôs neighborhood but lacks enough neighbors.
- Noise: Neither core nor border.
![alt text](./03-Unsupervised-Learning-And-Reinforcement-Learning/images/day46_corebordernoise.png)

- `eps`: Use a¬†k-distance plot¬†(k =¬†`min_samples`) to find the ‚Äúknee‚Äù for optimal Œµ.
- `min_samples`: Start with¬†`2 * data dimensions`¬†(adjust for noise tolerance).


#### Steps

1. Pick Parameters:¬†`eps`¬†(radius) and¬†`min_samples`¬†(density threshold).
2. Expand Clusters:
    - For each unvisited point, check if it‚Äôs a¬†core point¬†(enough neighbors in¬†`eps`).
    - If yes, form a cluster by adding all¬†density-reachable¬†points (core neighbors + their neighbors).
    - Mark non-core reachable points as¬†border.
3. Label Noise: Points not assigned to any cluster.



| **Strengths** | **Weaknesses** |  |
| --- | --- | --- |
| Finds¬†**any cluster shape** | Struggles with¬†**varying densities** |  |
| **No need for K** | Sensitive to¬†**Œµ and min_samples** |  |
| **Robust to outliers** | Poor performance in¬†**high dimensions** |  |


#### When to Use?

- Data has¬†noise¬†or¬†complex shapes¬†(e.g., geospatial data, anomaly detection).
![dbscan vs k means](./03-Unsupervised-Learning-And-Reinforcement-Learning/images/day46_dbscan_vs_kmeans.png)
- Avoid if clusters have¬†highly varying densities¬†(use HDBSCAN instead).

---
# Day 47: Hierarchical clustering, Silhouette score

#### Hierarchical Clustering
Hierarchical clustering builds a tree-like hierarchy (*dendrogram* - a tree diagram showing how clusters merge/split. height shows the distance of merging, and cutting at a height defines the number of clusters.) of clusters. Two main approaches:
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day47_hierarchcal.gif)
1. Agglomerative (Given below):¬†Start with each point as its own cluster, iteratively merge closest clusters.
2. Divisive (Reverse of Agglomerative):¬†Start with all points in one cluster, recursively split into smaller clusters.

#### Agglomerative Clustering Steps:
1. Initialize:¬†Treat each data point as a singleton cluster.
2. Compute Distance Matrix:¬†Measure pairwise distances (e.g., Euclidean, Manhattan).
3. Merge Clusters:¬†Combine the two closest clusters; update the distance matrix.
4. Repeat:¬†Continue merging until one cluster remains.

| **Pros** | **Cons** |
| --- | --- |
| No need to specify cluster count upfront | Computationally heavy (*O(n¬≥)*¬†time,¬†*O(n¬≤)*¬†space) |
| Dendrograms aid visualization | Sensitive to noise/outliers |
| Works well for small/mid-sized data | Merges are irreversible (local optima) |

#### Linkage Criteria
Defines how distances between clusters are calculated:
- Single Linkage: Minimum distance between clusters (prone to "chaining").
- Complete Linkage: Maximum distance (creates compact clusters).
- Average Linkage: Average distance between all pairs.
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day47_linkage_criteria.png)
- Ward‚Äôs Method: Minimizes variance when merging (minimizes total within-cluster variance).


#### Silhouette Score
A metric to evaluate clustering quality by measuring how similar a data point is to its own cluster (cohesion) compared to other clusters (separation).
![silhouettescore](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day47_silhouettescore.png)

- Range:  ‚àí1 (poor clustering) to  +1 (well-defined clusters).

![Notes](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day47_notes.jpg)

---
# Day 48: Dimensionality Reduction

Reduces the number of features in data while retaining meaningful patterns, addressing noise, computational cost, and visualization. Methods are hierarchically grouped into feature selection (keeping relevant features) and feature extraction (creating new features).

Dimensionality Redduction Hierarchy:
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day48_hierarchy_DR.png)
Feature Selection:
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day48_feature_selection.png) 


Dimensionality Reduction of a Data:
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day48_dimensionality_reduction_of_data.png) 

- Curse of Dimensionality: The curse of dimensionality refers to the challenges and inefficiencies that arise when analyzing data in high-dimensional spaces, such as sparsity, computational complexity, and loss of meaningful patterns.

![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day48_curseofdimensionality.png)
fig: As the dimensionality of data increases, the feature space becomes sparser, and the data is easier to separate. This is the curse of dimensionality in a nutshell.

![Notes: ](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day48_notes.jpg)
---
# Day 49: PCA (Principle Component analysis), Implementing with MNIST dataset

Goal: reduce dimensions while preserving maximum variance.

fig: pca projecting 2d data into 1d pc.
![project illustration](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day49_PCA_Projection_Illustration.gif)

Example: Reducing 10D data to 2D ‚Üí Use top 2 eigenvectors (highest eigenvalues).
#### Key Concepts
- Variance: Spread of data along a feature.
- Covariance: Measures how two variables vary together.
- Eigenvectors: Directions of maximum variance (principal components).
- Eigenvalues: Magnitude of variance along eigenvectors.
- Transformation: Project data onto new axes (eigenvectors) to reduce dimensions.

 #### Steps ‚Üí
![Steps PCA](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day49_steps_pca.png)

How to choose the Optimal PC??
- select the optimal number of pcs by checking the cumulative explained variance, aiming to retain 90%-95% of the total variance, or by using the elbow method on the explained variance plot where additional pcs add minimal value
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day49_choosing_best_pc.png)
Notes: 
![Notes](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day49_notes.jpg)

#### Visualizing MNIST dataset on 2D and 3D:
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day49_mnist_dataset_2d.png) 
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day49_mnist_dataset_3d.png)

[Notebook: Applying PCA on MNIST dataset](03-Unsupervised-Learning-And-Reinforcement-Learning/code/day49_pca_on_mnist.ipynb)

Conclusion: 
With about 100 PCs, our model predicts an accuracy of approximately 96%. In comparison, other models like KNN predict around 97% because KNN can capture more complex patterns in the data.


# Day 50: Visualizing and Comparing PCA, t-SNE, UMAP, and LDA + Revision with the course ML specialization:

Today was a bit hectic as I tried to understand and visualize various dimensionality reduction techniques. Here's a summary of my learnings and comparisons:

### Final Thoughts
- **t-SNE**: Captures local similarities well but sometimes distorts the global structure.
    ![t-SNE Visuals](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day50_t-SNE_visuals.gif)
- **UMAP**: Faster than t-SNE and preserves both local and global relationships better.
- **LDA**: Supervised technique, works best when class separation is important.

### Comparisons
![Comparisons](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day50_comparisions_pcs_tsne_umap_lda.png)

We explored four dimensionality reduction techniques for data visualization: PCA, t-SNE, UMAP, and LDA. We used them to visualize a high-dimensional dataset in 2D and 3D plots.

**Note**: It's easy to fall into the trap of considering one technique better than the other. At the end of the day, there's no perfect way to map high-dimensional data into low dimensions while preserving the entire structure. There's always a trade-off in the qualities each technique offers.

### Resources
- [Notebook: 3D Visualizations of PCA, t-SNE, UMAP, LDA](03-Unsupervised-Learning-And-Reinforcement-Learning/code/day50_pca_tsne_umap_lda_visualization.ipynb)
- [Article: Dimensionality Reduction for Data Visualization: PCA vs t-SNE vs UMAP](https://medium.com/towards-data-science/dimensionality-reduction-for-data-visualization-pca-vs-tsne-vs-umap-be4aa7b1cb29)

#### **Comparison of Results on MNIST**

| **Technique** | **Local Structure** | **Global Structure** | **Supervised?** | **Example Result** |
| --- | --- | --- | --- | --- |
| t-SNE | Preserved | Not preserved | No | Tight clusters of "2"s and "7"s, but arbitrary spacing between clusters. |
| UMAP | Preserved | Partially preserved | No | Tight clusters of "2"s and "7"s, with meaningful spacing between clusters. |
| LDA | Preserved | Preserved (class separation) | Yes | Distinct groups for each digit, optimized for classification. |


The learning is so hectic today, so I decided to revise the concepts we studied today with the help of Andrew NG. Here's the revision:

1. What is clustering?
    - Clustering is grouping data points into clusters where points in the same cluster are more similar to each other than to those in other clusters.
2. K-means?
    - K-means is a clustering algorithm that partitions data into K clusters by minimizing the distance between data points and their respective cluster centroids.
3. optimization objective?
    - The goal is to minimize the sum of squared distances between data points and their nearest cluster centroid.
4. Lab?? done// [Notebook: Lab Assignment](03-Unsupervised-Learning-And-Reinforcement-Learning/code/day50_C3_W1_KMeans_Assignment.ipynb)


---
# Day 51: Anomaly Detection:

#### What is Anomaly Detection?

- Identifying rare data points (anomalies) that deviate significantly from the majority of the data.
- Fraud detection, system failure prediction, intrusion detection, healthcare monitoring.
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day51_anomaly_detection_example.png)

#### Approaches for Anomaly Detection:

- Gaussian Distribution: Flag data points outside ¬±3œÉ (99.7% of data).
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day51_gaussian_distribution.png)
- Z-Score:¬†Z=(x‚àíŒº)œÉ*Z*=*œÉ*(*x*‚àí*Œº*); if¬†‚à£Z‚à£>3‚à£*Z*‚à£>3, mark as anomaly.
1. Unsupervised:
    - Clustering (K-means, DBSCAN): Anomalies lie far from cluster centers.
    - Isolation Forest: Randomly splits data; anomalies are easier to isolate.
    - Autoencoders: Neural networks that reconstruct input. High reconstruction error = anomaly.

### Evaluation Metrics
- Precision: % of detected anomalies that are real.
- Recall: % of true anomalies detected.
- F1-Score: Harmonic mean of precision and recall.
- ROC-AUC: Measures trade-off between TPR (recall) and FPR.

#### Choose Algorithm:
- Small data? Use statistical methods (Z-score).
- Large data? Try Isolation Forest or Autoencoders.

### Notes:
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day51_density_estimation.png)
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day51_classified_anomalies.png)
- Assuming data is normally distributed.
- Ignoring temporal/spatial context (e.g., seasonal trends).
- Not updating models as data evolves.

# Day 52: collaborative filtering

Best Article (Working behind CF): https://medium.com/@ashmi_banerjee/understanding-collaborative-filtering-f1f496c673fd


Collaborative Filtering is a recommendation algorithm that considers the similarities between different users when recommending an item to another user.

#### Making Recommendations
- Predict what a user might like based on similar users/items.
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day52_userbased_itembased.png)
1. User-User CF: Find users like you ‚Üí recommend what they liked.
2. Item-Item CF: Find items similar to what you liked ‚Üí recommend those.

the approach minimizes a regularized cost function using gradient descent, adjusting user and item parameters iteratively. the learning rate (Œ±) controls step size, balancing convergence speed and stability.
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day52_LossFuncitonin_CF.png)

while effective, it suffers from the cold start problem (new users/items lack data) and sparsity issues (many missing ratings). despite these challenges, it remains a widely used technique in recommender systems.

#### Mean Normalization
- Why?¬†Handle users who rate everything too high/low.


#### Collaborative Filtering vs Content-Based Filtering
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day52_differencces.png)
| **CF** | **Content-Based** |
| --- | --- |
| Uses user-item interactions | Uses item features (e.g., text, genre) |
| Example: Netflix recommendations | Example: News articles recommended based on text keywords |


----
# Day 53: Project @ Football Players Market Value Prediction - Introduction and Planning
Inspired by the work of [Youla Sozen](https://github.com/youlasozen/predicting-the-Market-Value-of-Footballers)

Every project starts with a problem or question. However, this project is different. it's all about having fun. As a football enthusiast, creating these kinds of projects is always enjoyable. The plan is straightforward, and I will implement it step by step.

### Project plan

Although this is a fun project, i aim to ensure the following:

- **accurate player valuation**: estimating market values to benefit clubs, agents, and investors.
- **transfer market efficiency**: preventing overpayment, aiding negotiation, and optimizing resource allocation.
- **risk assessment & player development**: evaluating investments while identifying young talents with high growth potential.
- **data-driven insights**: supporting fairer contract negotiations and improving decision-making in fantasy football & betting.

this is a future plan, and i will work towards achieving these goals in the coming days.

![Plan of Project](04-ML-Based-Football-Players-Market-Value-Prediction/images/project_plan.png)

#### Tips for the project (crafted with deepseek):
![Tips](<04-ML-Based-Football-Players-Market-Value-Prediction/images/Tips for the project.jpg>)

----
# Day 54: Project @ Football Players Market Value Prediction - Collecting Data (Scraping)
web scraping is a technique to collect data from the internet and convert it into a meaningful format, like a data frame, when direct downloads aren't available. in this project, i used the sofifa dataset. here's the main page of sofifa
![main page](04-ML-Based-Football-Players-Market-Value-Prediction/images/day54_sofifa_mainpage.png)

Code to scrape data:
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day54_data_scraping.png)

Plan for cleaning Data:
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day54_data_cleaning_plan.png) 

----
# Day 55: Project @ Football Players Market Value Prediction - Cleaning Data

just finished a major data cleaning session for my project. went through steps like handling missing values, converting currencies, splitting combined columns (height/weight), and ensuring consistent data types. cleaned up outliers, removed duplicates, and made sure everything‚Äôs ready for the next phase: EDA and model training. feeling good with the progress üòÅ

Here's a final look:
[Notebook: Data Cleaning](04-ML-Based-Football-Players-Market-Value-Prediction/notebooks/cleaning.ipynb)

Code:
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day55_data_cleaning.png)

# Day 56: Project @ Football Players Market Value Prediction - EDA

[Notebook: EDA (With Complete Documentation)](04-ML-Based-Football-Players-Market-Value-Prediction/notebooks/eda.ipynb)

I have mostly used Plotly to visualize as it is interactive and for such beginners like me, the visualization impact is powerful. as well as the codes are also easy to write.

### ***overall dataset insights***
1. ***what are the top 10 most valuable players?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_top10valueableplayers.png)
2. ***how does market value vary by position (e.g., are strikers more expensive than defenders)?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_market_value_with_position.png)
3. ***which teams have the highest average market value?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_correlation_attributes_marketValue.png)
4. ***what‚Äôs the distribution of market values (is it skewed towards a few expensive players)?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_dist_of_marketvalue.png)
5. ***how does age correlate with market value (are younger players generally worth more)?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_value_age_potential.png)

### ***player attributes vs. market value***

1. ***how does a player‚Äôs overall rating affect their market value?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_overall_rating_vs_market_value.png)
2. ***which individual attributes (e.g., pace, stamina, strength) correlate the most with market value?***
3. ***does international reputation (1-5 stars) impact market value?***
4. ***how do potential ratings compare to market value (are high-potential players priced higher)?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_marketvaleue_with_reaction_deribling.png)
5. ***do physical attributes (height, weight, strength) play a role in market value?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_strength_vs_marketvalue.png)

### ***position-specific insights***

1. ***are attacking midfielders (CAM/CM) more valuable than defensive midfielders (CDM/CM)?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_position_specific_insights.png)
2. ***how does pace affect wingers' (LW/RW) market value?***
3. ***do goalkeepers follow the same market trends as outfield players?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_goalkeeper.png)

### ***contract & transfer market impact***

1. ***does a player's contract end year affect their market value (e.g., do players with 1 year left have lower values)?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_contract_end_year.png)
2. ***are players on loan priced differently compared to permanent squad members?***



# Day 57: Project @ Football Players Market Value Prediction - Feature Engineering: (Creating features, Transforming Features)

[Notebook: Creating and Transforming Features](04-ML-Based-Football-Players-Market-Value-Prediction/notebooks/feature_engineering.ipynb)

spent 6+ hours experimenting with feature engineering. ran into some challenges, but made progress:

1. **position-based features**: grouped players into categories (attackers, midfielders, defenders, goalkeepers) with scores. will refine tomorrow.
2. **club-based features**: switched from one-hot encoding (curse of dimensionality) to target encoding using the mean market value for each club.
3. **contract-based features**: correlation with market value was neutral. most features, except age, overall, and potential, seemed less important.

![pairplot](04-ML-Based-Football-Players-Market-Value-Prediction/images/day57_pairplot_for_important_relationship.png)

![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day57_scores_fe.png)

---
# Day 58: Project @ Football Players Market Value Prediction - ML : (Linear Regression with Refined Features and deploying with Streamlit)
Just for fun:
![fun](04-ML-Based-Football-Players-Market-Value-Prediction/images/day58_fun.png)

Pairplots: 
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day58_pairplots.png)

So, before diving into feature engineering after cleaning the data, i tried out linear regression and got an r2 score of around 0.52. then, after applying some feature engineering and playing around with features, i ran the same model and got the r2 score up to 0.96 with only numerical features
<br>
today, i wasn‚Äôt fully happy with the result and got confused about feature selection and engineering. so i decided to convert all features into numerical, applied scaling and transformation, and reran the model , r2 score shot up to 0.97
<br>
Saved the model immediately, then tried deploying it with streamlit locally, with a user input form and inverse transformations (MOST HATED PART) . while the model‚Äôs still a work in progress and not perfect, i'm proud of what i‚Äôve learned so far. next steps are all about finding the best model and getting it deployed with some solid predictions and managing the form with the backend properly. 

![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day58_r2score.png)

Streamlit Preview: https://www.linkedin.com/posts/paudelsamir_day-58365-linear-regression-with-refined-activity-7294398498777501697-2vWi?utm_source=share&utm_medium=member_desktop

----
# Day 59: Project @ Football Players Market Value Prediction - Complete Streamlit Setup for our first Model - Linear Regression

Yesterday, I set up streamlit for my project with some help from ai tools. deploiyng isn't my strong suit, and it got pretty hectic trying to nail down the format and inputs and converting to model inputs. had to leave it unclear

<br>
So todayt's goal was get the ui sorted and code the input transformation for the model within streamlit. i could've tinkered with other models and tuned them, but finishing what i started felt right. a simple linear regression is doing surprisingly well for my needs. of course, i'll explore other algorithms and fine tune for better accuracy soon. planning to scale up from 5,000 to 20,000 rows in my dataset. let's see

<br>
Here's the demo where linear regression predicts player market values quite accurately. grabbed data from the site i scraped so that to visuailze properly. and its around 90 percent accurate for all the positions. that's already great !! Loving it

Here are some previewsL:

- KDB (Real Vs Predicted)
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day59_kdb_real.png) 
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day59_kdb_predicted.png)

- Lamine (Real Vs Predicted)
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day59_lamine_real.png) 
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day59_yamal_predicted.png)

-  Oblak (Real Vs Predicted)
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day59_oblak_real.png) 
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day59_oblak_predicted.png)

---
# Day 60: Project @ Football Players Market Value Prediction - Testing Ridge, Lasso, and Decision Trees



Today was fun! Started by handling outliers for the linear regression model, but didn‚Äôt see any improvement, so no luck there. Then I dove into applying PCA for dimensionality reduction. After converting everything to numerical features and applying all the feature engineering, I reduced the features from 50 to 40, and guess what? Model accuracy jumped to 99%! But here‚Äôs the twist, I can‚Äôt use this model for my project since I‚Äôm limited with deployment knowledge, and reverse transforming features while predicting is still the most hectic part of the process.

![Applying PCA with 40 features](04-ML-Based-Football-Players-Market-Value-Prediction/images/day60_applying_pca_with_40features.png)

Then I tried Ridge and Lasso regression. Ridge performed the best and outperformed Linear and Lasso, so I‚Äôll stick with Ridge for now until a simpler model comes along.

![Ridge Regression](04-ML-Based-Football-Players-Market-Value-Prediction/images/day60_Ridge_regression.png)
![Lasso Regression](04-ML-Based-Football-Players-Market-Value-Prediction/images/day60_lasso_regression.png)

Next up was Decision Tree Regressor. Applied it, and without hyperparameter tuning, I got around a 0.98 R2 score. I know Decision Trees are prone to overfitting, so I visualized, but couldn‚Äôt predict by myself. Decided to try hyperparameter tuning, but the results weren‚Äôt drastically different.

![Decision Tree Regressor](04-ML-Based-Football-Players-Market-Value-Prediction/images/day60_decision_tree_regressor.png)

At this point, the Decision Tree is the best model for the project. Let's see what‚Äôs coming next. Good luck, city. ùêíùê°ùêÆùêØùêöùê´ùêöùê≠ùê´ùê¢ üåô
![GridSearchCV Decision Tree](04-ML-Based-Football-Players-Market-Value-Prediction/images/day60_gridsearchcv_dt.png)

![Decision Tree Insights](04-ML-Based-Football-Players-Market-Value-Prediction/images/day60_decision_tree_insights.png)

Did i just wasted 2 hours?? üòÖüòÖ![Fun](04-ML-Based-Football-Players-Market-Value-Prediction/images/day60_fun.png)

[Notebook: Experimentation 1](04-ML-Based-Football-Players-Market-Value-Prediction/notebooks/experimentation_1.ipynb)

---
# Day 61: Project @ Football Players Market Value Prediction -Had to hit reset from Feature Engineering

<!-- > Today's goal is:
1. Apply Bagging (Random Forest with Multiple Decision Tree Regressor ) - evaluate performance and check if we can deploy it or not
2. Apply Boosting Algorithms: XGboost, LightGBM, AdaBoost and compare model performances
3. Apply stacking Random Forest, XGBoost, and LightGBM, with a simple model like Linear Regression as the final estimator.
4. Documentation, deploy the best and simple model and Finalize the Project.
 -->

Just 20 minutes ago, i realized i‚Äôve been making a huge mistake since day 4 with feature engineering. i found out today that as a beginner, it‚Äôs easy to mess up, but it's all part of the learning process. the mistkae was thinking about how to transform features back for deployment without realizing that features like overall rating, best oiverall, and potential are actually super correlated with market value. i was happy with the 99% accuracy, but i didn‚Äôt see the problem until now. i knew about overfitting can cause it and tried to fix it, but i never thought about visualizing feature importance and how it can affect the model.

<br>
So, iwas almost done with the project and deployed it to my local server using streamlit. feeling soooo dumb at the moment. the day before yesterday, i tested it with real players like KDB, Lamine Yamal, and Oblak, and ùê≠ùê°ùêû ùê©ùê´ùêûùêùùê¢ùêúùê≠ùê¢ùê®ùêßùê¨ ùê•ùê®ùê®ùê§ùêûùêù ùê†ùê®ùê®ùêù. ùê∞ùê°ùê≤?? ùêõùêûùêúùêöùêÆùê¨ùêû ùê¢ùê≠ ùê≠ùê®ùê≠ùêöùê•ùê•ùê≤ ùê´ùêûùê•ùê≤ùê¢ùêßùê† ùê®ùêß ùê£ùêÆùê¨ùê≠ ùê≠ùê°ùê´ùêûùêû ùê¢ùêßùê©ùêÆùê≠ùê¨ ùêÅùêûùê¨ùê≠ ùêéùêØùêûùê´ùêöùê•ùê•, ùêéùêØùêûùê´ùêöùê•ùê• ùêëùêöùê≠ùê¢ùêßùê†, ùêöùêßùêù ùêèùê®ùê≠ùêûùêßùê≠ùê¢ùêöùê•. ùê≤ùê®ùêÆ ùêúùêöùêß ùêûùêØùêûùêß ùê©ùê´ùêûùêùùê¢ùêúùê≠ ùê∞ùê°ùê®ùê•ùêû ùê≠ùê°ùê¢ùêßùê†ùê¨ ùê∞ùê¢ùê≠ùê° ùê£ùêÆùê¨ùê≠ ùê≠ùê°ùêûùê¨ùêû ùê¢ùêßùê©ùêÆùê≠ùê¨. The impact of other features is literally minimal. now, i have to rebuild the model from scratch again with better feature engineering. 

<br>
Even after this dumbest mistake, total wasted grinding, wasted time, wasted energy---for the first time in my learning journey, it feels like now i'm actually learning something.

![Context](04-ML-Based-Football-Players-Market-Value-Prediction/images/day61_context.webp)


---
# Day 62: Project @ Football Players Market Value Prediction - Finalizing Project and Deploying it

Finally, i‚Äôm able to go live with my first end-to-end ml project! üéâ you can check it out here: https://paudelsamir.streamlit.app/


- Today was all about polishing things i messed up, after some solid feature engineering, i was able to hit 85% accuracy, pretty solid start. then, i played around a hour with hyperparameter tuning, which bumped it up to 89%.
<br>

![actual vs predicted](./04-ML-Based-Football-Players-Market-Value-Prediction/images/day61_actual_vs_predicted.png)

![alt text](./04-ML-Based-Football-Players-Market-Value-Prediction/images/day61_feature_importance.png)

but the real magic happened when i tried ensemble learning techniques. after a bit of back and forth, gradient boosting took me all the way to 94% accuracy. and will be using the same algorithm for deployment too.
<br>

And with that, after 10 days of nonstop grinding, i‚Äôm officially closing this project. it‚Äôs been a fun ride, full of learning and surprises !!

ùêÜùê¢ùê≠ùêáùêÆùêõ ùêëùêûùê©ùê® For the Project: https://github.com/paudelsamir/ML-Based-Football-Players-Market-Value-Prediction


# Day 63: Content-Based Movie Recommender System - Preprocessing
Today, I focused on the preprocessing phase of building a content-based movie recommender system. I created a `tags` feature by combining key keywords from columns like genres, descriptions, top 3 cast members, and crew, especially the director. This step was crucial to ensure that the recommendation engine has a rich set of features to work with.
![Notes:](04-ML-Based-Football-Players-Market-Value-Prediction/images/day63_notes.jpg)

---
# Day 64: Content-Based Movie Recommender System - Building and Deployment

Today, I built the recommendation engine based on movie content similarity using vectorization (bag of words). I also deployed it with Streamlit, so now you can input a movie name and get the top 5 similar movies based on the similarity matrix. Additionally, I integrated an API to pull movie posters in real-time from the website TMDB!

ùêÇùê°ùêûùêúùê§ ùê®ùêÆùê≠ ùê°ùêû ùê•ùê¢ùêØùêû ùêùùêûùê¶ùê® ùê°ùêûùê´ùêû: https://lnkd.in/d7R3Wsnk

---
# Day 65: Diving into Deep Learning


Explored deep learning concepts, including its significance, how it differs from machine learning, and whether it will replace ML. Covered key architectures like Feedforward Neural Networks (FNNs), Convolutional Neural Networks (CNNs) for image processing, Recurrent Neural Networks (RNNs) for sequential data, Autoencoders for feature learning, and Generative Adversarial Networks (GANs) for data generation.
Notes from the day:
![Notes](05-Artificial-Neural-Network-And-Improvement/images/day65_notes1.jpg) 
![Notes](05-Artificial-Neural-Network-And-Improvement/images/day65_notes2.jpg)
- Types of Neural network:
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day66_Types-of-Neural-Networks.png)


# Day 66: Perceptrons

Today, I dived into the concept of perceptrons, which are the building blocks of neural networks. I explored the perceptron algorithm, its working mechanism, and how it can be used for binary classification tasks. A supervised learning algorithm used for binary classifiers.

- Steps in Prceptron Algorithm:
  ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day66_notes.jpg)

- **Perceptron from Scratch**:
  ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day66_perceptron_scratch.png)

### Visuals:
- **Training Data**:
  ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day66_data.png)
- **Perceptron Training**:
  ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day66_after_perceptron_training.png)

I explored the fundamentals of MLOps with this paper: Machine Learning Operations (MLOps): Overview, Definition, and Architecture : https://arxiv.org/pdf/2205.02302

---
# Day 67: Perceptron Loss Function and Gradient Descent

Today, I explored the Perceptron Loss Function, which helps adjust weights when misclassification occurs, ensuring better decision boundaries. I learned how the perceptron updates its weights using the weight update rule and how Gradient Descent optimizes the loss function by iteratively moving in the direction of the negative gradient.

![alt text](05-Artificial-Neural-Network-And-Improvement/images/day67_train_perceptron.png) 
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day67_Gradient_Descent_Animation.gif)

Notes:
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day67_notes.jpg)


---
# Day 68: Multilayer Perceptron

The problem with Perceptrons lies in their limitation to learn complex patterns and functions, especially those that are not linearly separable. A Perceptron is a single-layer neural network with binary outputs, and it can only solve problems where the data points are linearly separable. If the data is not linearly separable, a Perceptron cannot converge and find a solution.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day68_xor.png)


So the solution is Multilayer Perceptron:
![GIF](05-Artificial-Neural-Network-And-Improvement/images/day68_multilayer_nn.gif)

There's a website named: https://playground.tensorflow.org/

I practiced different optimizations there some of them are,
- Adding nodes to hidden layer
- Adding nodes to input layer
- Adding nodes to output layer (for multicalss)
- Adding nuber of hidden layer

The conclusion is you can classify any type of problem within regression and classifcion by optimizing those nodes and others like activation and regularization.


### Batch and Gradient Descent:
number of samples processed before updating model weights

![alt text](05-Artificial-Neural-Network-And-Improvement/images/day68_batch_size.png)

1. SGD (batch size = 1) ‚Üí updates after each individual sample (one row at a time). noisy but good for escaping local minima.
2. Mini-batch gradient descent (batch size = 16, 32, 64, etc.) ‚Üí updates after a small group of samples (e.g., rows 1-100). balances speed and stability.
3. Full-batch gradient descent (batch size = all samples) ‚Üí updates after seeing the entire dataset. very stable but slow and memory-heavy.

each batch in mini-batch or full-batch contains multiple rows, and the loss is computed over those samples before updating weights.

so in SGD, you're updating weights after every single row (which makes it very random and noisy). in mini-batch, you take a chunk of rows, calculate gradients over that group, then update weights. in full-batch, you process all the rows at once and then update.

Notes:
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day68_notes.jpg)

---
# Day 69: MLP notation, Forward Propagation
today, i deepened my understanding of multi-layer perceptrons (MLPs), including their formal notation and the calculation of weights and biases for each layer. i also explored forward propagation and practiced matrix multiplication by manually constructing and multiplying matrices to intuitively follow the perceptron‚Äôs computations.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day69_notes.jpg)

additionally, i studied MLP training with pytorch from the book deep learning with python by fran√ßois chollet.

additionally, i explored Image processing with datacamp, here's image representation of what i learned today
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day69_bonus_imageprocessing.png)

---
# Day 70: Loss Functions for Deep Learning

- Regression:
    - MSE : squares errors, punishes big mistakes more.
    - MAE : takes absolute difference, treats all errors equally.
    - Huber Loss: mix of mse & mae, good for outliers.
    ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day70_regression.png)

- Classificaiton:
    - Binary cross entropy : for yes/no classification (spam or not spam).
    ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day70_binary_crosss.jpg)
    - Categorical Cross entropy : for multiple classes (yes/no/ maybe).
    ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day70_categorical.jpg)
    - Sparse Categorical cross entropy - same as categorical but works with integer labels.
    - Hinge Loss: used in SVMs, pushes correct class far from the wrong ones.
    ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day70_hinge_loss.png)

- Autoencoders / VAE loss:
    - KL divergence
- GANs:
    - Discriminator loss : helps the discriminator tell real from fake.
    - Minmax Loss : generator tries to fool discriminator by minimizing its best-case performance.
- Object Detection and segmentation loss:
    - interseciton over union loss
    - smoooth l1 loss
    - dice loss
- Reinforcement loss:
    - policy gradient - rewards good actions.
    - Q-Learning loss : teaches agent to choose best long-term rewards.
    - proximal policy optimization
- Custom losss function:
    - Perceptrual loss
    - combined loss functions: mix of different losses for better results (e.g., cross-entropy + dice loss).

Notes:

![alt text](05-Artificial-Neural-Network-And-Improvement/images/day70_notes1.jpg) 
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day70_notes2.jpg)

---
# Day 71


<div id="bottom"></div>
<div align="right">
  <a href="#top" target="_blank">
    <img src="https://img.shields.io/badge/Back%20to%20top-orange?style=for-the-badge&logo=expo&logoColor=white" style="height: 25px;" />
  </a>
</div>

<!-- 
### Helping Hands
- [Kris Naik - YouTube](https://www.youtube.com/channel/UCNU_lfiiWBdtULKOw6X0Dig)
- [Statquest - YouTube](https://www.youtube.com/c/joshstarmer)
- [GitHub Repos](https://github.com/paudelsamir?tab=stars)
- [Medium Articles](https://medium.com/)
- [Machine Learning Mastery](https://machinelearningmastery.com/)
- [Kaggle Datasets - Glossary](https://www.kaggle.com/code/shivamb/data-science-glossary-on-kaggle)
- [Arxiv.org](https://arxiv.org/)
 -->


