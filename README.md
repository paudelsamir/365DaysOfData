
<div id="top"></div>
<div align="left">
  <!-- Left Badges (Last Updated & Repo Size) -->
  <img src="https://img.shields.io/github/last-commit/paudelsamir/365DaysOfData" alt="Last Updated" />
  <img src="https://img.shields.io/github/repo-size/paudelsamir/365DaysOfData" alt="Repo Size" />
  
</div>

![cover](./resources/images/cover.png)

> [!Note] 
>I’ll be sharing updates regularly on [**LinkedIn**](https://www.linkedin.com/in/paudelsamir/) and [**Twitter**](https://twitter.com/samireey)

<div align="right">
<a href="#bottom">
    <img src="https://img.shields.io/badge/▼_Navigate_to_Bottom-0D1117?style=for-the-badge&logoColor=white&labelColor=161B22" alt="Navigate to Bottom" />
</a>
</div>

## Projects Completed
will update soon !
<!-- | Projects | Description | Streamlit Deployment |
|----------|-------------|-----------|
|[Football Players Market Value Prediction](https://github.com/paudelsamir/365DaysOfData/tree/main/04-ML-Based-Football-Players-Market-Value-Prediction)| A 10 day end-to-end machine learning capstone project involving data scraping, cleaning, feature engineering, model training, and deployment to a cloud server. |[Live Demo 👆🏽](https://paudelsamir.streamlit.app/)|
|[Movie Recommender System](https://github.com/paudelsamir/Movie-Recommender-System)|An end-to-end content-based movie recommender system leveraging a dataset of 5000 movies from [Kaggle](https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata?select=tmdb_5000_movies.csv).| [Live Demo 👆🏽](https://movie-recommender-samir.streamlit.app/)|
|[Cat vs Dog Classifier](https://github.com/paudelsamir/cat-vs-dog-classifier)| A deep learning model leveraging VGG16 architecture, trained on an RTX 3050 Ti for 30 epochs, achieving 95% accuracy using the [Kaggle Dogs vs Cats dataset](https://www.kaggle.com/c/dogs-vs-cats). | [Live Demo 👆🏽](https://cat-vs-dog-classifier.streamlit.app/) | -->

 

## Resources
will update soon !
<!-- | Books & Courses  | Completion Status |
|--------------------|-------------------|
| [Machine Learning Specialization @Coursera](https://www.coursera.org/specializations/machine-learning-introduction) | ✅ |
| [Machine Learning Playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH) | ✅ |
| [Intro to Deep Learning @MIT](https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI) | ✅  |
| [Grokking Deep Learning @Andrew W. Trask](https://www.amazon.com/Grokking-Deep-Learning-Andrew-Trask/dp/1617293709) | ⌛ |
| [Deep Learning Playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) | ⌛ |
| [Deep Learning with PyTorch @ Datacamp](https://app.datacamp.com/learn/courses/intermediate-deep-learning-with-pytorch) | ⌛  |
| [Deep Learning for Coders with fastai & PyTorch @Oreilly](https://github.com/fastai/fastbook) | ⌛ |

| [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https://github.com/yanshengjia/ml-road/blob/master/resources/Hands%20On%20Machine%20Learning%20with%20Scikit%20Learn%20and%20TensorFlow.pdf)| 🏊⌛|  -->


## Progress
| Days | Date               | Topics                      | Resources    |
|------|--------------------|-----------------------------|--------------|
| [Day1](#day-01-basics-of-linear-algebra) |2024‑12‑14 |Basics of Linear Algebra  | [3blue1brown](https://www.3blue1brown.com/topics/linear-algebra) |
| [Day2](#day-02-decomposition-derivation-integration-and-gradient-descent) | 2024-12-15 | Decomposition, Derivation, Integration, and Gradient Descent | [3blue1brown](https://www.3blue1brown.com/topics/calculus) |
| [Day3](#day-03-supervised-machine-learning-regression-and-classificaiton) | 2024-12-16 |Supervised Learning, Regression and classification|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) |
| [Day4](#day-04-unsupervised-learning-clustering-dimensionality-reduction) | 2024-12-17 |Unsupervised Learning: Clustering and dimensionality reduction|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) |
| [Day5](#day-05-univariate-linear-regression) | 2024-12-18|Univariate linear Regression|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) |
| [Day6](#day-06-cost-function) | 2024-12-19 |Cost Functions|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) |
| [Day7](#day-07-gradient-descent) | 2024-12-20 |Gradient Descent|[CampusX, ](https://www.youtube.com/watch?v=ORyfPJypKuU) [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) |
| [Day8](#day-08-effect-of-learning-rate-cost-function-and-data-on-gd) | 2024-12-21 |Effect of learning Rate, Cost function and Data on GD|[CampusX, ](https://www.youtube.com/watch?v=ORyfPJypKuU) [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) |
| [Day9](#day-09-linear-regression-with-multiple-features-vectorization) | 2024-12-22 |Linear Regression with multiple features, Vectorization| [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) |
| [Day10](#day10-feature-scaling) | 2024-12-23 |Feature Scaling, Visualization of Multiple Regression and Polynomial Regression| [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) |
| [Day11](#day-11-feature-engineering-and-polynomial-regression) | 2024-12-24 |Feature Engineering, Polynomial Regression| [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) |
| [Day12](#day-12-linear-regression-using-scikit-learn) | 2024-12-25 |Scikit-Learn revision, Linear Regression using Scikit Learn| [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) |
| [Day13](#day-13-classification) | 2024-12-26 |LR lab, Classification|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day14](#day-14-logistic-regression-sigmoid-function) | 2024-12-27 |Logistic Regression, Sigmoid Function|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) [, CampusX](https://www.youtube.com/watch?v=ABrrSwMYWSg&list=PLKnIA16_Rmvb-ZTsM1QS-tlwmlkeGSnru&index=6)|
| [Day15](#day-15-decision-boundary-cost-function) | 2024-12-28 |Decision Boundary, Cost Function|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) [, CampusX](https://www.youtube.com/watch?v=ABrrSwMYWSg&list=PLKnIA16_Rmvb-ZTsM1QS-tlwmlkeGSnru&index=6)|
| [Day16](#day-16-gradient-descent-for-logical-regression) | 2024-12-29 |Gradient Descent for logical regression|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) [, CampusX](https://www.youtube.com/watch?v=ABrrSwMYWSg&list=PLKnIA16_Rmvb-ZTsM1QS-tlwmlkeGSnru&index=6)|
| [Day17](#day-17-underfitting-overfitting) | 2024-12-30 |Underfitting, Overfitting, Regularization Polynomial Features, Hyperparameters|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day18](#day-18-neurons-layer-neural-netowrk-forward-propagation) | 2024-12-31 |Neurons, Neural Netowrk, Forward Propagation|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day19](#day-19-forward-propagation) | 2025-01-01 |Forward Propagation, Tensorflow implementations|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day20](#day-20-python-implementation-from-scratch) | 2025-01-02 |Building and comparing models (Binary Classification)|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day21](#day-21-vectorization-model-training) | 2025-01-03 |Vectorization, Model training using Tensoflow|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day22](#day-22-activation-functions-softmax) | 2025-01-04 |Activation Functions, Softmax Intution|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day23](#day-23-implementation-of-softmax-regression) | 2025-01-05 |Implementing Softmax|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day24](#day-24-backpropagation---what-and-how-) | 2025-01-06 |Backpropagaton, What and how??|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day25](#day-25-backpropagation---why-advices-for-applying-machine-learning) | 2025-01-07 |Backpropagation - Why? Advices for applying machine Learning|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day26](#day-26-model-selection-and-trainingcross-validationtest-sets-bias-and-variance) | 2025-01-08 |Model selection, training test, cross validation, Bias and Variance, Learning curves|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day27](#day-27-machine-learning-development-process-ml-workflow) | 2025-01-09 |Machine Learning Development Process, ML workflow|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day28](#day-28-machine-learning-model-error-analysis-and-transfer-learning) | 2025-01-10 |Implementing ML model: Error Analysis and Transfer Learning|[Notebook: Implementation, ](02-Advanced-Learning-Algorithms/code/day28_implementation.ipynb)[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day29](#day-29-error-metrices-encoding-of-categorical-data-transoformers) | 2025-01-11 |Error Metrices, Encoding of Categorical Data, Transoformers|[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction) [, CampusX](https://www.youtube.com/watch?v=8osKeShYVRQ&list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH&index=65)|
| [Day30](#day-30-scikit-learn-pipelines--ridge-regression-l2-regularization) | 2025-01-12 | Scikit-Learn Pipelines & Ridge Regression (L2 Regularization) |[Documentation: Scikit-Learn](https://scikit-learn.org/1.5/modules/linear_model.html)  [, CampusX](https://www.youtube.com/watch?v=8osKeShYVRQ&list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH&index=65)|
| [Day31](#day-31-lasso-regression-l1-regularization-elastic-net-regularization) | 2025-01-13 | Lasso Regression (L1 Regularization), Elastic Net Regularization |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day32](#day-32-decision-tree-entropy-and-information-gain) | 2025-01-14 | Decision Tree Emtropy and Information Gain |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day33](#day-33-hyperparameters-of-dt-with-sclearn-regression-trees) | 2025-01-15 | Hyperparameters of Decision Tree with Scikit Learn, Regression Trees |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH) [, Visualize Yourself>>](https://decisiontreeclassifier.streamlit.app/)|
| [Day34](#day-34-visualization-using-dtreeviz-ensemble-learning-overview) | 2025-01-16 | Visualization Using DtreeViz(), Ensemble Learning |[Github Repo: Dtreeviz, ](https://github.com/parrt/dtreeviz) [ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day35](#day-35-voting-ensemble--classification-and-regression) | 2025-01-17 | Voting Ensemble >> Classification and Regression |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH) [, Visualize Yourself](https://votingclassifier.streamlit.app/)|
| [Day36](#day-36-bagging-ensemble--classification-and-regression) | 2025-01-18 |  Bagging Ensemble > Classification and Regression |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day37](#day-37-random-forest-intution-working-and-difference-with-bagging-random-forest-hyperparameters) | 2025-01-19 | Random Forest: Intution, Working and difference with bagging, Random Forest Hyperparameters |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day38](#day-38-boosting-ensemble-adaboost-boosting) | 2025-01-20 |  Boosting Ensemble: Adaboost Boosting |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day39](#day-39-understanding-gradientboosting-with-regression) | 2025-01-21 |  Understanding GradientBoosting with Regression |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day40](#day-40-gradient-boosting-with-classification) | 2025-01-22 | Gradient Boosting with Classification|[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH) [, Vlog Link](https://towardsdatascience.com/all-you-need-to-know-about-gradient-boosting-algorithm-part-2-classification-d3ed8f56541e)|
| [Day41](#day-41-variations-of-gradient-boosting-xgboost---introduction) | 2025-01-23 | XGboost Introduction |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day42](#day-42-xgboost-for-regression-and-classification-catboost-vs-xgboost-vs-lightgbm) | 2025-01-24 | XGBoost for Regression and Classification, Catboost Vs XGboost Vs LightGBM |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH) [,Research Paper](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf)|
| [Day43](#day-43-stacking-ensemble-understanding-blending-and-k-fold-methods) | 2025-01-25 | Stacking Ensemble, Understanding Blending and K fold |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day44](#day-44-k-nearest-neighbor-coding-knn-from-scratch-and-applying-on-different-datasets) | 2025-01-26 | K-Nearest Neighbor, Coding KNN from Scratch |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day45](#day-45-support-vector-machines) | 2025-01-27 | Support Vector Machine  |[ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)|
| [Day46](#day-46-k-means-clustering-dbscan) | 2025-01-28 | K-Means Clustering, DBSCAN | [Notebook: K-Means clustering Demo](02-Advanced-Learning-Algorithms/code/day46_kmeans-clustering-demo.ipynb) [, Notebook: DBSCAN demo](02-Advanced-Learning-Algorithms/code/day46_dbscan_demo.ipynb) |
| [Day47](#day-47-hierarchical-clustering-silhouette-score) | 2025-01-29 | Hierarchical Clustering, Silhouette Score | [Kaggle, ](https://www.kaggle.com/) [ML playlist @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH) |
| [Day49](#day-49-pca-principle-component-analysis-implementing-with-mnist-dataset) | 2025-01-30 | PCA (Principle Component Analysis), Implementing with MNIST dataset | [Notebook: Applying PCA on MNIST dataset](03-Unsupervised-Learning-And-Reinforcement-Learning/code/day49_pca_on_mnist.ipynb) |
| [Day50](#day-50-visualizing-and-comparing-pca-t-sne-umap-and-lda--revision-with-the-course-ml-specialization) | 2025-02-01 | Visualizing and Comparing PCA, t-SNE, UMAP, and LDA + Revision with the course ML specialization | [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day51](#day-51-anomaly-detection) | 2025-02-02 | Anomaly Detection |[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)[, Notebook: Anomaly Detection](03-Unsupervised-Learning-And-Reinforcement-Learning/code/day51_anomaly_detection.ipynb) |
| [Day52](#day-52-collaborative-filtering) | 2025-02-03 | Collaborative Filtering |[Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction)|
| [Day53](#day-53-project--football-players-market-value-prediction---introduction-and-planning) | 2025-02-04 | Project @ Football Players Market Value Prediction - Introduction and Planning | [Project Plan](04-ML-Based-Football-Players-Market-Value-Prediction/images/project_plan.png) |
| [Day54](#day-54-project--football-players-market-value-prediction---collecting-data-scraping) | 2025-02-05 | Project @ Football Players Market Value Prediction - Collecting Data (Scraping) | [Notebook](04-ML-Based-Football-Players-Market-Value-Prediction/notebooks/scraping.ipynb) |
| [Day55](#day-55-project--football-players-market-value-prediction---cleaning-data) | 2025-02-06 | Project @ Football Players Market Value Prediction - Cleaning Data | [Notebook](04-ML-Based-Football-Players-Market-Value-Prediction/notebooks/cleaning.ipynb) |
| [Day56](#day-56-project--football-players-market-value-prediction---eda) | 2025-02-07 | Project @ Football Players Market Value Prediction - EDA | [Notebook](04-ML-Based-Football-Players-Market-Value-Prediction/notebooks/eda.ipynb) |
| [Day57](#day-57-project--football-players-market-value-prediction---feature-engineering-creating-features-transforming-features) | 2025-02-08 | Project @ Football Players Market Value Prediction - Feature Engineering: (Creating features, Transforming Features) | [Notebook](04-ML-Based-Football-Players-Market-Value-Prediction/notebooks/feature_engineering.ipynb) |
| [Day58](#day-58-project--football-players-market-value-prediction---ml--linear-regression-with-refined-features-and-deploying-with-streamlit) | 2025-02-09 | Project @ Football Players Market Value Prediction - ML: (Linear Regression with Refined Features and deploying with Streamlit) | [Notebook](04-ML-Based-Football-Players-Market-Value-Prediction/notebooks/feature_engineering_2.ipynb.ipynb) |
| [Day59](#day-59-project--football-players-market-value-prediction---complete-streamlit-setup-for-our-first-model---linear-regression) | 2025-02-10 |Project @ Complete Streamlit setup for Linear Regression | [Streamlit Documentation](https://docs.streamlit.io/) |
| [Day60](#day-60-project--football-players-market-value-prediction---testing-ridge-lasso-and-decision-trees) | 2025-02-11 |Project @  Testing Ridge, Lasso, and Decision Trees | [Project @ Football Players Market Value Prediction](https://paudelsamir.streamlit.app/) |
| [Day61](#day-61-project--football-players-market-value-prediction--had-to-hit-reset-from-feature-engineering) | 2025-02-12 |Project @ Had to hit reset from Feature Engineering | [Project @ Football Players Market Value Prediction](https://paudelsamir.streamlit.app/) |
| [Day62](#day-62-project--football-players-market-value-prediction---finalizing-project-and-deploying-it) | 2025-02-13 | Project @ Finalizing Project and Deploying it | [Project @ Football Players Market Value Prediction](https://paudelsamir.streamlit.app/) |
| [Day63](#day-63-content-based-movie-recommender-system---preprocessing) | 2025-02-14 | Content-Based Movie Recommender System - Preprocessing | [Notebook](05-Content-Based-Movie-Recommender-System/notebooks/preprocessing.ipynb) |
| [Day64](#day-64-content-based-movie-recommender-system---building-and-deployment) | 2025-02-15 | Content-Based Movie Recommender System - Building and Deployment | [Live Demo](https://movie-recommender-samir.streamlit.app/) |
| [Day65](#day-65-diving-into-deep-learning) | 2025-02-16 | Diving into Deep Learning | [Intro to Deep Learning @MIT](https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI) |
| [Day66](#day-66-perceptrons) | 2025-02-17 | Perceptrons | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day67](#day-67-perceptron-loss-function-and-gradient-descent) | 2025-02-18 | Perceptron, Loss function and gradient Descent | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) [, Grokking Deep Learning @Andrew W. Trask](https://edu.anarcho-copy.org/Algorithm/grokking-deep-learning.pdf) | |
| [Day68](#day-68-multilayer-perceptron) | 2025-02-19 | Multilayer Perceptron | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day69](#day-69-mlp-notation-forward-propagation) | 2025-02-20 | MLP notation, Forward Propagation | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day70](#day-70-loss-functions-for-deep-learning) | 2025-02-21 | Loss Functions for deep learning | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day71](#day-71-deep-diving-backpropagation) | 2025-02-22 | Backpropagation, deep diving this time | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day72](#day-72-implementing-backpropagation-for-regression) | 2025-02-23 | Implementing Backpropagation for Regression | [Notebook: Backpropagation Regression](05-Artificial-Neural-Network-And-Improvement/code/day72_backpropagation_regression.ipynb) |
| [Day73](#day-73-implementing-backpropagation-for-classification) | 2025-02-24 | Implementing Backpropagation for Classification | [Notebook: Implementation Backprop Classification](05-Artificial-Neural-Network-And-Improvement/code/day73_backpropagation_classification.ipynb) |
| [Day74](#day-74-revising-old-days-memoization) | 2025-02-25 | Revising old days, Memoization | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day75](#day-75-vanishing-gradient-exploding-gradient) | 2025-02-26 | Vanishing Gradient, Exploding Gradient | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day76](#day-76-implementing-artificial-neural-networks-ann-for-different-datasets) | 2025-02-27 | Implementing artificial neural networks (ann) for different datasets | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day77](#day-77-improving-neural-networks) | 2025-02-28 | Improving Neural Networks | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day78](#day-78-sequence-modeling--rnns---just-overview) | 2025-03-01 | Sequence Modeling / RNNs - Just Overview | [Intro to Deep Learning @MIT](https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI) 
| [Day79](#day-79-transformers-attention---just-overview) | 2025-03-02 | Transformers Attention - Just Overview |[Intro to Deep Learning @MIT](https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI) |
| [Day80](#day-80-cnns---just-overview-part-1) | 2025-03-03 | CNNs - Just Overview Part 1 | [Intro to Deep Learning @MIT](https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI) |
| [Day81](#day-81-cnns---just-overview-part-2) | 2025-03-04 | CNNs - Just Overview Part 2 | [Intro to Deep Learning @MIT](https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI) |
| [Day82](#day-82-deep-generative-modeling---just-overview) | 2025-03-05 | Deep Generative Modeling - Just Overview | [Intro to Deep Learning @MIT](https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI) |
| [Day83](#day-83-reinforcement-learning---just-overview) | 2025-03-06 | Reinforcement Learning - Just Overview | [Intro to Deep Learning @MIT](https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI) |
| [Day84](#day-84-deep-learning-challenges--new-frontiers---just-overview) | 2025-03-07 | Deep Learning: Challenges & New Frontiers - Just Overview | [Intro to Deep Learning @MIT](https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI) |
| [Day85](#day-85--early-stopping-normalizing-inputs-dropout) | 2025-03-08 | Early Stopping & Normalizing Inputs, Droput | [Deep learning playlist @ CampusX, ](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn)[Grokking Deep Learning @Andrew W. Trask](https://www.amazon.com/Grokking-Deep-Learning-Andrew-Trask/dp/1617293709) |
| [Day86](#day-86-regularization-quantization) | 2025-03-09 | Regularization, Quantization | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day87](#day-87---activation-functions---revisited) | 2025-03-10 | Activation Functions - Revisited | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day88](#day-88-weight-initialization) | 2025-03-11 | Weight Initialization | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day89](#day-89-deeep-learning-optimizers) | 2025-03-12 | Deep Learning Optimizers | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day90](#day-90-keras-tuner) | 2025-03-13 | Keras Tuner | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day91](#day-91-deep-diving-into-cnns) | 2025-03-14 | Deep Diving into CNNs | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day92](#day-92-understanding-paddings-and-strides) | 2025-03-15 | Understanding Paddings and Strides | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day93](#day-93-backpropagation-in-cnns-a-quick-breakdown) | 2025-03-16 | Backpropagation in CNNs: A Quick Breakdown | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn)[, Grokking Deep Learning @Andrew W. Trask](https://www.amazon.com/Grokking-Deep-Learning-Andrew-Trask/dp/1617293709) |
| [Day94](#day-94-lenet5-cat-vs-dog-classification) | 2025-03-17 | LeNet5, Cat Vs Dog Classification | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn)[, Grokking Deep Learning @Andrew W. Trask](https://www.amazon.com/Grokking-Deep-Learning-Andrew-Trask/dp/1617293709) |
| [Day95](#day-95-gpu-slow-than-cpu-well-in-my-case) | 2025-03-18 | GPU slow than CPU - well in my case? | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) |
| [Day96](#day-96-data-augmentation-pretrained-models) | 2025-03-19 | Data Augmentation, Pretrained Models | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn)[, Grokking Deep Learning @Andrew W. Trask](https://www.amazon.com/Grokking-Deep-Learning-Andrew-Trask/dp/1617293709) |
| [Day97](#day-97-visualizing-convolutional-layers-transfer-learning) | 2025-03-20 | Visualizing Convolutional Layers, Transfer Learning | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn)[, Grokking Deep Learning @Andrew W. Trask](https://www.amazon.com/Grokking-Deep-Learning-Andrew-Trask/dp/1617293709) |
| [Day98](#day-98-keras-functional-api) | 2025-03-21 | Keras Functional API | [Deep learning playlist @ CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn) [, Article](https://machinelearningmastery.com/keras-functional-api-deep-learning/)|
| [Day99](#day-99-finishing-dog--cat-classifier-project) | 2025-03-21 | Finalizing Dog Cat Classifier Project | [Project - Live Demo](https://cat-vs-dog-classifier.streamlit.app/)|
| [Day100](#day-100-hidden-markov-model-quantum-machine-learning) | 2025-03-23 | Hidden Markov Model, Quantum Machine Learning | [Medium Article: Understanding Hidden Markov Models](https://towardsdatascience.com/hidden-markov-models-explained-698d24aafc95)|
| [Day101](#day-101-pytorch-exploring-surfacely) | 2025-03-24 | Exploring Pytorch Surfacely | [DL with Pytorch - Datacamp](https://app.datacamp.com/learn/courses/introduction-to-deep-learning-with-pytorch)|
| [Day102](#day-102-training-a-neural-network-with-pytorch) | 2025-03-25 | Training a neural network with pytorch | [DL with Pytorch - Datacamp](https://app.datacamp.com/learn/courses/introduction-to-deep-learning-with-pytorch)|
| [Day103](#day-103-evaluating-and-improving-models) | 2025-03-26 | Evaluating and improving models | [DL with Pytorch - Datacamp](https://app.datacamp.com/learn/courses/introduction-to-deep-learning-with-pytorch)|
| [Day104](#day-104-crawling-through-dl-with-pytorch---chapter-1) | 2025-03-27 | Crawling through DL with pytroch | [Deep Learning for Coders -Fast.Ai](https://course.fast.ai/Resources/book.html)[, @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvboy8bmDCjwNHgTaYH2puK7)|
| [Day105](#day-105-from-model-to-production--starting) | 2025-03-28 | Starting chapter 2 : from model to production | [Deep Learning for Coders -Fast.Ai](https://course.fast.ai/Resources/book.html)[, @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvboy8bmDCjwNHgTaYH2puK7)|
| [Day106](#day-106-exploring-autograd-and-portfolio-tweaks) | 2025-03-29 | Exploring Autograd and Portfolio Tweaks | [Deep Learning for Coders -Fast.Ai](https://course.fast.ai/Resources/book.html)[, @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvboy8bmDCjwNHgTaYH2puK7) |
| [Day107](#day-107-refining-portfolio-whole-day) | 2025-03-30 | Refining Portfolio whole day | [Deep Learning for Coders -Fast.Ai](https://course.fast.ai/Resources/book.html)[, @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvboy8bmDCjwNHgTaYH2puK7) |
| [Day108](#day-108-autograd-in-pytorch-deeper-understanding) | 2025-03-31 | Autograd in PyTorch: Deeper Understanding | [Deep Learning for Coders -Fast.Ai](https://course.fast.ai/Resources/book.html)[, @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvboy8bmDCjwNHgTaYH2puK7) |
| [Day109](#day-109-pytorch-training-pipeline-manual--using-nnmodule) | 2025-04-01 | PyTorch Training Pipeline (Manual + Using nn.Module) | [Deep Learning for Coders -Fast.Ai](https://course.fast.ai/Resources/book.html)[, @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvboy8bmDCjwNHgTaYH2puK7) |
| [Day110](#day-110-dataset--dataloader-class-in-pytorch) | 2025-04-02 | Dataset & DataLoader Class in PyTorch | [Deep Learning for Coders -Fast.Ai](https://course.fast.ai/Resources/book.html)[, @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvboy8bmDCjwNHgTaYH2puK7) |
| [Day111](#day-111-ann-on-fashion-mnist-gelu) | 2025-04-03 | ANN on Fashion MNIST, GELU | [Deep Learning for Coders -Fast.Ai](https://course.fast.ai/Resources/book.html)[, @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvboy8bmDCjwNHgTaYH2puK7) |
| [Day112](#day-112-ann-on-larger-fmnist-dataset-with-gpulocal-gelusilu-history) | 2025-04-04 | ANN on larger FMNIST dataset with GPU (local), GELU/SiLU history | [Deep Learning for Coders -Fast.Ai](https://course.fast.ai/Resources/book.html)[, @CampusX](https://www.youtube.com/playlist?list=PLKnIA16_Rmvboy8bmDCjwNHgTaYH2puK7) |
| [Day113](#day-113-optimizing-fmnist-nn-using-dropouts-regularization-and-batch-normalization-in-pytorch) | 2025-04-05 | Optimizing FMNIST NN using Dropouts, Regularization and Batch Normalization in Pytorch | [Notebook](08-Practical-Deep-Learning-With-Pytorch/code/day113_fmnist_regularization.ipynb) |
| [Day114](#day-114-rnns-revisited-karpathys-blog-project-planning) | 2025-04-08 | RNNs revisited, Karpathy's blog, Project Planning | [Karpathy Blog](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) |
| [Day115](#day-115-classifying-footballers-with-their-eyes---day-1) | 2025-07-08 | Classifying Footballers with their Eyes - Day 1 | [Project Notebook](10-Projects-Based-ML-DL/01-Image-Classification(guess_the_footballer_by_eyes)/day115.ipynb) |
| [Day116](#day-116-classifying-footballers-with-their-eyes--day-2) | 2025-07-09 | Classifying Footballers with their Eyes – Day 2 | [Project Notebook](10-Projects-Based-ML-DL/01-Image-Classification(guess_the_footballer_by_eyes)/day116.ipynb) |
| [Day117](#day-117-yolo-you-only-look-once) | 2025-07-10 | YOLO (You Only Look Once) | [YOLO Paper](https://arxiv.org/abs/1506.02640) |

---
<br>
<br>



# Day 01: Basics of Linear Algebra 
<!-- <img src="01-Supervised-Learning/images/importance_of_linear_algebra.png" width="400" /> -->

<!-- 
![Importance of Linear Algebra](./01-Supervised-Learning/images/importance_of_linear_algebra.png) -->
linear algebra is used to represent data, perform matrix operations, and solve equations in algorithms like regression, pca, and neural networks.
- Scalars, Vectors, Matrices, Tensors: Basic data structures for ML.
    ![](./01-Supervised-Learning/images/example_of_tensor.png)

- Linear Combination and Span: Representing data points as weighted sums. Used in Linear Regression and neural networks.
    ![](./01-Supervised-Learning/images/3dlinear_transformation.png)

- Determinants: Matrix invertibility, unique solutions in linear regression.

- Dot and Cross Product: Similarity (e.g., in SVMs) and vector transformations.
    ![](./01-Supervised-Learning/images/dot_product.png)

*Slow progress right?? but consistent wins the race!*

---
# Day 02: Decomposition, Derivation, Integration, and Gradient Descent

- Identity and Inverse Matrices: Solving equations (e.g., linear regression) and optimization (e.g., gradient descent).

- Eigenvalues and Eigenvectors: PCA, SVD, feature extraction; eigenvalues capture variance.
    ![](./01-Supervised-Learning/images/eigenvalue_eigenvector.png)

- Singular Value Decomposition (SVD): PCA, image compression, and collaborative filtering.

[Notes Here]()

### Calculus Overview:
- Functions & Graphs: Relationship between input (e.g., house size) and output (e.g., house price).

- Derivatives: Adjust model parameters to minimize error in predictions (e.g., house price).
    ![](./01-Supervised-Learning/images/area_of_circle.png)

- Partial Derivatives: Measure change with respect to one variable, used in neural networks for weight updates.

- Gradient Descent: Optimization to minimize the cost function (error).

- Optimization: Finding the best values (minima/maxima) of a function to improve predictions.

- Integrals: Calculate area under a curve, used in probabilistic models (e.g., Naive Bayes).
    ![](./01-Supervised-Learning/images/integration.png)

Revised statistics and probability concepts. Ready for the ML Specialization course!

---
# Day 03: Supervised Machine Learning: Regression and Classificaiton
<!-- [Notes credit](https://drive.google.com/file/d/1SO3WJZGSPx2jypBUugJkkwO8LZozBK7B/view?usp=sharing) -->

- Supervised Learning: <br>
![Supervised Learning](01-Supervised-Learning/images/day1_supervisedlearning.gif)
![](./01-Supervised-Learning/images/Supervised%20Learning.png)
- Regression:<br>
![](./01-Supervised-Learning/images/Regression_model.png)
- Classification:<br>
![](./01-Supervised-Learning/images/classification_model.png)
![](./01-Supervised-Learning/images/classification_model2.png)

---
# Day 04: Unsupervised Learning: Clustering, dimensionality reduction

data only comes with input x, but not output labels y. Algorithm has to find structure in data.
![Unsupervised Learinging](01-Supervised-Learning/images/day2_supervisedvsunsupervised.gif)
- Clustering: group similar data points together <br>
![alt text](./01-Supervised-Learning/images/clustering.png)
- dimensionality reduction: compress data using fewer numbers eg image compression<br> <img src="./01-Supervised-Learning/images/dimensionality_reduction.png" width = "300">
<!-- ![alt text](./01-Supervised-Learning/images/dimensionality_reduction.png) -->
- anomaly detection: find unusual data points eg fraud detection<br>

---
# Day 05: Univariate Linear Regression:
- Learned univariate linear regression and practiced building a model to predict house prices using size as input, including defining the hypothesis function, making predictions, and visualizing results.

[Notebook: Model Representation](./01-Supervised-Learning/code/day04_model_representation.ipynb)

<!-- ![](./01-Supervised-Learning/images/notes_univariate_linear_regression.jpg) -->

<img src="./01-Supervised-Learning/images/notes_univariate_linear_regression.jpg" width = "400">

![](./01-Supervised-Learning/images/notations_summary.png) - Univariate Linear Regression Quiz

![](./01-Supervised-Learning/images/univariate_linear_regression_quiz.png)

---
# Day 06: Cost Function:
![alt text](./01-Supervised-Learning/images/costfunction.jpg)
Visualization of cost function:
![Visualization of cost function](./01-Supervised-Learning/images/visualization_costfunction.png)

- manually reading these contour plot is not effective or correct, as the complexity increases, we need an algorithm which figures out the values w, b (parameters) to get the best fit time, minimizing cost function

[Notebook: Model Representation](./01-Supervised-Learning/code/day04_model_representation.ipynb)

*Gradient descent is an algorithm which does this task*

---
# Day 07: Gradient Descent
[Notebook: Gradient descent](./01-Supervised-Learning/code/day07_gradient-descent-code-from-scratch.ipynb)

![gradient descent](01-Supervised-Learning/images/day4_gradient_descent_parameter_a.gif)
learned the basics by assuming slope constant and with only the vertical shift.
later learned GD with both the parameters w and b.
![alt text](./01-Supervised-Learning/images/gradientdescent.png)
<!-- 
![alt text](./01-Supervised-Learning/images/implementation_of_gradient_descent.png) -->
 ![alt text](01-Supervised-Learning/images/gdnote.jpg) 

---
# Day 08: Effect of learning Rate, Cost function and Data on GD
- learning rate on GD:Affects the step size; too high can overshoot, too low can slow convergence
<img src="./01-Supervised-Learning/images/learningrate_eg1.png" width = "400">
<!-- ![alt text](./01-Supervised-Learning/images/learningrate_eg1.png) -->
<img src="./01-Supervised-Learning/images/learningrate_eg2.png" width = "400">
<!-- ![alt text](./01-Supervised-Learning/images/learningrate_eg2.png) -->
- cost function on GD:Smooth, convex functions help faster convergence; complex ones may trap in local minima

- Data on GD:Quality and scaling affect stability; more data improves gradient estimates

[Notebook: gradient descent animation 3d](./01-Supervised-Learning/code/day08_gradient-descent-animation(both-m-and-b).ipynb)

---
# Day 09: Linear Regression with multiple features, Vectorization

Predicts target using multiple features, minimizing error.
- Vectorization: Matrix operations replace loops for faster calculations.

![alt text\](image.png](01-Supervised-Learning/images/multifeatureLR.png)

[Lab1: Vectorization](./01-Supervised-Learning/code/day09_Python_Numpy_Vectorization_Soln.ipynb)
<br>

---
# Day10: Feature Scaling
[Lab2: Multiple Variable](./01-Supervised-Learning/code/day09_Lab02_Multiple_Variable_Soln.ipynb)

Today, I learned about feature scaling and how it helps improve predictions. There are multiple methods for feature scaling, including
- Min-Max Scaling
- Mean Normalization
- Z-Score Normalization
![alt text](./01-Supervised-Learning/images/notesfeature_scaling.png
)
To ensure proper convergence:
check the learning curve to confirm the loss is decreasing.
Start with a small learning rate and gradually increase to find the optimal value.

![alt text](./01-Supervised-Learning/images/gdforconvergence.png) 
![alt text](01-Supervised-Learning/images/choosinglearningrate.png)

---
# Day 11: Feature engineering and Polynomial Regression
feature engineering improves features to better predict the target.

eg If we need to predict the cost of flooring and have length and breadth of the room as features, we can use feature engineering to create a new feature, area (length × breadth), which directly impacts the flooring cost.

<img src="./01-Supervised-Learning/images/notes_featureengineering.jpg" width = "400">
![alt text](01-Supervised-Learning/images/notes_featureengineering.jpg)

explored polynomial regression that models the relationship between variables as a polynomial curve instead of a straight line

**Equation:**  
y = b₀ + b₁x + b₂x² + ... + bₙxⁿ 
It is useful for capturing nonlinear relationships in data.
![alt text](01-Supervised-Learning/images/visualizationof_polynomialfunctions.png)

![alt text](01-Supervised-Learning/images/scalingfeatures_complexfunctions.png)


[Lab1: Feature Scaling and Learning Rate](01-Supervised-Learning/code/day11_Feature_Scaling_and_Learning_Rate_Soln.ipynb) <br>
[Lab2: Feature Engineering and PolyRegression](01-Supervised-Learning/code/day11_FeatEng_PolyReg_Soln.ipynb)

---
# Day 12: Linear Regression using Scikit Learn
Had a productive session with linear regression in scikit learn. The lab helped me get a better grasp of the process, though I need more practice with tuning models. Also revisited the Scikit-Learn models ,more comfortable with them now

- Scikit-learn is an open-source Python library used for machine learning that provides simple and efficient tools for data analysis, including algorithms for classification, regression, clustering, and dimensionality reduction.

![](https://github.com/paudelsamir/365DaysOfData/blob/main/01-Supervised-Learning/images/LRusingscikitlearn.png)
![alt text](01-Supervised-Learning/images/LRusingscikitlearn.png)
![alt text](01-Supervised-Learning/images/scikitlearn_cleansheet1.png) 
![alt text](01-Supervised-Learning/images/scikitlearn_cleansheet2.png) <br>
[Notebook: ScikitLearn GD](01-Supervised-Learning/code/day12_Sklearn_GD_Soln.ipynb)

---
# Day 13: Classification

[Notebook: Graded Lab](./01-Supervised-Learning/code/day13_Linear_Regression_Lab.ipynb) 
![alt text](01-Supervised-Learning/images/code-snapshot_GD_LR.png)
<br>
[Notebook: Classification solution](./01-Supervised-Learning/code/day13_Classification_Soln.ipynb)

- Classification is the process of categorizing items into different groups based on shared characteristics, like classifying tumors into benign (non-cancerous) and malignant (cancerous) based on their growth behavior and potential to spread.
![](./01-Supervised-Learning/images/example_of_lr_on_categoricaldata.png)
The example above demonstrates that the linear model is insufficient to model categorical data. The model can be extended as described in the following lab.

---
# Day 14: Logistic Regression, Sigmoid Function

- Logistic Regression: A classification algorithm used to predict probabilities of binary outcomes.
![Logistic regression on categorical data](01-Supervised-Learning/images/day14_egoflogisticrregression.png)
- Sigmoid Function: A mathematical function that maps any input to a value between 0 and 1, used in logistic regression to model probabilities.
 ![sigmoid-function](01-Supervised-Learning/images/sigmoid_function.png)

[Notebook: Sigmoid Function](01-Supervised-Learning/code/day14_Sigmoid_function_Soln.ipynb)

---
# Day 15: Decision Boundary, Cost Function
[Notebook: Cost Function ](01-Supervised-Learning/code/day16_Cost_Function_Soln.ipynb)
- Decision Boundary: A line or surface that separates different classes in a classification problem based on the model’s predictions.
- cost function:
![formula cost function](01-Supervised-Learning/images/formula_costfunction.png)
![notes](01-Supervised-Learning/images/day14_15notes.jpg)
 
[Notebook: Decision boundary](01-Supervised-Learning/code/day15_Decision_Boundary_Soln.ipynb)

[Notebook Logistic Loss ](01-Supervised-Learning/code/day16_LogisticLoss_Soln.ipynb)

---
# Day 16: Gradient Descent for Logical Regression

[Notebook: Gradient Descent Model implementation](01-Supervised-Learning/code/day16_Gradient_Descent_Soln.ipynb)

[Notebook: GD with Scikit-learn](01-Supervised-Learning/code/day16_Scikit_Learn_Soln.ipynb)

Learned logistic regression cost, gradient descent, and sigmoid derivatives through step-by-step derivations and comparisons with linear regression.

![notes_day16](01-Supervised-Learning/images/day16_notes_GD.jpg)
![alt text](01-Supervised-Learning/images/gdoflogisticregression.png)

---
# Day 17: Underfitting, Overfitting


Today, explored teh concepts, overfitting (high variance), underfitting (high bias) and generalization(just right). 
Regularization to reduce Overfitting. Explored Regularized logistic regression.
- If the data is in non linear behaviour then we have to appply Ml algos like decision tree, random forest and svm.

Explored hypermeters of logistic regression, and gained some knowledge.
![text](01-Supervised-Learning/images/day17_notes2.jpg) 
![text](01-Supervised-Learning/images/day17_notes1.jpg)

[Notebook:Overfitting Solution ](01-Supervised-Learning/code/day17_Overfitting_Soln.ipynb)

![text](01-Supervised-Learning/images/day17_overfittingexample.png)


[Notebook:Regularization ](01-Supervised-Learning/code/day17_regularization.ipynb)
![text](01-Supervised-Learning/images/day17_regularizatiom.png) 


---
# Day 18: Neurons, Layer, Neural netowrk, forward propagation
- neural network: 
neural networks are machine learning algorithms that model complex patterns using multiple hidden layers and non-linear activation functions. they take inputs, pass them through hidden layers of neurons, and output a prediction.
![alt text](02-Advanced-Learning-Algorithms/images/day18_NeuralNetowkr.png) 

- Neurons:
a neuron takes weighted inputs, applies an activation function, and outputs a result. inputs can be features or outputs from previous neurons, with weights adjusting their influence.
![alt text](02-Advanced-Learning-Algorithms/images/day18_single_neuroninaction.gif)
fig: single neuron in action

- Synapse: 
synapses connect neurons and carry the weighted inputs. each connection has a weight that adjusts during training.

- weights: 
weights control the strength of connections between neurons. they are multiplied by inputs to influence the output, and are adjusted during training.

Popular activation functions include relu and sigmoid.

- Bias: 
bias is a constant added to the weighted input before applying the activation function, helping the model represent patterns that don’t pass through the origin.

- Layers: 
![alt text](02-Advanced-Learning-Algorithms/images/day18_Layers.png) 
  - **input layer**: holds the data for the model, with each neuron representing an attribute.
  - **hidden layer**: applies activation functions to the inputs and passes results to the next layer.
  - **output layer**: receives input from the last hidden layer and returns the model’s prediction.


---
# Day 19: Forward Propagation
- Forward Propagation: Input data is “forward propagated” through the network layer by layer to the final layer which outputs a prediction.


Notes for today:
![alt text](02-Advanced-Learning-Algorithms/images/day19_notes1.jpg) 
![alt text](02-Advanced-Learning-Algorithms/images/day19_notes2.jpg)
 

Matrix Representation:
![text](02-Advanced-Learning-Algorithms/images/day19_matrixrepn.png) 
How forward Prop works for digit classification??
![text](02-Advanced-Learning-Algorithms/images/day19_UnderstandingNN.gif)
[Notebook: Neurons and Layers](02-Advanced-Learning-Algorithms/code/day19_Neurons_and_Layers.ipynb) 
[Notebook: A small Neural Netowrk using tensoflow](02-Advanced-Learning-Algorithms/code/day19_NN_CoffeeRoasting_TF.ipynb)


- **tensorflow basics**:
    - **representation of data**:numpy arrays used for input (e.g., 2D arrays).
        
        ```python
        x = np.array([[1, 2, 3], [4, 5, 6]])
        
        ```
        
    - **building a neural network**:
        1. define layers:
            
            ```python
            layer1 = dense(units=25, activation='sigmoid')
            layer2 = dense(units=15, activation='sigmoid')
            layer3 = dense(units=1, activation='sigmoid')
            
            ```
            
        2. stack layers in a model:
            
            ```python
            model = sequential([layer1, layer2, layer3])
            
            ```
            
        3. compile and train:
            
            ```python
            model.compile(optimizer='adam', loss='binary_crossentropy')
            model.fit(x, y, epochs=10)
            
            ```
            
    - **visualization**:neurons connect layer by layer, with weights and biases computed at each step (refer to attached gif).

---
# Day 20: Python Implementation from Scratch

Implemented forward propagation to compute predictions and backpropagation to optimize weights for binary classification.
- AGI: An advanced AI capable of generalizing across tasks like humans.

![loss graph](02-Advanced-Learning-Algorithms/images/day20_modelaccuracies.png)

![model accuracy](02-Advanced-Learning-Algorithms/images/day_20_comparisonforwardvsbackward.png)

[Notebook: Building Models](02-Advanced-Learning-Algorithms/code/day20_BuildingForwardPropagation.ipynb)

---
# Day 21: Vectorization, Model Training
Exploredd Vectorization for efficient computation
- Tensorflow: model.compile, binary_crossentropy, model.fit
trained a binary classification model and tested its accuracy

![alt text](02-Advanced-Learning-Algorithms/images/day21_testaccuracy.png) 
Training Model with tensorflow:
![alt text](02-Advanced-Learning-Algorithms/images/day21_trainingNNwithtenserflow.png)
Notes for today:
![Notes for today](02-Advanced-Learning-Algorithms/images/day21_notes.jpg)


---
# Day 22: Activation Functions, Softmax
*the universal approximation theorem explains that a neural network with enough hidden neurons and non-linear activations like sigmoid or relu can approximate almost any function, even complex patterns like wavy graphs.*

#### Activation Functions:
commonly used activation functions include:
- sigmoid: squashes values between 0 and 1, often used for binary classification.
- relu: outputs 0 for negatives and the input itself for positives, commonly used in hidden layers.
- tanh: outputs between -1 and 1, useful for centered data.

![multiclass example](./02-Advanced-Learning-Algorithms/images/day22_eg_multiclassclassn.png)

for multiclass classification, softmax is ideal in the output layer as it converts logits into probabilities that sum to 1. during training, the model adjusts weights to maximize the correct class probability, using categorical cross-entropy loss. softmax generalizes logistic regression, which is typically used for binary classification. in both, activation and loss functions differ based on the output type.


Logistic Vs softmax:

![logistic vs softmax](./02-Advanced-Learning-Algorithms/images/day22_logisticvssoftmax.png)

Notes:

![Notes](02-Advanced-Learning-Algorithms/images/day22_notes1.jpg) 
![Notes](02-Advanced-Learning-Algorithms/images/day22_notes2.jpg)
---
# Day 23: Implementation of Softmax Regression

- Classification with multiple Outputs:
PS: there's difference between multiclass classificaiton and multilabel classification
[Notebook: Softmax/Multinomial in Iris dataset- Multilabel](02-Advanced-Learning-Algorithms/code/day23_softmax_demo.ipynb)

> *NOTE: softmax regression is a classification algorithm that calculates probabilities for multiple classes using a linear combination of inputs and the softmax function. the class with the highest probability is chosen as the prediction*

Improved Implementation of Softmax:
![Roundoff](02-Advanced-Learning-Algorithms/images/day23_roundofferrors.png)

Tensorflow implementation:
```python
model = Sequential(
    [ 
        Dense(25, activation = 'relu'),
        Dense(15, activation = 'relu'),
        Dense(4, activation = 'softmax')    # < softmax activation here

        ##         Dense(4, activation = 'linear')   #<-- Note
    ]
)
model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    ##     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #<-- Note ---- This is preferred model softmax and loss are combined for more accurate result.
    optimizer=tf.keras.optimizers.Adam(0.001),
)

model.fit(
    X_train,y_train,
    epochs=10
)
    
```

---
# Day 24: Backpropagation - what and how ?
- what? Backpropagation adjusts nneural network weights by propagating errors backward using the chain rule and optimizing them with gradient descent.
- how? Forward pass to compute outputs, Calculate errors, propagate them backward, and update weights iteratively. Take a look at document below.

Backpropagation example with Neural Network:
![Backpropagation example with Neural Network](02-Advanced-Learning-Algorithms/images/day24_backpropagation_example.png)

[Notebook: Backprop](02-Advanced-Learning-Algorithms/code/day25_Backprop.ipynb)

[pdf- How to implement??](02-Advanced-Learning-Algorithms/backpropagation_how.pdf)

*Notes:*

![alt text](02-Advanced-Learning-Algorithms/images/day24_notes.jpg) 
---
# Day 25: Backpropagation - Why? Advices for applying machine Learning

Today, I dived into the reasons behind backpropagation's effectiveness in training neural networks. It's not just about adjusting weights; it's the gradients that guide optimization, helping the model minimize error and improve predictions. The backpropagation process makes sure that the error gets distributed in a way that leads to better learning.

*How????*
- **Gradients**: they ensure efficient error minimization by guiding weight updates.
- **Optimization**: properly tuned gradients lead to smoother optimization and faster convergence.
- **Model Evaluation**: evaluating model performance becomes easier with backpropagation because of the systematic error propagation and weight adjustments.

*Notes:*

![alt text](02-Advanced-Learning-Algorithms/images/day25_notes.jpg)


---
# Day 26: Model selection and training/cross validation/test sets, Bias and Variance

mnist dataset: Label and our prediction after training
![alt text](02-Advanced-Learning-Algorithms/images/day26_predictionvslabel.png)
Errors in our prediction:
![alt text](02-Advanced-Learning-Algorithms/images/day26_errors_inMNIST.png) 
- the importance of splitting data into training, validation, and test sets 
cross-validation and its role in hyperparameter tuning.
- Diagnosing bias and variance with error trends: high bias = underfit, high variance = overfit.
- regulariaztion to handle the tradeoff between bias and variance.
- how learning curves reveal insights about model performance and whether gathering more data will help.
![summary of learning algorithm:](02-Advanced-Learning-Algorithms/images/day26_biasandvar_as_regularization.png) ![summary of learning algorithm:](02-Advanced-Learning-Algorithms/images/day26_debugging_learningalgo.png)

Notes: 
![notes](02-Advanced-Learning-Algorithms/images/day26_notes.jpg)

[Notebook: Practice Lab: Neural Networks for Handwritten Digit Recognition, Multiclass](02-Advanced-Learning-Algorithms/code/day25_NN_multiclass_mnist.ipynb)

[Notebook: Diagnosing Bias and Variance](02-Advanced-Learning-Algorithms/code/day26_Diagnosing_Bias_and_Variance.ipynb) 

[Notebook: Model Evaluation and selection](02-Advanced-Learning-Algorithms/code/day26_Model_Evaluation_and_Selection.ipynb)

---
# Day 27: Machine Learning Development Process, ML workflow
machine learning development process  

1. ml development is iterative, involving:  
   - choosing model and data architecture.  
   - training the model.  
   - diagnosing bias, variance, and errors.  

![alt text](02-Advanced-Learning-Algorithms/images/day27_fullMLworkflow.png)
2. error analysis: identify and fix patterns in model failures.  
3. adding data:  
   - data augmentation: modify existing data (e.g., distortions).  
   - data synthesis: create artificial data.  
4. transfer learning:  
   - reuse pre-trained models for similar tasks.  
   - fine-tune them with your own data.  
  

5. ml projects follow these steps:  
   - data collection.  
   - preprocessing.  
   - modeling.  
   - evaluation.  
   - deployment.  
   ![alt text](02-Advanced-Learning-Algorithms/images/day27_model_deployment.png)
   - monitoring.  


6. ethics and fairness -  
    ensure ethical use by:  
   - avoiding biased decisions in loans, jobs, etc.  
   - preventing harmful applications like deepfakes. 

Notes:
![Notes](02-Advanced-Learning-Algorithms/images/day27_notes.jpg)

---
# Day 28: Machine Learning Model: Error Analysis and Transfer Learning

[Notebook: Code Implementation from Scratch](02-Advanced-Learning-Algorithms/code/day28_implementation.ipynb)

- *Confusion Matrix Analysis:* the most frequent error is misclassifying 5 as 3. overall, the error rate is around 8%.
![alt text](02-Advanced-Learning-Algorithms/images/day28_confusion_matrix_mnist_first.png)

- *Iterations Insight:* after 200 iterations, the error rate does not decrease significantly, suggesting that 200 iterations are enough for the model to converge.
![alt text](02-Advanced-Learning-Algorithms/images/day28_error_reduction_with_tuning.png)

- *Data Augmentation Insight:* despite applying data augmentation, there was no improvement in accuracy. this is because the MNIST dataset is already preprocessed, with centered and normalized images, making the augmentation techniques less effective. in general, data augmentation works best when the dataset is smaller or images are not preprocessed.
![alt text](02-Advanced-Learning-Algorithms/images/day28_model_accuracy_beforeandafter.png)

- *Transfer Learning with MobileNetV2:* 
  - Training set accuracy improved from 56.95% to 73.02%.
  - Validation accuracy increased from 70.52% to 74.06%.
  - Loss decreased consistently for both training and validation sets, signaling better learning and generalization.
  - The model shows significant improvement over epochs. 
  - Using transfer learning, the model started with an initial accuracy of 74% (pre-trained on ImageNet). As training progressed, the model continued to adapt, improving with each epoch. transfer learning was effective, yielding good results with fewer epochs.
![alt text](02-Advanced-Learning-Algorithms/images/day28_transfer_learningimage.png)

---
# Day 29: Error Metrices, Encoding of Categorical Data, Transoformers

[Notebook: Lab week 3: Improving Model ](02-Advanced-Learning-Algorithms/code/day29_improvingML_models.ipynb) 

[Notebook: Error Metrics ](02-Advanced-Learning-Algorithms/code/day29_error_metrics.ipynb)
### Precision vs. Recall Trade-Off: 

- High Precision:
Only hire candidates you’re sure are good.
Result: Fewer bad hires, but you might miss some great ones.
Example: You hire 5 people, all are good, but you missed 10 other good ones.
- High Recall:
Hire as many as possible to ensure no good candidate is missed.
Result: You catch all great candidates but end up with some bad hires too.
Example: You hire 50 people, 20 are good, but 30 are bad.
- When to Focus on Each?
1. Precision: When mistakes (bad hires) are costly.
Example: Hiring a brain surgeon.

2. Recall: When missing good candidates is worse.
Example: Hiring for a customer service team.


#### Encoding of Categoriical Data:

| **Encoding Type** | **Use When** | **Example** |
| --- | --- | --- |
| **Label Encoding** | Small, unordered categories | Colors: `[Red, Blue]` |
| **Ordinal Encoding** | Ordered categories | Education: `[Low, High]` |
| **One-Hot Encoding** | Nominal data, fewer unique categories | Days: `[Mon, Tue, Wed]` |
<br>

#### Types of Transformers: 

| **Transformer** | **Purpose** | **Example Use Case** | **Input** | **Output** |
| --- | --- | --- | --- | --- |
| **Column Transformer** | Apply different transformations to different columns (e.g., scaling and encoding). | Scale age and one-hot encode city names. | `Age: [25, 35, 45]`, `City: [NY, LA, CHI]` | `[-1.22, 0, 0, 1]`, `[0, 1, 0, 0]`, `[1.22, 0, 1, 0]` |
| **Function Transformer** | Apply a mathematical function (e.g., log or sqrt) to all values. | Apply logarithmic transformation to data. | `[1, 10, 100]` | `[0.69, 2.39, 4.61]` |
| **Power Transformer** | Normalize and reduce skewness in data, making it more Gaussian-like. | Stabilize variance in highly skewed data. | `[1, 10, 100]` | `[-1.22, 0.0, 1.22]` |

---
# Day 30: Scikit-Learn Pipelines & Ridge Regression (L2 Regularization) 

I explored how to create pipelines in Scikit-learn to streamline the process of combining multiple steps (like preprocessing, model fitting, and regularization) into a single object. This simplifies workflows and ensures reproducibility.

There are three techniques of regularization:
- Ridge (L2)
- Lasso (L1)
- Elastic Net (Combination)

#### Key Understanding of Ridge Regression:
 1. How the coefficient Get affected?
 - Regularization shrinks the coefficients, preventing them from becoming too large, which reduces overfitting.
 ![alt text](02-Advanced-Learning-Algorithms/images/day30_howcoeff_effected.png)
 2. Higher Values are impacted more
 - The larger the regularization value (alpha), the more the coefficients are reduced.
 ![alt text](02-Advanced-Learning-Algorithms/images/day30_higher_coeff_effected_more.png)
 3. Impact on Bias Variance Tradeoff
 - Higher regularization increases bias but reduces variance, making the model more generalizable.
 ![alt text](02-Advanced-Learning-Algorithms/images/day30_impacton_biasvariance.png)
 4. Effect on Loss Function
 - adds a penalty term that limits the magnitude of the coefficients.
 ![alt text](02-Advanced-Learning-Algorithms/images/day30_effectonlossfunction.png)
 5. Why Ridge Regression is called so?
 - Named after the concept of creating a "ridge" or constraint on the model’s coefficients, preventing them from growing too large.

 <img src='https://explained.ai/regularization/images/lagrange-animation.gif'>

Notes:
![alt text](02-Advanced-Learning-Algorithms/images/day30_notes1.jpg)
![alt text](02-Advanced-Learning-Algorithms/images/day30_notes2.jpg) 

[Notebook: Key Understandings of ridge Regression](02-Advanced-Learning-Algorithms/code/day30_visualizing_ridgeregression_key_understanding.ipynb)

---
# Day 31: Lasso Regression (L1 Regularization), Elastic Net Regularization 

#### Lasso Regression:
1. **How are coefficients affected by λ (alpha)?**
    As λ increases, regularization strength grows, shrinking less important coefficients to **exactly zero**. Larger λ values lead to feature selection by removing irrelevant features.
    ![alt text](02-Advanced-Learning-Algorithms/images/day31_howcoeffafected.png)
    
2. **Are higher coefficients affected more?**
    No. Lasso affects **smaller coefficients more**, shrinking them to zero first. Larger coefficients remain relatively unaffected if they contribute significantly to the model.
    ![alt text](02-Advanced-Learning-Algorithms/images/day31_higher_coeff.png)
    
3. **Impact of λ on bias and variance**:
    - **Higher λ** increases bias (simpler model) and reduces variance.
    - **Lower λ** reduces bias (complex model) but increases variance.
    ![alt text](02-Advanced-Learning-Algorithms/images/day31_biasandvariance.png)

4. **Effect of Regularization on Loss Function**:
    - Lasso add λ sum(w_i) to the loss, promoting sparsity by penalizing the absolute magnitude of coefficients. Unlike Ridge, it can remove features entirely, improving interpretability.
    ![alt text](02-Advanced-Learning-Algorithms/images/day31_effectofregularizationon_costfunction.png)

#### Elastic Net Regularization:
Elastic Net combines L1 (Lasso) and L2 (Ridge) penalties. It selects important features by shrinking some coefficients to zero (like Lasso) and handles correlated features by shrinking coefficients without setting them to zero (like Ridge). It’s useful when features are both highly correlated and some are irrelevant. Elastic Net is controlled by two parameters: 
α (mix of Lasso and Ridge) and 
λ (regularization strength).
![alt text](02-Advanced-Learning-Algorithms/images/day31elasticvsridgevslasso.png)

Notes:
![alt text](02-Advanced-Learning-Algorithms/images/day31_notes.jpg)



---
# Day 32: Decision Tree: Entropy and Information Gain
A decision tree is a flowchart-like structure used for classification or regression, where data is split into branches based on conditions until a final decision (leaf) is reached.
![alt text](02-Advanced-Learning-Algorithms/images/day32_decisiontree.gif)

- Decision Tree on Categorical Variables: Splits data based on categories like "Sunny" or "Rainy".
- Decision Tree on Numerical Variables: Splits data using thresholds like Age > 30.
- How Decision Tree Works: Repeatedly splits data into smaller groups based on conditions.
- Terminology: Root (start), Branch (path), Leaf (decision).
-  Pro - Simple to understand; Con - Can overfit.
- Entropy: Measures uncertainty; low entropy = purer data.
- Entropy Calculation: ```∑ p(x) * log2(p(x)).```
- Information Gain: Reduction in entropy after splitting data. ``` IG=Entropy(before)−Weighted Entropy(after)``` 
- Gini Impurity: Measures group purity, faster than entropy.
- Why Use Gini Over Entropy: Simpler and computationally faster.

Notes;
![alt text](02-Advanced-Learning-Algorithms/images/day32_notes.jpg)


---
# Day 33: Hyperparameters of DT with sclearn, Regression Trees:

![alt text](02-Advanced-Learning-Algorithms/images/day33_notes.jpg)

Studied hyperparameters of Decision Trees in Scikit-learn and techniques to handle overfitting and underfitting.
- Criterion (gini, entropy, log loss): Determines the quality of a split.
- Splitter: Helps reduce overfitting with better random splits.
- Max Depth: Controls tree depth; too high causes overfitting, too low causes underfitting.
- Min Sample Split: Sets the minimum samples required to split a node.
- Min Sample Leaf: Sets the minimum samples per leaf.
- Max Features: Determines how many features to use for splits.
- Max Leaf Nodes: Limits the number of leaf nodes.
- Min Impurity Decrease: Controls splitting based on impurity reduction.

A Regression Tree predicts continuous variables by splitting data to minimize variance. The best split is determined by maximizing variance reduction, calculated as the variance of the root node minus the weighted average variance of the leaf nodes.


Code:
![alt text](02-Advanced-Learning-Algorithms/images/day33_dt_code.png) 
Output:
![alt text](02-Advanced-Learning-Algorithms/images/day33_dt_output.png)


---
# Day 34: Visualization Using dtreeviz, Ensemble Learning Overview

[Notebook: Visualization of Decision Tree Official](02-Advanced-Learning-Algorithms/code/day34_dtreeviz_sklearn_visualisations.ipynb) 
[Notebook: Visualization of Decision Tree ](02-Advanced-Learning-Algorithms/code/day34_dtreeviz_sklearn_pipeline_visualisations.ipynb)

- started learning about ensemble learning:
understood the concept of "wisdom of the crowd" in ensemble methods.
types of ensemble methods: voting, bagging, boosting, and stacking.
overview of random forest and bootstrapped aggregation (bagging).
learned how boosting adjusts predictions iteratively.

Visuals of Bagging, boosting and stacking:
![Notes](02-Advanced-Learning-Algorithms/images/day34_bagging-boosting-stacking.webp) 
Notes;
![Notes](02-Advanced-Learning-Algorithms/images/day34_notes.jpg)


---
# Day 35: Voting Ensemble > Classification and Regression

- Soft voting: logistic regression: 60% fraud, random forest: 80% fraud, svm: 40% fraud 
→ final probability: (60% + 80% + 40%) / 3 = 60%. 
![alt text](02-Advanced-Learning-Algorithms/images/day35_Voting.jpg)

- Hard voting: majority wins, logistic regression predicts "fraud," random forest predicts "not fraud," and svm predicts "fraud" → final prediction: "fraud."



Conclusions from [Notebook: voting Classifier](02-Advanced-Learning-Algorithms/code/day35_votingClassifier.ipynb)

- weighted voting: assigning weights to classifiers helps emphasize stronger models, further improving performance.

- same algorithm, different hyperparameters: tweaking hyperparameters (e.g., kernel degree in SVM) can lead to significant accuracy changes, highlighting the value of hyperparameter optimization.
#### Classification:
```python

voting_clf = VotingClassifier(
    estimators=[
        ('lr', log_reg),  # logistic regression: good for linear patterns
        ('rf', rand_forest),  # random forest: captures complex relationships
        ('svc', svm_clf)  # support vector machine: handles edge cases
    ],
    voting='soft',  # averages probabilities from all models for final prediction
    weights=[2, 1, 1],  # gives higher importance to logistic regression
    n_jobs=-1  # enables parallel processing for faster training
)


```
What to use? soft voting or hard voting, depends if possible use both and then try to find out:
![alt text](02-Advanced-Learning-Algorithms/images/day35_hard_voting.png) ![alt text](02-Advanced-Learning-Algorithms/images/day35_soft_voting.png)

#### Regression:
- voting regressor combines multiple regression models to predict continuous values.

similarly, in regression, soft voting averages continuous predictions, and weighted voting helps models with higher performance contribute more.

![Voting regressor:](02-Advanced-Learning-Algorithms/images/day35_voting_regressor.png)

Visualize voting regression here: [link](https://votingclassifier.streamlit.app/)

``` python
voting_reg = VotingRegressor(
    estimators=[
        ('lr', lin_reg),  # linear regression: good for linear relationships
        ('rf', rand_forest_reg),  # random forest regressor: handles non-linear patterns
        ('svr', svr_clf)  # support vector regressor: captures complex relationships
    ],
    weights=[2, 1, 1],  # assigns higher importance to linear regression
    n_jobs=-1  # enables parallel processing for faster training
)

```

---
# Day 36: Bagging Ensemble > Classification and Regression

- bagging (bootstrap aggregating) is an ensemble learning technique that combines predictions from multiple models trained on different subsets of the data (created via bootstrapping) to improve accuracy and reduce variance.

- Intution: 

    ![alt text](02-Advanced-Learning-Algorithms/images/day36_bagging.gif)
    - random sampling: create multiple datasets by sampling with replacement from the original dataset.
    - train independently: train a model (single preferred) on each bootstrapped dataset (e.g., decision trees).
    - combine predictions: aggregate their predictions by majority voting (classification) or averaging (regression).
    this reduces overfitting and increases stability, especially for high-variance models.

[Notebook: Bagging Intution](02-Advanced-Learning-Algorithms/code/day36_bagging_demo.ipynb)

#### Classification:
- Intution
![alt text](02-Advanced-Learning-Algorithms/images/day36_baggingclassifier.webp)
- Code Demo
``` python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

# initialize the bagging classifier
bagging_model = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(),  # base model for ensemble; here, decision trees
    n_estimators=10,                          # number of base models to train
    max_samples=1.0,                          # fraction of the dataset for each base model (1.0 = 100%)
    max_features=1.0,                         # fraction of features used in each bootstrap sample
    bootstrap=True,                           # sample datasets with replacement (True enables bootstrapping)
    bootstrap_features=False,                 # sample features with replacement (False = no feature bootstrapping)
    random_state=42                           # seed for reproducibility
)


```

#### Regression:
- Intution:
![alt text](02-Advanced-Learning-Algorithms/images/day36_baggingregressor.png)
- Code Demo:
``` python
bagging_model = BaggingRegressor(
    base_estimator=DecisionTreeRegressor(),  # base model for ensemble; here, decision trees
    n_estimators=10,                          # number of base models to train
    max_samples=1.0,                          # fraction of the dataset for each base model (1.0 = 100%)
    max_features=1.0,                         # fraction of features used in each bootstrap sample
    bootstrap=True,                           # sample datasets with replacement (True enables bootstrapping)
    bootstrap_features=False,                 # sample features with replacement (False = no feature bootstrapping)
    random_state=42                           # seed for reproducibility
)

```

---
# Day 37: Random Forest: Intution, Working and difference with bagging, Random Forest Hyperparameters

random forest is like a bunch of decision trees making a group decision. each tree gets a vote on the outcome, and the most votes win. it's like asking a bunch of experts for advice and going with the majority.

Sampling Techniques:
- row sampling
- column sampling
- combination

[Notebook Random Forest](02-Advanced-Learning-Algorithms/code/day37_RandomForest.ipynb)

#### Random Forest vs bagging:

- why random forest performs well: it reduces overfitting by averaging multiple decision trees trained on different random subsets of data and features, improving accuracy and robustness.

- random forest vs bagging: both use multiple trees, but random forest adds feature randomness at each split, making trees less correlated and boosting performance.

![alt text](02-Advanced-Learning-Algorithms/images/day37_notes.jpg)

[Notebook: Random forest Vs bagging](02-Advanced-Learning-Algorithms/code/day37_randomForestVsBagging.ipynb)

#### Random forest Hyperparameters:
``` python

model = RandomForestClassifier(
    # core hyperparameters
    n_estimators=100,         # number of decision trees in the forest
    max_features="sqrt",      # max features to consider at each split
    max_depth=None,           # max depth of each tree; None = grow fully
    min_samples_split=2,      # min samples needed to split a node
    min_samples_leaf=1,       # min samples required in a leaf node
    bootstrap=True,           # with replacement or without replacement

    # advanced hyperparameters
    max_leaf_nodes=None,      # max number of leaf nodes per tree; None = unlimited
    min_weight_fraction_leaf=0.0,  # min fraction of total weight for a leaf node
    class_weight=None,        # weights for handling class imbalance (e.g., 'balanced')
    ccp_alpha=0.0,            # complexity parameter for pruning; trade-off between size and accuracy
    criterion="gini",         # metric to evaluate splits: "gini" (default) or "entropy"
    warm_start=False,         # reuse previous trees for incremental training; False = train from scratch
    oob_score=False,          # whether to use out-of-bag samples to estimate generalization accuracy
    verbose=0,                # verbosity of output (0 = silent)
    n_jobs=-1,                # number of CPU cores for parallel processing; -1 = use all cores
    random_state=42           # seed for reproducibility
)

```
---
# Day 38: Boosting Ensemble: Adaboost Boosting

- boosting is a sequential ensemble learning method where models correct the errors of previous models to reduce bias.
combines weak learners (like decision stumps) to create a strong predictive model.
![Boosting](02-Advanced-Learning-Algorithms/images/day38_boosting.png)

Notes:
![alt text](02-Advanced-Learning-Algorithms/images/day38_notes.jpg)

#### AdaBoost Exploration:

step-by-step understanding of adaboost’s workflow, including:
- calculating sample weights and weak learner errors.
- updating model contributions based on performance.
- final prediction via weighted majority vote.
- visualized how adaboost focuses on difficult samples.

implemented adaboost from scratch without using sklearn.
![Adaboost from scratch](02-Advanced-Learning-Algorithms/images/day38_adaboostfromscratch.gif)

[Notebook: Adaboost Implementation](02-Advanced-Learning-Algorithms/code/day38_adaboost_from_scratch.ipynb)


---
# Day 39: Understanding GradientBoosting with Regression:

 "a model that learns step-by-step by fixing the mistakes of the previous model."
 ![Algorithm:](02-Advanced-Learning-Algorithms/images/day39_gradient_boost_algorithm.png)
 ![alt text](02-Advanced-Learning-Algorithms/images/day39_gradient_boosting.png)

- Comparision between Adaboost and Gradietn boost:
![alt text](02-Advanced-Learning-Algorithms/images/day39_adaboost_vs_gradientboost.jpg)


 > boosting + gradients (gradients = direction to minimize error).

``` 
pseudo-  residual = actual - predicted
new prediction = old prediction + (learning_rate × residual)
```


- Practice:  [Notebook: gradient boosting regressor using scikit learn](02-Advanced-Learning-Algorithms/code/day39_gradient_boosting_regressor.ipynb)

#### gradient boosting variations:
- xgboost: uses regularization and handles missing data efficiently.
- lightgbm: faster with large datasets.
- catboost: great for categorical data.

```python
    gb = GradientBoostingRegressor(
        n_estimators=100,      # number of trees
        learning_rate=0.1,     # step size for updates
        max_depth=3,           # depth of each tree
        min_samples_split=2,   # min samples to split a node
        min_samples_leaf=1,    # min samples per leaf
        subsample=1.0,         # fraction of samples per tree
        max_features=None,     # use all features
        random_state=42        # ensures reproducibility
    )
```

Notes:
![alt text](02-Advanced-Learning-Algorithms/images/day39_notes.jpg)


---
# Day 40: Gradient Boosting with Classification

## Overview
- Gradient boosting improves classification by minimizing log-loss iteratively.
- Each tree focuses on correcting errors made by previous trees.
- Uses gradients of the loss function to guide updates.

#### Algorithm
1. Initialize predictions: `F0(x)` (log-odds for classification).
2. For each iteration:
   - Compute pseudo-residuals:  
     `ri = - ∂(loss) / ∂F(xi)`
   - Fit a weak learner (tree) to `ri`.
   - Update predictions:  
     `F(x) = F(x) + η * h(x)`  
     where `η` is the learning rate.

#### Loss Function
- Binary classification: log-loss.  
  `Loss = -[y log(p) + (1 - y) log(1 - p)]`
- Multiclass classification: softmax loss.
```python
gb = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)
```

Notes:
![alt text](02-Advanced-Learning-Algorithms/images/day40_notes.jpg)

[Notebook: Gradient Boosting Classification](02-Advanced-Learning-Algorithms/code/day40_gradient_boosting_classification.ipynb)

[Blog Link](https://towardsdatascience.com/all-you-need-to-know-about-gradient-boosting-algorithm-part-2-classification-d3ed8f56541e)

Visuals:
![Visualizations](02-Advanced-Learning-Algorithms/images/day40_geometric_intution1.png)
![Visualizations](02-Advanced-Learning-Algorithms/images/day40_geometric_intution2.png) 


---
# Day 41: Variations of Gradient Boosting: XGBoost - Introduction.

- **xgboost** 
- **lightgbm** 
- **catboost**

#### XGBoost:
![alt text](02-Advanced-Learning-Algorithms/images/day41_xgboost_overview.png)
xgboost (eXtreme Gradient Boosting) is an advanced implementation of gradient boosting that addresses some key limitations in traditional gradient boosting and adaboost.
here's a quick rundown of what i've learned so far.
![Benchmark](02-Advanced-Learning-Algorithms/images/day41_benchmarkPerformance.png)
- Flexibility
    - cross-platform support means xgboost works on different operating systems without much hassle.
    - it supports multiple languages like python, c++, and r, which makes it easy to integrate with your preferred tech stack.
    - integrating with other libraries like scikit-learn or spark is a breeze.
    - it can handle all kinds of ml problems — whether it's classification, regression, or ranking.

- Speed
    - parallel processing is key. it uses all your cores to speed things up, so you get faster results.
    - optimized data structures (like DMatrix) help reduce memory usage and make computations more efficient.
    - it’s cache-aware, so it knows how to make use of your cpu cache and speed up data retrieval.
    - xgboost handles datasets larger than memory through out-of-core computing.
    - distributed computing helps when your dataset is huge and you need to scale your work across multiple machines.
    - with gpu support, xgboost accelerates matrix calculations, making it much faster for large datasets.

- Performance (Why this is different from other algos??)
    - regularization is a big win. it prevents overfitting with L1 and L2 regularization, keeping your model generalizable.
    - it automatically handles missing values, learning the best way to fill them in.
    - sparsity-aware split finding is great for sparse data (think text data or categorical features).
    - finding efficient splits in trees is another xgboost feature that speeds up training without compromising accuracy.
    - tree pruning helps reduce tree size after training, improving the model’s performance on unseen data.


---
# Day 42: XGBoost for Regression and Classification, Catboost Vs XGboost Vs LightGBM

Notes on what i explored:
![Notes](02-Advanced-Learning-Algorithms/images/day42_notes1.jpg) 
![Notes](02-Advanced-Learning-Algorithms/images/day42_notes2.jpg) 
![Notes](02-Advanced-Learning-Algorithms/images/day42_notes3.jpg)


1. What if we have two or more than two features?
We can scan all features , and do all possible splits for all features, then we will calculate gain and similarity score , and select feature which has max gain, algo use greedy search

2. What if multiple feature and second feature is categorical (like binary: yes/no, male/female or muticlass: colors)?
You have to encode them using OHE or other. or use othre variants of GDboost. idenntify unique values in catagorical columns , and consider both as a potential split point ., then calculate gain and similarity scores ., and select with maximum gain .

2. What if the feature is binary categorical or multiclass categorical?
older versions: encode them (e.g., one-hot, label, or target encoding).
newer versions: native support for categorical features—no encoding needed.



#### Catboost Vs LightGBM Vs XGBoost:
![alt text](02-Advanced-Learning-Algorithms/images/day42_catboostVsLightGbmvsXGboost.png)
- **categorical features**
    - **catboost** handles them natively—no encoding needed.
    - **lightgbm/xgboost** require encoding, but xgboost supports label encoding in newer versions.
- **speed**
    - **lightgbm** is the fastest, especially for large datasets.
    - **catboost** is slower but optimized for small/mid datasets.
    - **xgboost** is slower than both due to its exhaustive computations.
- **memory usage**
    - **lightgbm** uses the least memory.
    - **catboost** and **xgboost** consume more, especially xgboost for large datasets.
- **overfitting prevention**
    - all three handle overfitting well, but **catboost** excels in datasets prone to overfitting due to ordered boosting.
- **use case**
    - **catboost**: small/mid datasets with many categorical features.
    - **lightgbm**: large datasets where speed and memory are critical.
    - **xgboost**: general-purpose, robust across various dataset types.

---
# Day 43: Stacking Ensemble: Understanding Blending and K-fold methods
~ blending: splits the data into a training set and a holdout set to train base models and then a meta-model on the predictions of the base models.~

~ k-fold stacking: uses cross-validation to generate predictions for the meta-model by training base models on different training folds and predicting on the validation fold.
~

![Notes:](02-Advanced-Learning-Algorithms/images/day43_notes.jpg) 
![Notes:](02-Advanced-Learning-Algorithms/images/day43_notes2.jpg)

#### !Hyperparameters Tuning:
- identify hyperparameters:
for decision trees: max_depth, min_samples_split, etc.
for neural networks: learning rate, batch size, number of layers, etc.
- choose a tuning method:
grid search: try every combination of parameters (computationally expensive).
random search: test random combinations (faster).
bayesian optimization: automatically find the best parameters using probabilistic methods.
automated tuning: libraries like optuna or hyperopt.
- cross-validate:
use k-fold cross-validation to test parameter combinations.
- pick the best:
finalize hyperparameters that minimize error metrics or maximize accuracy.
![alt text](02-Advanced-Learning-Algorithms/images/day43_hyperparameter_and_models.png)

#### 1. manual tuning

- tweak hyperparameters based on experience or trial-and-error.
- works for simple models but not scalable.

#### 2. grid search

- systematically tests all combinations of hyperparameter values.
- pros: exhaustive, finds the best combo (if time permits).
- cons: computationally expensive, impractical for large spaces.
- example:
    
    ```python
    from sklearn.model_selection import GridSearchCV
    param_grid = {'max_depth': [3, 5, 10], 'min_samples_split': [2, 5, 10]}
    grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)
    grid_search.fit(X_train, y_train)
    print(grid_search.best_params_)
    
    ```
    

#### 3. random search

- samples random combinations of hyperparameters.
- pros: faster, effective for large search spaces.
- cons: might miss optimal combinations.
- example:
    
    ```python
    from sklearn.model_selection import RandomizedSearchCV
    from scipy.stats import randint
    param_dist = {'max_depth': randint(3, 20), 'min_samples_split': randint(2, 20)}
    random_search = RandomizedSearchCV(DecisionTreeClassifier(), param_dist, n_iter=100, cv=5)
    random_search.fit(X_train, y_train)
    print(random_search.best_params_)
    
    ```

#### 4. bayesian optimization

- predicts the best hyperparameters using probabilistic models (e.g., Gaussian processes).
- pros: efficient in high-dimensional spaces.
- cons: complex implementation.
- libraries: `Optuna`, `Hyperopt`, `BayesSearchCV`.

#### 5. evolutionary algorithms

- optimizes hyperparameters using natural selection (e.g., genetic algorithms).
- library: `TPOT`.

#### 6. automated tools

- `auto-sklearn`: automates model selection + hyperparameter tuning.
- `H2O.ai`: distributed hyperparameter tuning.
- `Optuna`: fast, user-friendly library for hyperparameter search.

[Link to the blogpost](https://www.analytixlabs.co.in/blog/what-are-hyperparameters/#What_is_a_Hyperparameter)

---
# Day 44: K Nearest Neighbor, Coding KNN from scratch and applying on different datasets:
Predictions are based on the majority vote (classification) or average (regression) of the K closest data points in the training set.

#### **Step-by-Step Workflow**:

1. **Choose K**: Number of neighbors to consider.
2. **Calculate Distance**:
    - Common metrics: **Euclidean** (default), Manhattan, or Minkowski.
3. **Find K Nearest Neighbors**: Identify the K points closest to the query.
4. **Make Prediction**:
    - **Classification**: Majority class among neighbors.
    - **Regression**: Average value of neighbors.

#### **3. Choosing the Right K**

- **Small K** (e.g., K=1): High variance, sensitive to noise (overfitting).
- **Large K** (e.g., K=50): High bias, smoother boundaries (underfitting).
- **Rule of Thumb**: Start with K=n*K*=*n* (where n*n* = number of samples) or use **cross-validation**.

Overfitting and Underfitting:
![alt text](02-Advanced-Learning-Algorithms/images/day44_fitting.png) 
![alt text](02-Advanced-Learning-Algorithms/images/day44_overfits.png)

Code of KNN using Python:
![alt text](02-Advanced-Learning-Algorithms/images/day44_KNNclass.png) 

Finally After Hyperparameter tuning, KNN models imporves for California House price prediction:
![alt text](02-Advanced-Learning-Algorithms/images/day44_california_house_prediction_hyperparam_tuuning.png) 

Notes:
![Notes:](02-Advanced-Learning-Algorithms/images/day44_notes.jpg)

---
# Day 45: Support Vector Machines:

1. What is SVM?
Goal: SVM finds the "best" hyperplane to separate data into classes.

Key Idea: Maximize the margin (distance between the hyperplane and the nearest data points, called support vectors).

- svm helps classify data by finding the best dividing line (hyperplane).
- **support vectors**: closest points to the line that influence its position. kind of like the apples and oranges closest to the ruler.
- **margin**: the gap between the line and the nearest points. svm tries to make this as wide as possible for better separation.
    ![Svm](02-Advanced-Learning-Algorithms/images/day45_svm.png)
- **hard margin svm**: works only when data is clean and separable. not great for noisy or messy data.
    - **example**: classifying perfectly labeled "cat" vs. "dog" images where there’s no overlap.
- **soft margin svm**: allows some mistakes for better flexibility with noisy/overlapping data.
    - **example**: separating spam and non-spam emails, where some emails are hard to classify.
- **kernel trick**: useful when the data isn’t linearly separable. it projects data into a higher dimension to make it separable.
    - **example**: in handwriting recognition, svm can map curvy letters into a higher space to separate them more easily.

More Notes like optimization regularization:
![Notes](02-Advanced-Learning-Algorithms/images/day45_notes.jpg)


---
# Day 46: K-Means Clustering, DBSCAN

## K-Means (centeroid based):

#### **Algorithm Steps**

1. Initialize Centroids: Randomly select **K data points** as initial centroids(Can lead to suboptimal clusters.) (or use **k-means++**(Distributes initial centroids to improve stability and speed) for smarter initialization).
2. Assign Points to Clusters: For each data point, compute the **Euclidean distance- used by default** to all centroids. Assign the point to the **nearest centroid**.
3. Recalculate Centroids: Compute the **mean** of all points in each cluster to update centroids.
4. Repeat: Reassign points and update centroids until:
    - Centroids stabilize (change < tolerance threshold).
    - Maximum iterations are reached.

![Kmeans](02-Advanced-Learning-Algorithms/images/day46_Kmeans_algo.gif)

#### **Applications**
- Customer Segmentation
- Image Compression
- Document Clustering


``` python
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Preprocess data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Fit K-means
kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10)
kmeans.fit(X_scaled)

# Get labels and centroids
labels = kmeans.labels_
centroids = scaler.inverse_transform(kmeans.cluster_centers_)
```


#### Choosing the Optimal K

1. Elbow Method: Plot inertia vs. K; the "elbow" (point where inertia decline slows) suggests optimal K.
2. Silhouette Score: Measures how similar a point is to its cluster vs others. Higher score (closer to 1) = better clustering.
3. Domain Knowledge: Use prior understanding of the data to guide K selection (e.g., customer segments in marketing).

[Notebook: K-Means clustering Demo](02-Advanced-Learning-Algorithms/code/day46_kmeans-clustering-demo.ipynb)

*One limitation of K-means is that you must specify the number of clusters beforehand.*

## DBSCAN (density based clustering)
[Visualize: DBSCAN here](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)

![Why DBSCAN](./03-Unsupervised-Learning-And-Reinforcement-Learning/images/day46_whyDBSCAN.png)

[Notebook: DBSCAN demo](./03-Unsupervised-Learning-And-Reinforcement-Learning/code/day46_dbscan_demo.ipynb)

Groups data into density-based clusters (arbitrary shapes) and flags outliers.

- Core Point: Has ≥ `min_samples` neighbors within radius `eps`.
- Border Point: In a core point’s neighborhood but lacks enough neighbors.
- Noise: Neither core nor border.
![alt text](./03-Unsupervised-Learning-And-Reinforcement-Learning/images/day46_corebordernoise.png)

- `eps`: Use a k-distance plot (k = `min_samples`) to find the “knee” for optimal ε.
- `min_samples`: Start with `2 * data dimensions` (adjust for noise tolerance).


#### Steps

1. Pick Parameters: `eps` (radius) and `min_samples` (density threshold).
2. Expand Clusters:
    - For each unvisited point, check if it’s a core point (enough neighbors in `eps`).
    - If yes, form a cluster by adding all density-reachable points (core neighbors + their neighbors).
    - Mark non-core reachable points as border.
3. Label Noise: Points not assigned to any cluster.



| **Strengths** | **Weaknesses** |  |
| --- | --- | --- |
| Finds **any cluster shape** | Struggles with **varying densities** |  |
| **No need for K** | Sensitive to **ε and min_samples** |  |
| **Robust to outliers** | Poor performance in **high dimensions** |  |


#### When to Use?

- Data has noise or complex shapes (e.g., geospatial data, anomaly detection).
![dbscan vs k means](./03-Unsupervised-Learning-And-Reinforcement-Learning/images/day46_dbscan_vs_kmeans.png)
- Avoid if clusters have highly varying densities (use HDBSCAN instead).

---
# Day 47: Hierarchical clustering, Silhouette score

#### Hierarchical Clustering
Hierarchical clustering builds a tree-like hierarchy (*dendrogram* - a tree diagram showing how clusters merge/split. height shows the distance of merging, and cutting at a height defines the number of clusters.) of clusters. Two main approaches:
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day47_hierarchcal.gif)
1. Agglomerative (Given below): Start with each point as its own cluster, iteratively merge closest clusters.
2. Divisive (Reverse of Agglomerative): Start with all points in one cluster, recursively split into smaller clusters.

#### Agglomerative Clustering Steps:
1. Initialize: Treat each data point as a singleton cluster.
2. Compute Distance Matrix: Measure pairwise distances (e.g., Euclidean, Manhattan).
3. Merge Clusters: Combine the two closest clusters; update the distance matrix.
4. Repeat: Continue merging until one cluster remains.

| **Pros** | **Cons** |
| --- | --- |
| No need to specify cluster count upfront | Computationally heavy (*O(n³)* time, *O(n²)* space) |
| Dendrograms aid visualization | Sensitive to noise/outliers |
| Works well for small/mid-sized data | Merges are irreversible (local optima) |

#### Linkage Criteria
Defines how distances between clusters are calculated:
- Single Linkage: Minimum distance between clusters (prone to "chaining").
- Complete Linkage: Maximum distance (creates compact clusters).
- Average Linkage: Average distance between all pairs.
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day47_linkage_criteria.png)
- Ward’s Method: Minimizes variance when merging (minimizes total within-cluster variance).


#### Silhouette Score
A metric to evaluate clustering quality by measuring how similar a data point is to its own cluster (cohesion) compared to other clusters (separation).
![silhouettescore](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day47_silhouettescore.png)

- Range:  −1 (poor clustering) to  +1 (well-defined clusters).

![Notes](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day47_notes.jpg)

---
# Day 48: Dimensionality Reduction

Reduces the number of features in data while retaining meaningful patterns, addressing noise, computational cost, and visualization. Methods are hierarchically grouped into feature selection (keeping relevant features) and feature extraction (creating new features).

Dimensionality Redduction Hierarchy:
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day48_hierarchy_DR.png)
Feature Selection:
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day48_feature_selection.png) 


Dimensionality Reduction of a Data:
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day48_dimensionality_reduction_of_data.png) 

- Curse of Dimensionality: The curse of dimensionality refers to the challenges and inefficiencies that arise when analyzing data in high-dimensional spaces, such as sparsity, computational complexity, and loss of meaningful patterns.

![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day48_curseofdimensionality.png)
fig: As the dimensionality of data increases, the feature space becomes sparser, and the data is easier to separate. This is the curse of dimensionality in a nutshell.

![Notes: ](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day48_notes.jpg)
---
# Day 49: PCA (Principle Component analysis), Implementing with MNIST dataset

Goal: reduce dimensions while preserving maximum variance.

fig: pca projecting 2d data into 1d pc.
![project illustration](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day49_PCA_Projection_Illustration.gif)

Example: Reducing 10D data to 2D → Use top 2 eigenvectors (highest eigenvalues).
#### Key Concepts
- Variance: Spread of data along a feature.
- Covariance: Measures how two variables vary together.
- Eigenvectors: Directions of maximum variance (principal components).
- Eigenvalues: Magnitude of variance along eigenvectors.
- Transformation: Project data onto new axes (eigenvectors) to reduce dimensions.

 #### Steps →
![Steps PCA](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day49_steps_pca.png)

How to choose the Optimal PC??
- select the optimal number of pcs by checking the cumulative explained variance, aiming to retain 90%-95% of the total variance, or by using the elbow method on the explained variance plot where additional pcs add minimal value
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day49_choosing_best_pc.png)
Notes: 
![Notes](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day49_notes.jpg)

#### Visualizing MNIST dataset on 2D and 3D:
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day49_mnist_dataset_2d.png) 
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day49_mnist_dataset_3d.png)

[Notebook: Applying PCA on MNIST dataset](03-Unsupervised-Learning-And-Reinforcement-Learning/code/day49_pca_on_mnist.ipynb)

Conclusion: 
With about 100 PCs, our model predicts an accuracy of approximately 96%. In comparison, other models like KNN predict around 97% because KNN can capture more complex patterns in the data.


# Day 50: Visualizing and Comparing PCA, t-SNE, UMAP, and LDA + Revision with the course ML specialization:

Today was a bit hectic as I tried to understand and visualize various dimensionality reduction techniques. Here's a summary of my learnings and comparisons:

### Final Thoughts
- **t-SNE**: Captures local similarities well but sometimes distorts the global structure.
    ![t-SNE Visuals](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day50_t-SNE_visuals.gif)
- **UMAP**: Faster than t-SNE and preserves both local and global relationships better.
- **LDA**: Supervised technique, works best when class separation is important.

### Comparisons
![Comparisons](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day50_comparisions_pcs_tsne_umap_lda.png)

We explored four dimensionality reduction techniques for data visualization: PCA, t-SNE, UMAP, and LDA. We used them to visualize a high-dimensional dataset in 2D and 3D plots.

**Note**: It's easy to fall into the trap of considering one technique better than the other. At the end of the day, there's no perfect way to map high-dimensional data into low dimensions while preserving the entire structure. There's always a trade-off in the qualities each technique offers.

### Resources
- [Notebook: 3D Visualizations of PCA, t-SNE, UMAP, LDA](03-Unsupervised-Learning-And-Reinforcement-Learning/code/day50_pca_tsne_umap_lda_visualization.ipynb)
- [Article: Dimensionality Reduction for Data Visualization: PCA vs t-SNE vs UMAP](https://medium.com/towards-data-science/dimensionality-reduction-for-data-visualization-pca-vs-tsne-vs-umap-be4aa7b1cb29)

#### **Comparison of Results on MNIST**

| **Technique** | **Local Structure** | **Global Structure** | **Supervised?** | **Example Result** |
| --- | --- | --- | --- | --- |
| t-SNE | Preserved | Not preserved | No | Tight clusters of "2"s and "7"s, but arbitrary spacing between clusters. |
| UMAP | Preserved | Partially preserved | No | Tight clusters of "2"s and "7"s, with meaningful spacing between clusters. |
| LDA | Preserved | Preserved (class separation) | Yes | Distinct groups for each digit, optimized for classification. |


The learning is so hectic today, so I decided to revise the concepts we studied today with the help of Andrew NG. Here's the revision:

1. What is clustering?
    - Clustering is grouping data points into clusters where points in the same cluster are more similar to each other than to those in other clusters.
2. K-means?
    - K-means is a clustering algorithm that partitions data into K clusters by minimizing the distance between data points and their respective cluster centroids.
3. optimization objective?
    - The goal is to minimize the sum of squared distances between data points and their nearest cluster centroid.
4. Lab?? done// [Notebook: Lab Assignment](03-Unsupervised-Learning-And-Reinforcement-Learning/code/day50_C3_W1_KMeans_Assignment.ipynb)


---
# Day 51: Anomaly Detection:

#### What is Anomaly Detection?

- Identifying rare data points (anomalies) that deviate significantly from the majority of the data.
- Fraud detection, system failure prediction, intrusion detection, healthcare monitoring.
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day51_anomaly_detection_example.png)

#### Approaches for Anomaly Detection:

- Gaussian Distribution: Flag data points outside ±3σ (99.7% of data).
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day51_gaussian_distribution.png)
- Z-Score: Z=(x−μ)σ*Z*=*σ*(*x*−*μ*); if ∣Z∣>3∣*Z*∣>3, mark as anomaly.
1. Unsupervised:
    - Clustering (K-means, DBSCAN): Anomalies lie far from cluster centers.
    - Isolation Forest: Randomly splits data; anomalies are easier to isolate.
    - Autoencoders: Neural networks that reconstruct input. High reconstruction error = anomaly.

### Evaluation Metrics
- Precision: % of detected anomalies that are real.
- Recall: % of true anomalies detected.
- F1-Score: Harmonic mean of precision and recall.
- ROC-AUC: Measures trade-off between TPR (recall) and FPR.

#### Choose Algorithm:
- Small data? Use statistical methods (Z-score).
- Large data? Try Isolation Forest or Autoencoders.

### Notes:
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day51_density_estimation.png)
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day51_classified_anomalies.png)
- Assuming data is normally distributed.
- Ignoring temporal/spatial context (e.g., seasonal trends).
- Not updating models as data evolves.

# Day 52: collaborative filtering

Best Article (Working behind CF): https://medium.com/@ashmi_banerjee/understanding-collaborative-filtering-f1f496c673fd


Collaborative Filtering is a recommendation algorithm that considers the similarities between different users when recommending an item to another user.

#### Making Recommendations
- Predict what a user might like based on similar users/items.
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day52_userbased_itembased.png)
1. User-User CF: Find users like you → recommend what they liked.
2. Item-Item CF: Find items similar to what you liked → recommend those.

the approach minimizes a regularized cost function using gradient descent, adjusting user and item parameters iteratively. the learning rate (α) controls step size, balancing convergence speed and stability.
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day52_LossFuncitonin_CF.png)

while effective, it suffers from the cold start problem (new users/items lack data) and sparsity issues (many missing ratings). despite these challenges, it remains a widely used technique in recommender systems.

#### Mean Normalization
- Why? Handle users who rate everything too high/low.


#### Collaborative Filtering vs Content-Based Filtering
![alt text](03-Unsupervised-Learning-And-Reinforcement-Learning/images/day52_differencces.png)
| **CF** | **Content-Based** |
| --- | --- |
| Uses user-item interactions | Uses item features (e.g., text, genre) |
| Example: Netflix recommendations | Example: News articles recommended based on text keywords |


----
# Day 53: Project @ Football Players Market Value Prediction - Introduction and Planning
Inspired by the work of [Youla Sozen](https://github.com/youlasozen/predicting-the-Market-Value-of-Footballers)

Every project starts with a problem or question. However, this project is different. it's all about having fun. As a football enthusiast, creating these kinds of projects is always enjoyable. The plan is straightforward, and I will implement it step by step.

### Project plan

Although this is a fun project, i aim to ensure the following:

- **accurate player valuation**: estimating market values to benefit clubs, agents, and investors.
- **transfer market efficiency**: preventing overpayment, aiding negotiation, and optimizing resource allocation.
- **risk assessment & player development**: evaluating investments while identifying young talents with high growth potential.
- **data-driven insights**: supporting fairer contract negotiations and improving decision-making in fantasy football & betting.

this is a future plan, and i will work towards achieving these goals in the coming days.

![Plan of Project](04-ML-Based-Football-Players-Market-Value-Prediction/images/project_plan.png)

#### Tips for the project (crafted with deepseek):
![Tips](<04-ML-Based-Football-Players-Market-Value-Prediction/images/Tips for the project.jpg>)

----
# Day 54: Project @ Football Players Market Value Prediction - Collecting Data (Scraping)
web scraping is a technique to collect data from the internet and convert it into a meaningful format, like a data frame, when direct downloads aren't available. in this project, i used the sofifa dataset. here's the main page of sofifa
![main page](04-ML-Based-Football-Players-Market-Value-Prediction/images/day54_sofifa_mainpage.png)

Code to scrape data:
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day54_data_scraping.png)

Plan for cleaning Data:
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day54_data_cleaning_plan.png) 

----
# Day 55: Project @ Football Players Market Value Prediction - Cleaning Data

just finished a major data cleaning session for my project. went through steps like handling missing values, converting currencies, splitting combined columns (height/weight), and ensuring consistent data types. cleaned up outliers, removed duplicates, and made sure everything’s ready for the next phase: EDA and model training. feeling good with the progress 😁

Here's a final look:
[Notebook: Data Cleaning](04-ML-Based-Football-Players-Market-Value-Prediction/notebooks/cleaning.ipynb)

Code:
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day55_data_cleaning.png)

# Day 56: Project @ Football Players Market Value Prediction - EDA

[Notebook: EDA (With Complete Documentation)](04-ML-Based-Football-Players-Market-Value-Prediction/notebooks/eda.ipynb)

I have mostly used Plotly to visualize as it is interactive and for such beginners like me, the visualization impact is powerful. as well as the codes are also easy to write.

### ***overall dataset insights***
1. ***what are the top 10 most valuable players?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_top10valueableplayers.png)
2. ***how does market value vary by position (e.g., are strikers more expensive than defenders)?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_market_value_with_position.png)
3. ***which teams have the highest average market value?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_correlation_attributes_marketValue.png)
4. ***what’s the distribution of market values (is it skewed towards a few expensive players)?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_dist_of_marketvalue.png)
5. ***how does age correlate with market value (are younger players generally worth more)?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_value_age_potential.png)

### ***player attributes vs. market value***

1. ***how does a player’s overall rating affect their market value?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_overall_rating_vs_market_value.png)
2. ***which individual attributes (e.g., pace, stamina, strength) correlate the most with market value?***
3. ***does international reputation (1-5 stars) impact market value?***
4. ***how do potential ratings compare to market value (are high-potential players priced higher)?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_marketvaleue_with_reaction_deribling.png)
5. ***do physical attributes (height, weight, strength) play a role in market value?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_strength_vs_marketvalue.png)

### ***position-specific insights***

1. ***are attacking midfielders (CAM/CM) more valuable than defensive midfielders (CDM/CM)?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_position_specific_insights.png)
2. ***how does pace affect wingers' (LW/RW) market value?***
3. ***do goalkeepers follow the same market trends as outfield players?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_goalkeeper.png)

### ***contract & transfer market impact***

1. ***does a player's contract end year affect their market value (e.g., do players with 1 year left have lower values)?***
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day56_contract_end_year.png)
2. ***are players on loan priced differently compared to permanent squad members?***



# Day 57: Project @ Football Players Market Value Prediction - Feature Engineering: (Creating features, Transforming Features)

[Notebook: Creating and Transforming Features](04-ML-Based-Football-Players-Market-Value-Prediction/notebooks/feature_engineering.ipynb)

spent 6+ hours experimenting with feature engineering. ran into some challenges, but made progress:

1. **position-based features**: grouped players into categories (attackers, midfielders, defenders, goalkeepers) with scores. will refine tomorrow.
2. **club-based features**: switched from one-hot encoding (curse of dimensionality) to target encoding using the mean market value for each club.
3. **contract-based features**: correlation with market value was neutral. most features, except age, overall, and potential, seemed less important.

![pairplot](04-ML-Based-Football-Players-Market-Value-Prediction/images/day57_pairplot_for_important_relationship.png)

![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day57_scores_fe.png)

---
# Day 58: Project @ Football Players Market Value Prediction - ML : (Linear Regression with Refined Features and deploying with Streamlit)
Just for fun:
![fun](04-ML-Based-Football-Players-Market-Value-Prediction/images/day58_fun.png)

Pairplots: 
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day58_pairplots.png)

So, before diving into feature engineering after cleaning the data, i tried out linear regression and got an r2 score of around 0.52. then, after applying some feature engineering and playing around with features, i ran the same model and got the r2 score up to 0.96 with only numerical features
<br>
today, i wasn’t fully happy with the result and got confused about feature selection and engineering. so i decided to convert all features into numerical, applied scaling and transformation, and reran the model , r2 score shot up to 0.97
<br>
Saved the model immediately, then tried deploying it with streamlit locally, with a user input form and inverse transformations (MOST HATED PART) . while the model’s still a work in progress and not perfect, i'm proud of what i’ve learned so far. next steps are all about finding the best model and getting it deployed with some solid predictions and managing the form with the backend properly. 

![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day58_r2score.png)

Streamlit Preview: https://www.linkedin.com/posts/paudelsamir_day-58365-linear-regression-with-refined-activity-7294398498777501697-2vWi?utm_source=share&utm_medium=member_desktop

----
# Day 59: Project @ Football Players Market Value Prediction - Complete Streamlit Setup for our first Model - Linear Regression

Yesterday, I set up streamlit for my project with some help from ai tools. deploiyng isn't my strong suit, and it got pretty hectic trying to nail down the format and inputs and converting to model inputs. had to leave it unclear

<br>
So todayt's goal was get the ui sorted and code the input transformation for the model within streamlit. i could've tinkered with other models and tuned them, but finishing what i started felt right. a simple linear regression is doing surprisingly well for my needs. of course, i'll explore other algorithms and fine tune for better accuracy soon. planning to scale up from 5,000 to 20,000 rows in my dataset. let's see

<br>
Here's the demo where linear regression predicts player market values quite accurately. grabbed data from the site i scraped so that to visuailze properly. and its around 90 percent accurate for all the positions. that's already great !! Loving it

Here are some previewsL:

- KDB (Real Vs Predicted)
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day59_kdb_real.png) 
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day59_kdb_predicted.png)

- Lamine (Real Vs Predicted)
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day59_lamine_real.png) 
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day59_yamal_predicted.png)

-  Oblak (Real Vs Predicted)
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day59_oblak_real.png) 
![alt text](04-ML-Based-Football-Players-Market-Value-Prediction/images/day59_oblak_predicted.png)

---
# Day 60: Project @ Football Players Market Value Prediction - Testing Ridge, Lasso, and Decision Trees



Today was fun! Started by handling outliers for the linear regression model, but didn’t see any improvement, so no luck there. Then I dove into applying PCA for dimensionality reduction. After converting everything to numerical features and applying all the feature engineering, I reduced the features from 50 to 40, and guess what? Model accuracy jumped to 99%! But here’s the twist, I can’t use this model for my project since I’m limited with deployment knowledge, and reverse transforming features while predicting is still the most hectic part of the process.

![Applying PCA with 40 features](04-ML-Based-Football-Players-Market-Value-Prediction/images/day60_applying_pca_with_40features.png)

Then I tried Ridge and Lasso regression. Ridge performed the best and outperformed Linear and Lasso, so I’ll stick with Ridge for now until a simpler model comes along.

![Ridge Regression](04-ML-Based-Football-Players-Market-Value-Prediction/images/day60_Ridge_regression.png)
![Lasso Regression](04-ML-Based-Football-Players-Market-Value-Prediction/images/day60_lasso_regression.png)

Next up was Decision Tree Regressor. Applied it, and without hyperparameter tuning, I got around a 0.98 R2 score. I know Decision Trees are prone to overfitting, so I visualized, but couldn’t predict by myself. Decided to try hyperparameter tuning, but the results weren’t drastically different.

![Decision Tree Regressor](04-ML-Based-Football-Players-Market-Value-Prediction/images/day60_decision_tree_regressor.png)

At this point, the Decision Tree is the best model for the project. Let's see what’s coming next. Good luck, city. 𝐒𝐡𝐮𝐯𝐚𝐫𝐚𝐭𝐫𝐢 🌙
![GridSearchCV Decision Tree](04-ML-Based-Football-Players-Market-Value-Prediction/images/day60_gridsearchcv_dt.png)

![Decision Tree Insights](04-ML-Based-Football-Players-Market-Value-Prediction/images/day60_decision_tree_insights.png)

Did i just wasted 2 hours?? 😅😅![Fun](04-ML-Based-Football-Players-Market-Value-Prediction/images/day60_fun.png)

[Notebook: Experimentation 1](04-ML-Based-Football-Players-Market-Value-Prediction/notebooks/experimentation_1.ipynb)

---
# Day 61: Project @ Football Players Market Value Prediction -Had to hit reset from Feature Engineering

<!-- > Today's goal is:
1. Apply Bagging (Random Forest with Multiple Decision Tree Regressor ) - evaluate performance and check if we can deploy it or not
2. Apply Boosting Algorithms: XGboost, LightGBM, AdaBoost and compare model performances
3. Apply stacking Random Forest, XGBoost, and LightGBM, with a simple model like Linear Regression as the final estimator.
4. Documentation, deploy the best and simple model and Finalize the Project.
 -->

Just 20 minutes ago, i realized i’ve been making a huge mistake since day 4 with feature engineering. i found out today that as a beginner, it’s easy to mess up, but it's all part of the learning process. the mistkae was thinking about how to transform features back for deployment without realizing that features like overall rating, best oiverall, and potential are actually super correlated with market value. i was happy with the 99% accuracy, but i didn’t see the problem until now. i knew about overfitting can cause it and tried to fix it, but i never thought about visualizing feature importance and how it can affect the model.

<br>
So, iwas almost done with the project and deployed it to my local server using streamlit. feeling soooo dumb at the moment. the day before yesterday, i tested it with real players like KDB, Lamine Yamal, and Oblak, and 𝐭𝐡𝐞 𝐩𝐫𝐞𝐝𝐢𝐜𝐭𝐢𝐨𝐧𝐬 𝐥𝐨𝐨𝐤𝐞𝐝 𝐠𝐨𝐨𝐝. 𝐰𝐡𝐲?? 𝐛𝐞𝐜𝐚𝐮𝐬𝐞 𝐢𝐭 𝐭𝐨𝐭𝐚𝐥𝐥𝐲 𝐫𝐞𝐥𝐲𝐢𝐧𝐠 𝐨𝐧 𝐣𝐮𝐬𝐭 𝐭𝐡𝐫𝐞𝐞 𝐢𝐧𝐩𝐮𝐭𝐬 𝐁𝐞𝐬𝐭 𝐎𝐯𝐞𝐫𝐚𝐥𝐥, 𝐎𝐯𝐞𝐫𝐚𝐥𝐥 𝐑𝐚𝐭𝐢𝐧𝐠, 𝐚𝐧𝐝 𝐏𝐨𝐭𝐞𝐧𝐭𝐢𝐚𝐥. 𝐲𝐨𝐮 𝐜𝐚𝐧 𝐞𝐯𝐞𝐧 𝐩𝐫𝐞𝐝𝐢𝐜𝐭 𝐰𝐡𝐨𝐥𝐞 𝐭𝐡𝐢𝐧𝐠𝐬 𝐰𝐢𝐭𝐡 𝐣𝐮𝐬𝐭 𝐭𝐡𝐞𝐬𝐞 𝐢𝐧𝐩𝐮𝐭𝐬. The impact of other features is literally minimal. now, i have to rebuild the model from scratch again with better feature engineering. 

<br>
Even after this dumbest mistake, total wasted grinding, wasted time, wasted energy---for the first time in my learning journey, it feels like now i'm actually learning something.

![Context](04-ML-Based-Football-Players-Market-Value-Prediction/images/day61_context.webp)


---
# Day 62: Project @ Football Players Market Value Prediction - Finalizing Project and Deploying it

Finally, i’m able to go live with my first end-to-end ml project! 🎉 you can check it out here: https://paudelsamir.streamlit.app/


- Today was all about polishing things i messed up, after some solid feature engineering, i was able to hit 85% accuracy, pretty solid start. then, i played around a hour with hyperparameter tuning, which bumped it up to 89%.
<br>

![actual vs predicted](./04-ML-Based-Football-Players-Market-Value-Prediction/images/day61_actual_vs_predicted.png)

![alt text](./04-ML-Based-Football-Players-Market-Value-Prediction/images/day61_feature_importance.png)

but the real magic happened when i tried ensemble learning techniques. after a bit of back and forth, gradient boosting took me all the way to 94% accuracy. and will be using the same algorithm for deployment too.
<br>

And with that, after 10 days of nonstop grinding, i’m officially closing this project. it’s been a fun ride, full of learning and surprises !!

𝐆𝐢𝐭𝐇𝐮𝐛 𝐑𝐞𝐩𝐨 For the Project: https://github.com/paudelsamir/ML-Based-Football-Players-Market-Value-Prediction


# Day 63: Content-Based Movie Recommender System - Preprocessing
Today, I focused on the preprocessing phase of building a content-based movie recommender system. I created a `tags` feature by combining key keywords from columns like genres, descriptions, top 3 cast members, and crew, especially the director. This step was crucial to ensure that the recommendation engine has a rich set of features to work with.
![Notes:](04-ML-Based-Football-Players-Market-Value-Prediction/images/day63_notes.jpg)

---
# Day 64: Content-Based Movie Recommender System - Building and Deployment

Today, I built the recommendation engine based on movie content similarity using vectorization (bag of words). I also deployed it with Streamlit, so now you can input a movie name and get the top 5 similar movies based on the similarity matrix. Additionally, I integrated an API to pull movie posters in real-time from the website TMDB!

𝐂𝐡𝐞𝐜𝐤 𝐨𝐮𝐭 𝐡𝐞 𝐥𝐢𝐯𝐞 𝐝𝐞𝐦𝐨 𝐡𝐞𝐫𝐞: https://lnkd.in/d7R3Wsnk

---
# Day 65: Diving into Deep Learning


Explored deep learning concepts, including its significance, how it differs from machine learning, and whether it will replace ML. Covered key architectures like Feedforward Neural Networks (FNNs), Convolutional Neural Networks (CNNs) for image processing, Recurrent Neural Networks (RNNs) for sequential data, Autoencoders for feature learning, and Generative Adversarial Networks (GANs) for data generation.
Notes from the day:
![Notes](05-Artificial-Neural-Network-And-Improvement/images/day65_notes1.jpg) 
![Notes](05-Artificial-Neural-Network-And-Improvement/images/day65_notes2.jpg)
- Types of Neural network:
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day66_Types-of-Neural-Networks.png)


# Day 66: Perceptrons

Today, I dived into the concept of perceptrons, which are the building blocks of neural networks. I explored the perceptron algorithm, its working mechanism, and how it can be used for binary classification tasks. A supervised learning algorithm used for binary classifiers.

- Steps in Prceptron Algorithm:
  ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day66_notes.jpg)

- **Perceptron from Scratch**:
  ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day66_perceptron_scratch.png)

### Visuals:
- **Training Data**:
  ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day66_data.png)
- **Perceptron Training**:
  ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day66_after_perceptron_training.png)

I explored the fundamentals of MLOps with this paper: Machine Learning Operations (MLOps): Overview, Definition, and Architecture : https://arxiv.org/pdf/2205.02302

---
# Day 67: Perceptron Loss Function and Gradient Descent

Today, I explored the Perceptron Loss Function, which helps adjust weights when misclassification occurs, ensuring better decision boundaries. I learned how the perceptron updates its weights using the weight update rule and how Gradient Descent optimizes the loss function by iteratively moving in the direction of the negative gradient.

![alt text](05-Artificial-Neural-Network-And-Improvement/images/day67_train_perceptron.png) 
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day67_Gradient_Descent_Animation.gif)

Notes:
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day67_notes.jpg)


---
# Day 68: Multilayer Perceptron

The problem with Perceptrons lies in their limitation to learn complex patterns and functions, especially those that are not linearly separable. A Perceptron is a single-layer neural network with binary outputs, and it can only solve problems where the data points are linearly separable. If the data is not linearly separable, a Perceptron cannot converge and find a solution.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day68_xor.png)


So the solution is Multilayer Perceptron:
![GIF](05-Artificial-Neural-Network-And-Improvement/images/day68_multilayer_nn.gif)

There's a website named: https://playground.tensorflow.org/

I practiced different optimizations there some of them are,
- Adding nodes to hidden layer
- Adding nodes to input layer
- Adding nodes to output layer (for multicalss)
- Adding nuber of hidden layer

The conclusion is you can classify any type of problem within regression and classifcion by optimizing those nodes and others like activation and regularization.


### Batch and Gradient Descent:
number of samples processed before updating model weights

![alt text](05-Artificial-Neural-Network-And-Improvement/images/day68_batch_size.png)

1. SGD (batch size = 1) → updates after each individual sample (one row at a time). noisy but good for escaping local minima.
2. Mini-batch gradient descent (batch size = 16, 32, 64, etc.) → updates after a small group of samples (e.g., rows 1-100). balances speed and stability.
3. Full-batch gradient descent (batch size = all samples) → updates after seeing the entire dataset. very stable but slow and memory-heavy.

each batch in mini-batch or full-batch contains multiple rows, and the loss is computed over those samples before updating weights.

so in SGD, you're updating weights after every single row (which makes it very random and noisy). in mini-batch, you take a chunk of rows, calculate gradients over that group, then update weights. in full-batch, you process all the rows at once and then update.

Notes:
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day68_notes.jpg)

---
# Day 69: MLP notation, Forward Propagation
today, i deepened my understanding of multi-layer perceptrons (MLPs), including their formal notation and the calculation of weights and biases for each layer. i also explored forward propagation and practiced matrix multiplication by manually constructing and multiplying matrices to intuitively follow the perceptron’s computations.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day69_notes.jpg)

additionally, i studied MLP training with pytorch from the book deep learning with python by françois chollet.

additionally, i explored Image processing with datacamp, here's image representation of what i learned today
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day69_bonus_imageprocessing.png)

---
# Day 70: Loss Functions for Deep Learning

- Regression:
    - MSE : squares errors, punishes big mistakes more.
    - MAE : takes absolute difference, treats all errors equally.
    - Huber Loss: mix of mse & mae, good for outliers.
    ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day70_regression.png)

- Classificaiton:
    - Binary cross entropy : for yes/no classification (spam or not spam).
    ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day70_binary_crosss.jpg)
    - Categorical Cross entropy : for multiple classes (yes/no/ maybe).
    ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day70_categorical.jpg)
    - Sparse Categorical cross entropy - same as categorical but works with integer labels.
    - Hinge Loss: used in SVMs, pushes correct class far from the wrong ones.
    ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day70_hinge_loss.png)

- Autoencoders / VAE loss:
    - KL divergence
- GANs:
    - Discriminator loss : helps the discriminator tell real from fake.
    - Minmax Loss : generator tries to fool discriminator by minimizing its best-case performance.
- Object Detection and segmentation loss:
    - interseciton over union loss
    - smoooth l1 loss
    - dice loss
- Reinforcement loss:
    - policy gradient - rewards good actions.
    - Q-Learning loss : teaches agent to choose best long-term rewards.
    - proximal policy optimization
- Custom losss function:
    - Perceptrual loss
    - combined loss functions: mix of different losses for better results (e.g., cross-entropy + dice loss).

Notes:

![alt text](05-Artificial-Neural-Network-And-Improvement/images/day70_notes1.jpg) 
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day70_notes2.jpg)

---
# Day 71: Deep Diving Backpropagation

I already explored backpropagation in andrew ng’s ml specialization course, but that was more of a surface level explanation just the mechanics of how it works.

Today, i’m diving deep. like, REALLY deep. i want an intuitive, mathematical understanding of backpropagation, not just the algorithmic steps. all my tracing and derivations are going into my handwritten notes. this is for intutive approach to understand Regression part.

Notes:
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day71_notes1.jpg) 
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day71_notes2.jpg)

Tomorrow, i’ll probably implement backprop from scratch, test it on a proper dataset, and try to visualize what’s actually happening. the key question: how?

Also, i might challenge myself to explain why backprop works in my own words. maybe even turn it into an article.


---
# Day 72: Implementing backpropagation for regression  

Today was all about applying what i learned yesterday to code and visualizing backpropagation.  

first, i created a toy dataset that looks like this:  
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day72_sample_data.png)  

Then, i wrote functions to implement backpropagation from scratch. after running the training loop, here’s what the final parameters looked like—no keras, no tensorflow, just raw python:  
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day72_final_weights_python.png)  

all the code is in my notebook:  
[Notebook: Backpropagation Regression](05-Artificial-Neural-Network-And-Improvement/code/day72_backpropagation_regression.ipynb)  

I also tried using keras' sequential api to train the same model. after around 700 epochs, the error dropped significantly.  
![Keras](05-Artificial-Neural-Network-And-Improvement/images/day72_regression_keras.png)  

final weights with keras:  
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day72_final_weights_keras.png)  


PS: intentionally chose a confusing dataset to mess with my own head. 


---
# Day 73: Implementing Backpropagation for Classification

i already implemented backprop for regression, both handwritten and in code. today, i'm tweaking it for classification.

few things to change:  
- loss function is **binary cross-entropy** instead of MSE  
- activation function is **sigmoid** instead of linear  

but the backprop algo stays the same. all derivatives are now based on the new log function. since i already get the intuition, i'm skipping the math and just coding it.  

#### Notebook: [implementation backprop classification](05-Artificial-Neural-Network-And-Improvement/code/day73_backpropagation_classification.ipynb)

sample data looks like this:  
![sample data](05-Artificial-Neural-Network-And-Improvement/images/day73_sample_data.png)

final parameters after training:  
![final parameters](05-Artificial-Neural-Network-And-Improvement/images/day73_final_params.png)

function to update parameters:  
![parameter update function](05-Artificial-Neural-Network-And-Improvement/images/day73_functionPto_update_parameter.png)

at last, i tried implementing the same using tensorflow to see how it compares to my scratch implementation:  
![tensorflow code](05-Artificial-Neural-Network-And-Improvement/images/day73_code_in_tensorflow.png)

---
# Day 74: Revising old days, Memoization

Today, i revised concept of gradient and derivatives, focusing on how subtracting the gradient term is helping minimize loss. gradient  relies on derivatives to find the optimal weights, and the learning rate controls the step size too high can cause overshooting, while too low leads to slow convergence. another key takeaway was memoization, a technique to store previously computed values to optimize calculations. in neural networks, repeated derivative computations can slow down training, and memoization helps speed things up by avoiding redundant calculations. this approach is widely used in dynamic programming and can improve efficiency in deep learning models.

Notes:
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day74_notes1.jpg) ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day74_notes2.jpg)


---
# Day 75: Vanishing Gradient, Exploding Gradient


Today i first revised Gradient descent in NN:
- Batch : faster to complete epochs ( batch size = all)
- Stochastic : faster to converge ( batch size = 1)
- Mini- Batch : mostly suitable ( batch size around center)


- vanishing gradient: when gradients become too small, causing early layers to learn very slowly or not at all. example: in deep networks using sigmoid activation, earlier layers stop updating because gradients shrink to near zero.

- exploding gradient: when gradients become too large, leading to unstable updates and divergence. example: in rnn training, weights keep multiplying large gradients, causing values to explode to infinity.

![problems](05-Artificial-Neural-Network-And-Improvement/images/day75_gradients_problems.png)

#### How to Handle VGD problem:

- use better activation functions – replace sigmoid/tanh with relu, leaky relu, or elu.
Using Sigmoid:
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day75_using_sigmoid.png)
Using ReLU:
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day75_using_relu_insteadof_sigmoid.png)
This is the final weights comparision:
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day75_old_vs_new_weights.png) 
- use proper weight initialization – xavier/glorot for sigmoid/tanh, he initialization for relu.
- batch normalization – normalizes activations to maintain stable gradients.
- residual connections (skip connections) – used in resnets to allow gradients to flow easily.
- gradient clipping – caps gradients to prevent them from becoming too small.


---
# Day 76: Implementing artificial neural networks (ann) for different datasets  

- Experimented with ann on two datasets: mnist for handwritten digit classification and a gre dataset for graduate admission prediction. the goal was to train models and analyze performance across different domains. 

[Notebook: GRE prediction](05-Artificial-Neural-Network-And-Improvement/code/day76_gre_prediction.ipynb) <br>
[Notebook: MNIST Classification](05-Artificial-Neural-Network-And-Improvement/code/day76_mnist_classification.ipynb)

1. trained an ann on mnist to classify handwritten digits (0-9) using a sequential model with dense layers. training ran for 30 epochs with promising results.  
    - **training results (30 epochs):**  
        ![mnist training](05-Artificial-Neural-Network-And-Improvement/images/day76_mnist_training_30epochs.png)  
    - **model summary:**  
        ![mnist model summary](05-Artificial-Neural-Network-And-Improvement/images/day76_mnist_model_summary.png)  
    - **sample predictions:**  
        ![mnist prediction](05-Artificial-Neural-Network-And-Improvement/images/day76_mnist_prediction.png)  

- applied ann to predict graduate school admission chances based on gre scores, gpa, and other factors. dataset required preprocessing before feeding into the model. trained for 100 epochs.  
    - **sample dataset:**  
        ![gre dataset sample](05-Artificial-Neural-Network-And-Improvement/images/day76_gre_dataset_sample.png)  
    - **training results (100 epochs):**  
        ![gre training](05-Artificial-Neural-Network-And-Improvement/images/day76_gre_training_100epochs.png)  
    - **model summary:**  
        ![gre model summary](05-Artificial-Neural-Network-And-Improvement/images/day76_gre_model_summary.png)  
    - **model accuracy evaluation:**  
        ![gre model accuracy](05-Artificial-Neural-Network-And-Improvement/images/day76_gre_model_accuracy.png)  

*next steps: hyperparameter tuning, dropout layers for regularization, and testing on additional datasets.*


# Day 77: Improving Neural Networks

Notes:
![notes](05-Artificial-Neural-Network-And-Improvement/images/day77_notes.jpg)

#### Fine-tuning Neural Network Hyperparameters

Fine-tuning neural network hyperparameters is about adjusting key settings to improve learning

* *Learning rate* - controls how fast the model updates. Too high = unstable, too low = slow learning.
* *Batch size* - number of samples processed before an update. Small = noisy but frequent updates, large = stable but slow.
* *Epochs* - full passes through data. Too few = underfitting, too many = overfitting.
* *Layers & neurons* - more can improve learning but make training harder. 
* *Activation function* - decides neuron output; common ones are ReLU, sigmoid, tanh.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day77_activation_function.png)
* *Dropout* - turns off some neurons randomly to prevent overfitting.
* *Optimizer* - algorithm that adjusts weights efficiently (Adam, SGD, etc.).

#### Problem Solving Strategies

* *Vanishing/exploding gradient* – gradients shrink or blow up, stopping learning → use ReLU, batch norm, weight init, or residual connections.
* *Not enough data* – small datasets cause poor generalization → apply data augmentation, transfer learning, or synthetic data generation.
* *Slow training* – long training times due to large models or bad optimizers → use mini-batches, better optimizers, mixed precision, and GPUs.
* *Overfitting* – model memorizes training data but fails on new data → apply dropout, regularization, early stopping, and data augmentation.
* *Underfitting* – model is too simple and fails to learn patterns → increase complexity, train longer, improve features, and reduce regularization.
* *Imbalanced data* – one class dominates, leading to biased predictions → use class weighting, oversampling, or synthetic data (SMOTE/GANs).
* *Poor generalization* – model does well in training but fails on real-world data → ensure diverse data, reduce leakage, use domain adaptation, or adversarial training.

##### Transfer Learning
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day77_transfer_learning.png)
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day77_how_tl_works.png)


# Day 78: Sequence Modeling / RNNs - Just Overview
> *I just thought ki Before diving deeper into neural network improvement techniques, I should first gain a surface-level understanding of deep learning concepts I'll be tackling in the future as this learning technique is helping me alot. For this purpose, I found an excellent YouTube playlist: [MIT 6.S191: Introduction to Deep Learning](https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI). There are approximately 10 to 15 videos that I plan to watch to build a foundational overview and i too will deep dive into these later*

![alt text](05-Artificial-Neural-Network-And-Improvement/images/day78_notes.jpg) 
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day78_note2.jpg) 

Implementation Preview
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day78_rnnfromscratch.png)
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day78_rnn_implement_intf.png) 


---
# Day 79: Transformers , Attention - Just Overview

Transformers replace RNNs by using **self-attention**, enabling parallel processing and handling long-range dependencies efficiently. Introduced in **"Attention Is All You Need" (2017)**, they power models like BERT and GPT.  

Self-attention computes **Query (Q), Key (K), and Value (V)** matrices to determine word relationships. **Multi-head attention** allows the model to capture different contextual meanings.  

The transformer consists of **encoder-decoder blocks** with **self-attention, feed-forward layers, and normalization**. Encoders learn representations, while decoders generate sequences.  

Transformers are used in **chatbots, translation, search engines, and AI coding assistants**. Key models include **BERT (bi-directional understanding), GPT (text generation), and T5 (text-to-text tasks)**.  


![alt text](05-Artificial-Neural-Network-And-Improvement/images/day79_notes.jpg) ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day79_transofrmer.png) ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day79_encoder_decoder_parts.png)

---
# Day 80: CNNs - Just Overview Part 1
- Computer vision enables machines to interpret visual data.  
- Used in **self-driving cars, medical imaging, surveillance, AR**.  

### What Computers "See"  
- Images are **matrices of pixel values** (grayscale: single matrix, RGB: 3 matrices).  
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day80_what_computers_see.png)

### Feature Extraction & Convolution  
- CNNs learn **features** like edges, textures, and shapes automatically.  
- **Convolution**: uses filters (kernels) to extract patterns from images.  
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day80_cnn_filters.png)
- **Example Kernel (Edge Detection - traditional filter)**:  ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day80_edge_detection.png)

### CNN Architecture  
1. **Convolution Layers**: detect patterns.  
2. **ReLU Activation**: makes model non-linear.  
3. **Pooling Layers**: reduce size, keep key features.  
4. **Fully Connected Layers**: classify objects.  
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day80_cnn_arch.png)

### Object Detection  
use yolo when speed matters more than precision (e.g., real-time apps).
use rcnn when accuracy is critical and speed isn’t a constraint.
use faster rcnn for a balance between accuracy and speed.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day80_yolovsrcnn.png)

### Self-Driving Cars  
- CNNs help detect **lanes, pedestrians, and traffic signs**.  
- End-to-end models predict **steering angles** using video input.  
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day80_Self-driving-car-simulator-deepdrive.webp) ![alt text](05-Artificial-Neural-Network-And-Improvement/images/day80_self_driving_cars.png)
---
# Day 81: CNNs - Just Overview Part 2

yesterday, i watched a video on cnn. the goal was just to explore it for a day, but i feel like this is an interesting topic. so today, i want to learn more—like, in-depth—about the feature extraction part, which i find the most interesting aspect of cnn.

i learned to use relu and understood convolution layers and how they work yesterday. but today, i learned about pooling and how it reduces the size. i also explored the classification process in more depth.

notes:
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day81_notes.jpg)
note: cnn by itself doesn't handle rotation and scaling well. for that, use data augmentation.



---
# Day 82: Deep Generative modeling - Just overview

Watched this single video : https://www.youtube.com/watch?v=Dmm4UG-6jxA&t=3242s

today’s deep dive into **generative models** gave me a solid grasp of how ai can not only recognize patterns but also **create** new data from scratch. these models are the backbone of modern generative ai, and understanding them is key to keeping up with the field.

### **Generative models: what they do**

- they don’t just classify or analyze data; they **generate entirely new data** that resembles what they learned from.
- examples include **autoencoders, variational autoencoders (VAEs), GANs, and diffusion models**—each with its own strengths.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day82_generative_discriminative.png) 

### **Latent variable models: finding the hidden structure**

- autoencoders learn to compress data into lower-dimensional representations and then reconstruct it.
- vaes take this further by adding randomness, making them better at **generating diverse outputs** instead of just memorizing patterns.
- plato’s cave analogy clicked here—observed data is like shadows on a wall, while latent variables represent the actual objects casting those shadows.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day82_latent_variable.png) 

### **Generative adversarial networks (GANs): competition makes better results**

- a **generator** creates fake data, while a **discriminator** tries to catch the fakes.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day82_gan_discriminator.png) 
- through constant feedback, both improve, leading to **highly realistic outputs**.
- training is tricky—imbalanced learning can cause **mode collapse**, where the generator keeps making similar outputs instead of diverse ones.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day82_vae_encoder_decpder.png) 

### **Regularization & structure in vaes**

- forcing the latent space into a structured distribution (often gaussian) helps ensure **smoothness and continuity**, making vaes more useful for controlled generation.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day82_vae_summary.png)

### **Applications beyond images**

- these models aren't just for ai art—they power **speech synthesis, text generation, and even domain adaptation** (like CycleGAN for translating images without paired data).

### **Diffusion models: a new generative powerhouse**

- instead of generating images all at once (like GANs), diffusion models **gradually refine noise into meaningful images**.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day82_diffusion_model.png) 
- they’re more stable and produce higher-quality results, making them the **future of generative ai**.

# Day 83: Reinforcement Learning - Just Overview
with the help of this video: https://www.youtube.com/watch?v=8JVRbHAVCws&t=3242s
- explored key concepts of reinforcement learning (RL), including Q-learning and policy learning algorithms.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day83_downsides_q_learning.png) 

- dived into Deep Q Networks (DQN) and their role in handling complex environments, like Atari games.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day83_deepq_network.png) 

- understood the difference between discrete and continuous actions and how they impact RL models.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day83_vista.png)

- learned about real-world applications of RL, from robotics to game AI, and cutting-edge technologies like AlphaGo and MuZero.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day83_alphago.png) 

- explored training techniques like policy gradients and how they improve decision-making in RL agents.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day83_policy_learning.png) 
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day83_policy_gradient_training.png) 
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day83_summary.png) 
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day83_nnotes.jpg)

---
# Day 84: Deep learning: challenges & new frontiers - Just Overvview

Video Link: https://www.youtube.com/watch?v=N1fbskTpwZ0&t=3021s

![alt text](05-Artificial-Neural-Network-And-Improvement/images/day84_ai_hype.png)
- neural network failure modes – ai can fail unpredictably, often overconfident in wrong answers.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day84_NN_limitations.png) 
- uncertainty in deep learning – models lack awareness of their own mistakes, leading to unreliable predictions.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day84_uncertainty_dl.png) 
- adversarial attacks – tiny changes in input can trick ai into making completely wrong decisions.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day84_deeplearning_alchemy.png) 
- algorithmic bias – ai inherits and amplifies biases from training data, leading to unfair outcomes.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day84_algorithmic_biases.png) 

### generative ai & diffusion models

- the landscape – diffusion models are replacing GANs in high-quality image generation.
- diffusion process – gradually add noise to data and train a model to reverse it.
- noising & denoising – AI learns to reconstruct images by stepwise noise removal.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day84_diffusion.png) 
- text to image, beyond images – models like DALL·E generate images, but AI is expanding into text, music, and 3D.

### large language models (LLMs)

- using llms to generate text – models predict words to generate human-like text.
- limitations – hallucinations, bias, and lack of real understanding.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day84_llms.png) 
- more parameters = better performance, but higher computational cost.
- how they work – transformers + attention mechanism process and generate context-aware text

# Day 85:  Early Stopping, Normalizing Inputs, Dropout

These are the techniques i will cover in upcoming days:

1. Vanishing Gradients
    - Activation Functions
    - Weight Initialization

2. Overfitting
    - Reduce Complexity/Increase Data
    - Dropout Layers
    - Regularization (L1 & L2)
    - Early Stopping

3. Normalization
    - Normalizing inputs
    - Batch Normalization
    - Normalizing Activations

4. Gradient Checking and Clipping

5. Optimizers
    - Momentum
    - Adagrad
    - RMSprop

6. Learning rate scheduling

7. Hyperparameter Tuning
    - No. of hidden layers
    - Nodes/layer
    - Batch size

Today, I explored Early Stopping, Normalizing Inputs, and Dropout techniques for improving neural network performance.

- **Early Stopping**: Prevents overfitting by halting training when validation performance drops.
- **Normalizing Inputs**: Scales features to a consistent range, aiding in faster and more stable learning.
- **Dropout**: Reduces overfitting by randomly deactivating neurons during training, forcing the model to generalize better.

[Notebook: Dropout on Regression](05-Artificial-Neural-Network-And-Improvement/code/day85_dropout_regression.ipynb)
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day85_regression_nodropoutvsdropout.png) 
[Notebook: Dropout on Classification](05-Artificial-Neural-Network-And-Improvement/code/day85_dropout_classification.ipynb) 
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day85_dropout_vs_nodropout.png)



---
# Day 86: Regularization, Quantization


L1 and L2 regularization are typically used for smaller networks. For larger networks, it is better to use neural network-specific regularization which is dropout regularization.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day86_notes.jpg)

An evaluation procedure must be used when using a regularizer to monitor that regularization process. For this, we can plot model performance against the number of epochs during the training process.

![Regularization](05-Artificial-Neural-Network-And-Improvement/images/day86_regularization_techniques.png) 
![Regularization](05-Artificial-Neural-Network-And-Improvement/images/day86_l1andl2regularization.png)


[Notebook: without Regularization vs Applying Regularization ](05-Artificial-Neural-Network-And-Improvement/code/day86_regularization.ipynb)
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day86_withoutl2_vs_withl2.png)

#### Quantization:
quantization in deep learning reduces the precision of numbers in a model to save memory and speed up processing. it can be done in two ways: post-training quantization (ptq), which converts the model to lower precision after training for faster performance but may lose some accuracy, and quantization-aware training (qat), where quantization is simulated during training, resulting in better accuracy but requiring more time. frameworks like TensorFlow provide tools for both methods to help deploy lighter and faster models.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day86_quantization.png)



# Day 87 - Activation Functions - Revisited

why needed? introduce non-linearity to capture complex patterns.

![alt text](05-Artificial-Neural-Network-And-Improvement/images/day87_notes1.jpg) 
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day87_notes2.jpg)
ideal properties: non-linear, differentiable, computationally inexpensive, zero-centered, non-saturating.

- use relu + he init, sigmoid/tanh + xavier init.
- relu in early layers, tanh or sigmoid in later/output layers.
- prefer gelu for transformers.
- avoid dying relu with leaky relu or prelu.

relu: fast, simple but can die.
leaky relu: small slope for negatives, avoids dead neurons.
prelu: learnable slope, more flexible.
elu: better generalization, but expensive.
selu: self-normalizing, good for deep nets.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day87_activation_functions.png) 

---
# Day 88: Weight Initialization

**Zero Init**: All weights as zero → no learning (same gradients).

```python
weights = np.zeros((input_size, output_size))
```
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day88_zeros_initialization.png)

**One Init**: All weights as one → same issue, no symmetry breaking.

```python
weights = np.ones((input_size, output_size))
```

✅ **Random Init**: Small random values.

```python
weights = np.random.randn(input_size, output_size) * 0.01
```

✅ **Xavier Init** (for tanh/sigmoid):

```python
weights = np.random.randn(input_size, output_size) * np.sqrt(1 / input_size)
```
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day88_xavier_initialization.png) 

✅ **He Init** (for ReLU):

```python
weights = np.random.randn(input_size, output_size) * np.sqrt(2 / input_si
```
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day88_he_normal_initialization.png) 

[Notebook: Weight Initialization](05-Artificial-Neural-Network-And-Improvement/code/day87_weight_initialization.ipynb)

[Notebook: Xavier and He initialization](05-Artificial-Neural-Network-And-Improvement/code/day87_xavier_and_he_init.ipynb)

Notes:
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day88_notes.jpg) 
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day88_notes2.jpg) 


---
# Day 89: Deeep Learning Optimizers
- **Gradient Descent**: Fundamental optimizer that updates weights by moving in the opposite direction of the gradient of the loss function w.r.t. the weights. It's basic but effective for many problems.
```python
from tensorflow.keras.optimizers import SGD
optimizer = SGD(learning_rate=0.01)
```
![bgd](05-Artificial-Neural-Network-And-Improvement/images/day89_batch_gd.png)

- **Stochastic Gradient Descent**: Optimizes with each sample instead of the entire dataset, which is faster but more noisy.
```python
from tensorflow.keras.optimizers import SGD
optimizer = SGD(learning_rate=0.01)
```

- **Momentum**: Momentum helps accelerate SGD by moving along relevant directions and dampening oscillations. It adds a fraction of the previous update to the current one.
```python
from tensorflow.keras.optimizers import SGD
optimizer = SGD(learning_rate=0.01, momentum=0.9)
```
![momentum](05-Artificial-Neural-Network-And-Improvement/images/day89_momentum.png)

- **NAG**: An improvement over momentum. It looks ahead by calculating the gradient not just at the current position but slightly ahead, giving better convergence.
```python
from tensorflow.keras.optimizers import SGD
optimizer = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)
```
![nag](05-Artificial-Neural-Network-And-Improvement/images/day89_nag.png)

- **AdaGrad**: Adapts the learning rate to the parameters, performing larger updates for infrequent parameters and smaller updates for frequent ones.
```python
from tensorflow.keras.optimizers import Adagrad
optimizer = Adagrad(learning_rate=0.01)
```
![adagrad](05-Artificial-Neural-Network-And-Improvement/images/day89_adagrad.png)

- **RMSProp**: Divides the learning rate by an exponentially decaying average of squared gradients. Useful for non-stationary objectives.
```python
from tensorflow.keras.optimizers import RMSprop
optimizer = RMSprop(learning_rate=0.001)
```
![rmsprop](05-Artificial-Neural-Network-And-Improvement/images/day89_rmsprop.png)

- **ADAM (Adaptive Moment Estimation)**: Adaptive learning rate method that combines the advantages of RMSprop and momentum. Popular for its efficiency.
```python
from tensorflow.keras.optimizers import Adam
optimizer = Adam(learning_rate=0.001)
```
![Adam](05-Artificial-Neural-Network-And-Improvement/images/day89_adam.png)


Overall: 
![overall](05-Artificial-Neural-Network-And-Improvement/images/day89_overall.gif)
Notes:
![notes](05-Artificial-Neural-Network-And-Improvement/images/day89_notes1.jpg) 
![notes](05-Artificial-Neural-Network-And-Improvement/images/day89_notes2.jpg) 
![notes](05-Artificial-Neural-Network-And-Improvement/images/day89_notes3.jpg)

Interactive Visualization of Optimization Algorithms in Deep Learning: https://emiliendupont.github.io/2018/01/24/optimization-visualization/


---
# Day 90: Keras Tuner
hyperparameters (like learning rate, batch size, optimizer) directly impact model performance. tuning helps optimize accuracy and generalization.

in my case, i worked with the diabetes dataset and focused on tuning key hyperparameters like learning rate, batch size, optimizer, number of neurons, and dropout rate. each of these parameters influences different aspects of training—for example, the learning rate affects how quickly the model converges, while dropout helps prevent overfitting.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day90_applying_keras_tuner.png)

to streamline the tuning process, i used keras tuner’s RandomSearch. i defined a hypermodel where parameters like the number of layers, neurons, dropout rates, learning rates, and optimizer types were set as tunable. the objective was to maximize validation accuracy. i also configured settings like max_trials to control the search space and executions_per_trial to ensure consistent evaluation.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day90_best_hyperparams.png)
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day90_best_val_accuracy_after_tuning.png)
after running the tuning process, the model achieved around 79% accuracy. the tuning helped balance model complexity and performance, reducing overfitting and improving generalization.
![alt text](05-Artificial-Neural-Network-And-Improvement/images/day90_final_no_overfitting.png)
for further improvements, i could fine-tune hyperparameters like the learning rate and dropout in smaller increments, try advanced optimizers like adamw, or implement early stopping to avoid unnecessary training once the model stops improving.


---
# Day 91: Deep Diving into CNNs:

Focused on planning a deep dive into cnns. explored why anns fall short for cnn tasks and uncovered some fascinating cnn applications. along the way, stumbled upon some surprisingly cool ideas for future projects. fueled by that curiosity, i tried something basic today—simple, but a solid starting point.

today, i explored opencv from scratch, debugged image loading issues, applied basic filters using custom convolution kernels in pure python as well as with Opencv, and created amazingly undefinable custom filters with the excitement.
![alt text](06-Convolutional-Neural-Network/images/face_detection.png) 
[Notebook:Trying OpenCV for the first time](06-Convolutional-Neural-Network/code/day91_opencv_forthe_firsttime.ipynb)
Watch out notebook what i did with this lovely image:
![alt text](06-Convolutional-Neural-Network/images/girl.png)

Notes:
![alt text](06-Convolutional-Neural-Network/images/day91_notes.jpg)

### architectures
- **early arch**
    - lenet-5
    - alexnet
    - vgg net (16/19)
- **modern arch**
    - resnet (skip connection)
    - inception net (google net)
    - mobilenet
    - efficientnet

### Plan after Architecture
1. **data augmentation** *(rotation, scaling, flipping)*
2. **transfer learning** *(using pretrained models like vgg, resnet)*
3. **object detection** *(yolo, r-cnn, ssd, etc.)*
4. **image segmentation** *(u-net, mask r-cnn)*
5. **attention mechanisms** *(squeeze, spatial channel-wise)*
6. **self-supervised learning** *(contrastive, ssl architectures)*


---
# Day 92: Understanding Paddings and Strides

How CNNs working with Grayscale and Rgb images??
![alt text](06-Convolutional-Neural-Network/images/day92_conv_of_grayscale.gif) 
![alt text](06-Convolutional-Neural-Network/images/day92_conv_of_rgb.gif)

padding and strides are important in convolutional neural networks (cnns) because they affect feature extraction, output size, and computational efficiency.

Padding is used to prevent reduction in spatial dimensions and retain edge information. for example, a 5x5 image with a 3x3 filter produces a 3x3 feature map, which keeps shrinking with more layers. adding padding helps maintain the size.
![alt text](06-Convolutional-Neural-Network/images/day92_after_applying_padding.gif)
there are two common types of padding:

- valid: no padding, meaning the output size shrinks.
- same: pads the input so the output size remains the same.

the output size with padding is calculated as:

(n + 2p - f + 1) × (n + 2p - f + 1), where n is the input size, f is the filter size, and p is the padding amount.

in keras, padding is applied like this Demo for MNIST:

```python
# Importing necessary libraries
import tensorflow as tf
from tensorflow.keras.layers import Dense, Conv2D, Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras.datasets import mnist

# Loading MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Creating a Sequential model
model = Sequential()

# Adding Convolutional layers with valid padding
model.add(Conv2D(32, kernel_size=(3,3), padding='valid', activation='relu', input_shape=(28,28,1)))
model.add(Conv2D(32, kernel_size=(3,3), padding='valid', activation='relu'))
model.add(Conv2D(32, kernel_size=(3,3), padding='valid', activation='relu'))

model.add(Flatten())

model.add(Dense(128, activation='relu'))
model.add(Dense(10, activation='softmax'))

# Printing model summary
model.summary()
```

strides control how far the filter moves across the image at each step. a stride of (1,1) moves one pixel at a time, while higher strides skip pixels, reducing spatial dimensions and computation time.
![stride = 2](06-Convolutional-Neural-Network/images/day92_stride_two.gif)
output size with strides is calculated as:

((n + 2p - f) / s + 1) × ((n + 2p - f) / s + 1), where s is the stride value.

higher strides help capture larger patterns but reduce spatial resolution. for example, a stride of (2,2) makes the filter shift 2 pixels at a time:

```python
# Importing necessary libraries
import tensorflow as tf
from tensorflow.keras.layers import Dense, Conv2D, Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras.datasets import mnist

# Loading MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Creating a Sequential model with strides
model = Sequential()

model.add(Conv2D(32, kernel_size=(3,3), padding='same', strides=(2,2), activation='relu', input_shape=(28,28,1)))
model.add(Conv2D(32, kernel_size=(3,3), padding='same', strides=(2,2), activation='relu'))
model.add(Conv2D(32, kernel_size=(3,3), padding='same', strides=(2,2), activation='relu'))

model.add(Flatten())

model.add(Dense(128, activation='relu'))
model.add(Dense(10, activation='softmax'))

# Printing model summary
model.summary()
```

in summary, padding helps retain image details and control output size, while strides affect computational efficiency and feature abstraction. tuning these parameters is key to optimizing cnns.

#### Pooling:
pooling is used to downsample feature maps, reducing their size while retaining important information. it helps prevent overfitting and reduces computation.

common types of pooling:
![alt text](06-Convolutional-Neural-Network/images/day92_pooling.webp)
max pooling: selects the maximum value in a region.
average pooling: takes the average of values in a region.
for example, a 2x2 max pooling layer with stride 2 reduces feature maps to half their original size.


```python

# importing necessary libraries
import tensorflow as tf
from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.datasets import mnist

# loading mnist dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# reshaping input data
X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255
X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255

# creating a sequential model
model = Sequential()

# adding convolutional layers with padding
model.add(Conv2D(32, kernel_size=(3,3), padding='same', activation='relu', input_shape=(28,28,1)))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))

model.add(Conv2D(64, kernel_size=(3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))

model.add(Conv2D(128, kernel_size=(3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))

# flattening and adding dense layers
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(10, activation='softmax'))

# printing model summary
model.summary()

```

---
# Day 93: Backpropagation in cnns: a quick breakdown

cnns learn by adjusting their parameters through backpropagation. here's how:

1. **forward propagation:**
    - input passes through **convolution → relu → max pooling → flatten → fully connected layers → output**.
2. **backpropagation:**
    - calculate **loss** and propagate errors backward.
    - use the **chain rule** to compute gradients for weights, biases, and activations.
3. **layer-wise backprop:**
    - **convolutional layers:** errors backpropagate through filters.
    - **max pooling:** only max-selected neurons contribute gradients.
    - **flattening:** reshapes gradients before sending them back.

Notes: 
![alt text](06-Convolutional-Neural-Network/images/day93_notes1.jpg) 
![alt text](06-Convolutional-Neural-Network/images/day93_notes2.jpg)



---
# Day 94: LeNet5, Cat Vs Dog Classification
LeNet-5, introduced by Yann LeCun in 1989, is one of the earliest convolutional neural networks (CNNs), designed for handwritten character recognition. It consists of **seven layers**:

![alt text](06-Convolutional-Neural-Network/images/day95_summary.png) 
1. **Input Layer:** 32×32 grayscale image.
2. **Conv Layer 1 (C1):** 6 filters (5×5), output size **28×28×6**.
![alt text](06-Convolutional-Neural-Network/images/day95_first_layer.png) 
3. **Pooling Layer 1 (S2):** Average pooling (2×2), output **14×14×6**.
![alt text](06-Convolutional-Neural-Network/images/day95_second_layer.png) 
4. **Conv Layer 2 (C3):** 16 filters (5×5), selectively connected, output **10×10×16**.
![alt text](06-Convolutional-Neural-Network/images/day95_third_layer.png)
5. **Pooling Layer 2 (S4):** Average pooling (2×2), output **5×5×16**.
![alt text](06-Convolutional-Neural-Network/images/day95_4th_layer.png) 
6. **Fully Connected (C5):** 120 neurons, connected to all **5×5×16** inputs.
![alt text](06-Convolutional-Neural-Network/images/day95_fifth_layer.png) 
7. **Fully Connected (F6):** 84 neurons.
![alt text](06-Convolutional-Neural-Network/images/day95_sixth_layer.png) 
8. **Output Layer:** Softmax with **10 classes (digits 0-9)**.
![alt text](06-Convolutional-Neural-Network/images/day95_outputlayer.png) 

Implementation:
``` python
def build_lenet(input_shape):
  # Define Sequential Model
  model = tf.keras.Sequential()
  
  # C1 Convolution Layer
  model.add(tf.keras.layers.Conv2D(filters=6, strides=(1,1), kernel_size=(5,5), activation='tanh', input_shape=input_shape))
  
  # S2 SubSampling Layer
  model.add(tf.keras.layers.AveragePooling2D(pool_size=(2,2), strides=(2,2)))

  # C3 Convolution Layer
  model.add(tf.keras.layers.Conv2D(filters=6, strides=(1,1), kernel_size=(5,5), activation='tanh'))

  # S4 SubSampling Layer
  model.add(tf.keras.layers.AveragePooling2D(pool_size=(2,2), strides=(2,2)))

  # C5 Fully Connected Layer
  model.add(tf.keras.layers.Dense(units=120, activation='tanh'))

  # Flatten the output so that we can connect it with the fully connected layers by converting it into a 1D Array
  model.add(tf.keras.layers.Flatten())

  # FC6 Fully Connected Layers
  model.add(tf.keras.layers.Dense(units=84, activation='tanh'))

  # Output Layer
  model.add(tf.keras.layers.Dense(units=10, activation='softmax'))

  # Compile the Model
  model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(lr=0.1, momentum=0.0, decay=0.0), metrics=['accuracy'])

  return model

```


---
# Day 95: GPU slow than CPU - well in my case?

Today, I trained an MNIST model on both CPU (Ryzen 7 6000) and GPU (RTX 3050 Ti), expecting a significant speedup with the GPU. Instead, the CPU performed **slightly faster**, and when I tried adding a **small CNN**, my **GPU environment crashed**, while the CPU handled it fine (but slower).

- The GPU initially took longer due to **kernel warm up** and **data transfer overhead**.
- **Batch Size Impact** – GPUs perform best with **larger batch sizes** (e.g., 512+), while I used a smaller batch.
- **Data Bottlenecks** – My CPU handled **data preloading better**, while the GPU might have suffered from inefficient memory access.
- **GPU Utilization** – The GPU wasn’t fully utilized, likely due to **suboptimal parallelism** in my setup.

And When I added **even a small convo layer**, my **GPU environment crashed**, but the CPU ran it (slowly). here’s what possible reason i estimated?

- My RTX 3050 Ti has **4GB VRAM**, and it might be running out.
- Despite setting up a dedicated GPU environment (with **proper CUDA, cuDNN, and TensorFlow versions**), I couldn’t debug this issue today.
- There might be a **compatibility issue** between TensorFlow, CUDA, and my system's architecture.

Could it be a **CUDA/cuDNN issue or VRAM exhaustion?**

 I used **two separate virtual environments**:

- **CPU environment** (normal setup)
- **GPU environment** (proper versions of CUDA, TensorFlow, and cuDNN)
Still, I **couldn’t debug the crashes today**. Any suggestions on debugging the **bit architecture issue** or possible **TensorFlow config problems**? and is it abnormal CPU performing better than GPU in my optimization??

Here's the comparision:
![GPU vs CPU](06-Convolutional-Neural-Network/images/day95_with_gpu.png) 
![GPU vs CPU](06-Convolutional-Neural-Network/images/day95_with_cpu.png)

Some rough notebooks:
[Notebook: CPU](06-Convolutional-Neural-Network/code/day95_cpu.ipynb) 
[Notebook: GPU](06-Convolutional-Neural-Network/code/day95_gpu.ipynb)

---
# Day 96: Data Augmentation, Pretrained Models

Data augmentation is crucial in machine learning, especially for tasks like computer vision, to enhance model performance and prevent overfitting. It involves applying various transformations to existing data, such as rotation, translation, scaling, flipping, shearing, zooming, and adjusting brightness and contrast. These techniques help in creating a larger and more diverse training dataset, thereby improving model generalization.
![alt text](06-Convolutional-Neural-Network/images/day96_data_augmentationon_cat.png)

**Why Use Data Augmentation?**
1. **Increased Dataset Size:** Enhances model training with more examples.
2. **Regularization:** Adds noise to prevent overfitting.
3. **Improved Generalization:** Helps models perform better on unseen data.

[Notebook: data augmentation on cifar10 frog](06-Convolutional-Neural-Network/code/day96_data_augmentation_cifar.ipynb)

#### Pretrained models in CNN:
Notes:
![Notes](06-Convolutional-Neural-Network/images/day96_notes.jpg)

before deep learning took over, imagenet models relied on classical ml methods like svm, decision trees, and hand-crafted features (think hog, sift, and lbp). this worked, but scaling to millions of images? a nightmare. then alexnet (2012) happened—deep cnns trained with relu activations and dropout on gpus. it crushed traditional methods, slashing classification error rates by half.
![alt text](06-Convolutional-Neural-Network/images/day96_alexnet.png)
![alt text](06-Convolutional-Neural-Network/images/day96_alexnet_summary.png) 

vgg (2014) pushed deeper with 3x3 convolutions, proving that simplicity + depth = power. same year, googlenet (inception v1) introduced inception modules—parallel conv layers reducing parameter overhead while boosting efficiency. resnet (2015) then solved the vanishing gradient problem with skip connections, making ultra-deep networks (152 layers!) trainable.

today, pretrained models built on imagenet—resnet, vgg, inception, efficientnet—are the backbone of modern deep learning. explored these today, and yeah, standing on the shoulders of giants makes life easier.

[PaperLink: ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)

[Medium Article on alexnet](https://medium.com/@siddheshb008/alexnet-architecture-explained-b6240c528bd5)

Keras pretrained models: https://keras.io/api/applications/
![alt text](06-Convolutional-Neural-Network/images/day96_keras_pretrained_models.png)

I will test the examples from that website using their code there in following notebook:
[Notebook: Pretrained Model Testing](06-Convolutional-Neural-Network/code/day96_pretrainedmodels_testing.ipynb)


![side syb side predictions](06-Convolutional-Neural-Network/images/day96_resnet_vgnet_predictions.png)

---
# Day 97: Visualizing Convolutional Layers, Transfer learning

Today, i'll be using the article : https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/ with respect to following topics.
- Visualizing Convolutional Layers
- Pre-fit VGG Model
- How to Visualize Filters
- How to Visualize Feature Maps

[Notebook: visualizing Layers with elephant image](./06-Convolutional-Neural-Network\code\day97_visualizing_cnns.ipynb)

notes:
![Notes](06-Convolutional-Neural-Network/images/day97_notes.jpg)

#### Transfer learning:
![alt text](06-Convolutional-Neural-Network/images/day97_drivers.png)
transfer learning helps train deep learning models efficiently by leveraging pre-trained networks like vgg, resnet, or mobilenet. instead of starting from scratch, we use the convolutional base (which extracts features) and replace the fully connected layers with our own classifier.

two main approaches:
1. **feature extraction** – freeze the convolutional layers and train only the new classifier. useful when the target dataset is similar to the original dataset.
2. **fine-tuning** – unfreeze some deeper layers and retrain them along with the classifier. this helps when the target dataset is quite different.

Resource: https://www.tensorflow.org/tutorials/images/transfer_learning


# Day 98: Keras functional API

### How to use keras functional api for deep learning?
Today i went through an article: https://machinelearningmastery.com/keras-functional-api-deep-learning/

The Sequential model API is great for developing deep learning models in most situations, but it also has some limitations.

I started with the Sequential API to build familiarity:

- Architecture: Simple CNN with `Conv2D → MaxPooling → Flatten → Dense → Output`.
- Workflow:
    - Loaded data, normalized pixels (`0-1`), reshaped images (`28x28x1`), and one-hot encoded labels.
    - Built a linear stack of layers:
        
        ```
        model = Sequential([
            Conv2D(32, (3,3),
            MaxPooling2D(),
            Flatten(),
            Dense(128),
            Dense(10, activation='softmax')
        ])
        ```
        
    - Trained with `model.fit()`, achieving ~91% validation accuracy in 10 epochs.

---

### 2. Transition to Functional API
![alt text](06-Convolutional-Neural-Network/images/day98_sequential_vs_functional.png)

I  rebuilt the same model using the Functional API to see the syntax shift:

- Input Layer: Explicitly defined with `Input(shape=(28,28,1))`.
- Layer Connections: Layers are chained like functions:
    
    ```
    x = Conv2D(32, (3,3)(input_layer)
    x = MaxPooling2D()(x)
    ...
    ```
    
- Model Definition: Declared inputs/outputs explicitly:
    
    ```
    model_func = Model(inputs=input_layer, outputs=output_layer)
    ```
    
---

![alt text](06-Convolutional-Neural-Network/images/day98_functional_model.png) 
- Branch 1: `Conv2D(3x3) → MaxPooling → Flatten`.
- Branch 2: `Conv2D(5x5) → MaxPooling → Flatten`.
- Merged: Combined branches with `concatenate([branch1, branch2])`.

A more powerful model (1.1M params vs. 694K in Sequential) with higher accuracy (~92%).
![alt text](06-Convolutional-Neural-Network/images/day98_scenarios.png) 


Summary of Concepts:
- Multiple Inputs: Different data types (image + tabular) are processed separately and then combined.
- Multiple Outputs: The model can predict several things at once, e.g., class labels and regression values.
- Skip Connections: Layers are skipped and added back later (helpful for deeper models).
- Shared Layers: The same layer is applied to multiple inputs (e.g., comparing two images).
![alt text](06-Convolutional-Neural-Network/images/day98_summary_concepts.png)

---
# Day 99: Finishing Dog & Cat classifier Project:


Today was a mess but somehow made it through. Started by setting up my local GPU for model training , should’ve been easy, but ended up wasting like 2 hours battling dependency conflicts again. once i finally got that sorted, I could train the NN on the gpu.
First tried a basic CNN and it gave me a 75% val accuracy. felt good, but i wasn't gonna stop there, so i swapped to VGG16. boom>94% . then came another headache: deploying it on streamlit. the server kept complaining about model dependencies. after a ton of trial and error, i finally got it to work.
But it’s up and running now! you can check it out here: https://cat-vs-dog-classifier.streamlit.app/


What a day !!

For more information about the project refer Github Repo: https://github.com/paudelsamir/cat-vs-dog-classifier



---
# Day 100: Hidden Markov Model, Quantum Machine Learning

> ***after an intense 10-hour session yesterday working on neural networks and a project, i decided to take it slow today and shift gears. breaking the flow, i delved into some of the most critical topics in data science, particularly those related to training models.***

i stumbled upon these topics during a discussion on twitter by a few data enthusiasts, which piqued my curiosity.so i tried to write some personal thoughts on it and refined it later with ChatGPT.
So if you are here for the deep dive with the proper mathematical understanding please refer the the resources i shared at last of this blog !! Thank you !!

- here's original blog link I tried to craft: [Medium](https://medium.com/@paudelsamir/hidden-markov-models-quantum-computing-in-machine-learning-ca5e166f934f)

---

## hidden markov models: understanding sequential data

today's exploration into *hidden markov models (hmm)* opened up a fascinating world of possibilities in data science. stepping into this space felt like gaining a deeper understanding of how to work with sequential data—data where the *order* of events matters. in real life, there are many situations where we can’t directly observe the hidden process, but we can see the outcomes of it. 

> think of *weather forecasting*: i can’t directly measure the weather, but i can infer it from the mood of people. this concept of hidden states that drive observable events applies to all kinds of problems, from speech recognition to bioinformatics.

with *HMMs*, i now have a tool that lets me model this hidden process, making sense of complex, sequential data, and predicting future outcomes based on past observations. it's like gaining the ability to look behind the curtain of what’s really happening in a system, even if i can’t directly see it.

### how does it work?
HMMs use probabilistic models to represent systems that transition between a set of *hidden* states. the beauty of HMMs is that they allow us to model the dynamics of a process even when the true states remain unseen. imagine you're trying to figure out what's happening in a black-box system—HMMs give you a way to make educated guesses about those hidden processes based on the data you can observe.

---

## quantum computing in machine learning: a new frontier

moving on to *quantum computing in machine learning* (QML), this felt like a whole new level of possibility. quantum computers aren’t just faster than classical computers; they’re a completely different way of computing. they use *qubits*, which can hold multiple states simultaneously, allowing quantum computers to process large datasets and solve optimization problems much faster than traditional computers.

> in machine learning, where training models or finding the best solution can take a long time, this speed-up is a **game-changer**. quantum machine learning (QML) opens up a world where we can train complex models more efficiently and solve problems that were once considered too difficult for classical systems.

### the quantum edge
what's so mind-blowing about quantum computing is that it can handle problems that seem *impossible* for today’s hardware—especially when it comes to big data or high-dimensional problems. imagine the ability to solve optimization problems in *seconds* that would otherwise take *days* using classical methods. in the world of machine learning, this speed-up means we can approach complex problems with a whole new set of tools.

---

## real-world applications: QML at the forefront

one of the most exciting things about QML is the potential to revolutionize fields like *drug discovery* and *financial modeling*. imagine being able to process molecular data for new medicines or predict stock market trends at lightning speeds—things that would normally take days or weeks using classical methods. quantum computing offers the promise of real-time insights and hyper-efficient problem-solving.

as we’re standing on the edge of this quantum revolution, i can’t help but think about how real-world industries will evolve. quantum computing could not just *accelerate* processes, but fundamentally transform how we approach some of the world’s toughest problems.

---

## wrapping up: the future of AI and data science

both *hidden markov models* and *quantum computing in machine learning* are still developing, but they already hold the potential to transform industries and fields of research. as i continue to explore these areas, i’m excited to dive deeper into quantum algorithms, QML models, and advanced data processing techniques. the journey into AI and data science is just beginning, and the opportunities seem *limitless*. 

each step i take builds on the last, leading me to a future where quantum-enhanced AI might not just be possible, but *normal*. stay tuned, because this is just the beginning.

---

> *what do you think?* i love where this is going, and i'm looking forward to seeing how these fields develop further!

### 📚 **Resources:**

- [Hidden Markov Model Explanation – YouTube](https://www.youtube.com/watch?v=fX5bYmnHqqE&t=631s)
- [Intro to Quantum Machine Learning – YouTube](https://www.youtube.com/watch?v=prZMpThbU3E)
- [Hidden Markov Model (HMM) – Towards Data Science](https://towardsdatascience.com/hidden-markov-model-hmm-simple-explanation-in-high-level-b8722fa1a0d5/)
- [What is Quantum Machine Learning (QML)? – Medium](https://medium.com/be-tech-with-santander/what-is-quantum-machine-learning-qml-1960c83425f4)
- [Quantum Machine Learning Repository – GitHub](https://github.com/jjprietotorres/QuantumML)
- [HMM Applications – Tanmay Binaykiya](https://tanmaybinaykiya.github.io/hmm-applications)




# Day 101: Pytorch Exploring Surfacely
Today, I started exploring the Datacamp Deep Learning with PyTorch course, gaining a surface level understanding of PyTorch and implementing basic codes to familiarize myself with the framework.

![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day101_module1.png) ![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day101_module2.png)


Below is some code for revision crafted by chatgpt:
- pytorch tensors:
    - create a tensor:
        
        ```python
        
        import torch
        x = torch.tensor([1.0, 2.0, 3.0])
        ```
        
    - random tensor (for weights):
        
        ```python
        w = torch.randn(3, 4)  # shape (3, 4)
        ```
        
    - move to gpu (if available):
        
        ```python
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        x = x.to(device)
        ```
        
- checking and adding tensors:
    - inspect tensor properties:
        
        ```python
        print(x.shape, x.dtype, x.device)
        ```
        
    - element-wise addition:
        
        ```python
        y = torch.tensor([3.0, 4.0, 5.0])
        z = x + y
        ```
        
- linear layer network:
    - use `torch.nn.Linear` to define a linear transformation:
        
        ```python
        linear = torch.nn.Linear(3, 2)  # input size = 3, output size = 2
        output = linear(x)  # forward pass
        ```
        
- understanding weights:
    - access weights and biases in a layer:
        
        ```python
        print(linear.weight)
        print(linear.bias)
        ```
        
- building a simple neural network:
    
    ```python
    class SimpleNN(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.layer1 = torch.nn.Linear(3, 5)
            self.layer2 = torch.nn.Linear(5, 1)
    
        def forward(self, x):
            x = torch.relu(self.layer1(x))
            return self.layer2(x)
    
    model = SimpleNN()
    print(model)
    ```
    
- counting parameters:
    
    ```python
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params}")
    ```
    

---

### neural network architecture and hyperparameters

- activation functions:
    - relu, sigmoid, softmax (from `torch.nn.functional`):
        
        ```python
        import torch.nn.functional as F
        x = torch.tensor([-1.0, 0.0, 1.0])
        print(F.relu(x))       # [0, 0, 1]
        print(F.sigmoid(x))    # [0.2689, 0.5, 0.7311]
        print(F.softmax(x, dim=0))
        
        ```
        
- binary classifier:
    
    ```python
    model = torch.nn.Sequential(
        torch.nn.Linear(2, 1),
        torch.nn.Sigmoid()
    )
    ```
    
- cross-entropy loss:
    
    ```python
    loss_fn = torch.nn.CrossEntropyLoss()
    ```
    
- manual weight update:
    
    ```python
    with torch.no_grad():
        for param in model.parameters():
            param -= 0.01 * param.grad
    ```
    
- using optimizers:
    
    ```python
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
    optimizer.step()
    optimizer.zero_grad()  # clear gradients
    ```
---
# Day 102: Training a Neural Network with PyTorch

Just spent an hour continuing datacamp: 
![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day102_module1.png)

the content below is crafted by chatgpt for the revision purpose if needed!!!
### loading data in pytorch
use `TensorDataset` to wrap tensors and `DataLoader` to handle batching and shuffling.

```python
from torch.utils.data import TensorDataset, DataLoader
dataset = TensorDataset(X, y)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
```

### training loop

forward pass, compute loss, backward pass, update weights.

```python
for epoch in range(epochs):
    for batch in dataloader:
        optimizer.zero_grad()
        output = model(batch)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
```

### mse loss

used for regression.

```python
import torch.nn as nn
criterion = nn.MSELoss()
loss = criterion(predictions, targets)
```

### relu & leaky relu

relu: `F.relu(x)`, leaky relu: `F.leaky_relu(x, 0.01)`. relu helps with vanishing gradients, leaky relu prevents dead neurons.

### activation functions

relu is the most common. sigmoid and tanh suffer from vanishing gradients. softmax is for classification.

### learning rate & momentum

learning rate controls step size, momentum helps accelerate updates.

```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

```

### experimenting with lr & momentum

high lr → unstable, low lr → slow. use schedulers to adjust. momentum ~0.9 speeds up training but may overshoot.

```python
from torch.optim.lr_scheduler import StepLR
scheduler = StepLR(optimizer, step_size=10, gamma=0.1)
scheduler.step()
```

---
# Day 103: Evaluating and Improving Models
> just wrapped up the datacamp course : next up >> deep dive
> started diving into practical deep learning for coders (course.fast.ai)

![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day103_finished.png)

Code used in the course:

```python
# 1. layer initialization and transfer learning
import torch.nn as nn
import torchvision.models as models

model = models.resnet18(pretrained=True)  # transfer learning
for param in model.parameters():
    param.requires_grad = False  # freeze all layers

# 2. fine-tuning process
for param in model.fc.parameters():
    param.requires_grad = True  # unfreeze last layer
optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-4)

# 3. freeze layers of a model
for param in model.conv1.parameters():
    param.requires_grad = False  # freeze specific layer

# 4. layer initialization
def init_weights(m):
    if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight)  # or He initialization
        nn.init.zeros_(m.bias)

model.apply(init_weights)  # apply initialization

# 5. evaluating model performance
from torchmetrics.classification import Accuracy
accuracy = Accuracy(task="binary")  # or "multiclass" for multi-class

# 6. writing the evaluation loop
def evaluate(model, dataloader, criterion, device):
    model.eval()
    total_loss, correct = 0, 0
    with torch.no_grad():
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            y_pred = model(X)
            loss = criterion(y_pred, y)
            total_loss += loss.item()
            correct += (y_pred.argmax(1) == y).sum().item()
    return total_loss / len(dataloader), correct / len(dataloader.dataset)

# 7. calculating accuracy using torchmetrics
accuracy = Accuracy()
y_true = torch.tensor([0, 1, 1, 0])
y_pred = torch.tensor([0, 1, 0, 0])
acc = accuracy(y_pred, y_true)
print(f"Accuracy: {acc.item()}")

# 8. fighting overfitting - dropout
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc = nn.Linear(128, 64)
        self.dropout = nn.Dropout(p=0.5)  # 50% dropout

    def forward(self, x):
        x = self.fc(x)
        return self.dropout(x)  # apply dropout

# 9. experimenting with dropout
dropout_layer = nn.Dropout(p=0.3)
x = torch.rand(5, 5)
print(dropout_layer(x))  # randomly drops values

# 10. understanding overfitting
# - monitor loss gap (train vs val loss)
# - apply dropout, weight decay, augment data

# 11. improving model performance
# - batch norm: stabilizes training
model = nn.Sequential(
    nn.Linear(128, 64),
    nn.BatchNorm1d(64),
    nn.ReLU()
)


```
---

# Day 104: Crawling through DL with pytorch - Chapter 1

- around 3 hours today
- finished chapter 1 of deep learning for coders
- watched lesson 1 on youtube: https://lnkd.in/di_Ehsbq
- watched another pytorch video tensors related from campusx
- [Code Part Here >>>](08-Practical-Deep-Learning-With-Pytorch/code/day104_tensors_implementation.ipynb)


Representational images
![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day104_chapter1_book.png) 
![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day104_datablocks_api.png) 
![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day104_ml_model_at_high_level.png) 
![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day104_myths.png) 
![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day104_tensors.png) 
---


# Day 105: From Model to production : starting

- had a slower day, as expected
- spent time debugging an ML model with friends
- watched some youtube and practiced tensors
- started reading chapter 2 but didnt make significant progress

Representational Images
![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day105_autograd.png) 
![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day105_ch2.png)
---


# Day 106: Exploring Autograd and Portfolio Tweaks

- Focused on PyTorch's autograd; watched YouTube tutorials and read a few articles (no actual coding today).
- Read 2-3 pages from the "Deep Learning for Coders" book.
- Spent around 2 hours tweaking portfolio designs, mostly adapting ideas from others' designs.

![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day106_autograd_nn.png) 
![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day106_data_to_dataloaders.png)
---


# Day 107: Refining Portfolio whole day:

- spent the whole day refining the portfolio : paudelsamir.github.io
- no major progress in ML today—just watched some pytorch videos on youtube
---


# Day 108: Autograd in pytorch : deeper understanding

Autograd is a core component in pytorch that provides automatic differentiation for tensor operations, It enables gradient computation,which is essential for training machine learning models using optimization algorithms like gradient descent.

[Notebook: autograd implementation](08-Practical-Deep-Learning-With-Pytorch/code/day107_autograd.ipynb)

Images: Lession 2 @ Practical DL for coders...
![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day108_lesson2.png) 
![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day108_loss_vs_metric.png)

---

# Day 109: Pytorch Training Pipeline (Manual + Using nn.Module)

When working with pytorch, you have two main options for building your models:

one is using low level manual apporach where you have complete control over everything from defining weights to handling forward pass manually...

Here's the code implementation:
[Code Implementattion: Manual](08-Practical-Deep-Learning-With-Pytorch/code/day109_pytorch_training_pipeline.ipynb)
![Code Implementattion: Manual](08-Practical-Deep-Learning-With-Pytorch/images/day109_training_pypeline_manual.png) \


Other is nn.Module: is a high level abstraction for model building, it makes creating and training models much easier by handling much of the underlying complexity for you. it offers pre-built layers, loss functions, activation functions and other utilities.

#### Key Components of `torch.nn` in PyTorch


**1. Layers (Modules)**

- Building blocks of neural networks, like `nn.Linear`, `nn.Conv2d`, `nn.LSTM`, and activations like `nn.ReLU`.
Handle the flow of data through the model, automatically managing weights.

**2. Loss Functions**

- Measure how well the model’s predictions match the target values.
- Common ones: `nn.MSELoss()`, `nn.CrossEntropyLoss()`, `nn.BCEWithLogitsLoss()`.

**3. Optimizers**

- Update model parameters based on gradients.
- Common optimizers: `torch.optim.SGD`, `torch.optim.Adam`, `torch.optim.RMSprop`.

**4. Neural Network Modules (`nn.Module`)**

- Base class for all neural network models.
- Provides functions like `forward()`, `parameters()`, `state_dict()`.
- Simplifies model building by automatically managing layers and gradients.

**5. Data Handling (DataLoader, Dataset)**

- `Dataset`: Custom class to handle data.
- `DataLoader`: Manages batching, shuffling, and loading data in parallel.

**6. Utilities**

- **Dropout**: Regularization technique (`nn.Dropout`).
- **BatchNorm**: Normalizes activations (`nn.BatchNorm1d`).
- **Embedding**: Maps indices to vectors for NLP tasks (`nn.Embedding`).

**7. Activation Functions**

- Introduce non-linearity.
- Common ones: `nn.ReLU`, `nn.Sigmoid`, `nn.Tanh`.

Implementation:
![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day109_training_pipeline_auto.png)

---

# Day 110: Dataset & DataLoader Class in PyTorch

The `Dataset` class in PyTorch is used to represent your data. It allows you to define how to load and preprocess your data.

**Key Methods**:
1. `__init__`: Initialize the dataset (e.g., load files, set up variables).
2. `__len__`: Return the total number of samples.
3. `__getitem__`: Retrieve a single data sample.

**Example**:
```python
from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]
```

The `DataLoader` class is used to load data in batches, shuffle it, and handle parallel processing.

**Key Parameters**:
- `batch_size`: Number of samples per batch.
- `shuffle`: Whether to shuffle the data.
- `num_workers`: Number of subprocesses for data loading.

**Example**:
```python
from torch.utils.data import DataLoader

dataset = CustomDataset(data, labels)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

for batch_data, batch_labels in dataloader:
    print(batch_data, batch_labels)
```
[Notebook: Dataloader Implementation and Improving our Previous Model](08-Practical-Deep-Learning-With-Pytorch/code/day110_dataloader.ipynb) 

Batches separation using dataloader (just simple dataset):
![Notebook: Dataloader Implementation](08-Practical-Deep-Learning-With-Pytorch/images/day110_dataloader.png)

Training on our breast cancer dataset using dataloader improving from 52 to 95 percent accuracy:
![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day110_dataloader_on_breast_cancer.png)
---


# Day 111: ANN on fashion MNIST, GELU
Today was fun, first i started implementing ANN with pytorch taking a dataset from Kaggle and with the help of some reference videos. i coded my NN.
![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day111_ann_pytorch.png)


then, started reading the paper about GELU (Gaussian Error Linear Unit), an activation function used in deep learning. here are the key points:

activation functions like ReLU and tanh are common but GELU provides a smooth approximation to ReLU while incorporating probabilistic properties.
![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day111_GELU.png)
---


# Day 112: ANN on larger fmnist dataset with GPU(local), GELU/SiLU history:

Yesterday I just trained on 6000 data only with CPU using pytroch leading 82 percent accuracy,
today from scratch again i trained full fmnist dataset around 60000 data using gpu upto 100 epochs and got around 89 percent accuracy. 

![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day112_gpu_training.png) 
![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day112_GELU_SiLU.png)
Summary:
![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day112_gelu.png)
---

# Day 113: Optimizing FMNIST NN using Dropouts, Regularization and Batch Normalization in Pytorch
Yesterday, i was trying training neural network on whole dataset with around 60k data, got around 88 percent accuracy on test dataset, but while testing with train data, noticed that my model is overfitting. acheiving around 98 percent of accuracy, so today's my goal is just simple reduce overfittingk

i have plan applying Dropout, Regularizaiton and batch norm. and let's see what happens?

achieve 81 on test tooo. 
![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day113_able_to_achieve_81_on_training.png)

well this isn't perfect model! i'll be working on hyperparameter tuning too later for the best later///////

>[!IMPORTANT]
>Lately, have been really enjoying reading research papers, but on searching of efficient way to understand and learn,
> I found an insightful lecture by the legend [Andrew Ng](https://youtube.com/watch?v=733m6qBH-jI&t=3196s). Highly recommended for anyone diving into this topic!
  on youtube
> so spent few hours watching and took notes to summarize everything my way

![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day113_how.jpg) ![alt text](08-Practical-Deep-Learning-With-Pytorch/images/day113_why.jpg)
---

# Day 114: RNNs revisited, Karpathy's blog, Project Planning

Today i simply explored RNNs, simply revising it as well as reading blogs from amazing people.
here're some glimpse from karpathy blog:
![alt text](07-Recurrent-Neural-Network/day114_akblog.png) ![alt text](07-Recurrent-Neural-Network/day114_karpathy_rnns.png)
rnns can remember stuff — normal neural nets forget everything after one step. rnns don’t. they keep a memory so they’re great with stuff like text, music, or anything that happens in a sequence.

they feel like magic — you train them on a bunch of text, and they start making up new text that kinda makes sense. even with simple setups, the results can be weirdly good.

rnns work with any length of input — they don’t need the data to be a fixed size. you can feed them short or long sentences, doesn’t matter. they just keep looping through it.

you can use rnns even on non-sequence data — stuff like images can be processed step-by-step if you want. like looking at one part at a time and building understanding gradually.

they learn programs, not just patterns — training an rnn is kinda like teaching it how to think step by step, not just react. like giving it a tiny brain with memory.

they’re better than they look — people used to think rnns are hard to train, but turns out they’re not that tricky. even small ones can do surprising things.

Inspring from this: https://github.com/lilianweng/stock-rnn
am starting handson project with RNN ( predicting stock price) probably will use tensorflow
also i have been exploring pytorch since while, so staring a project with pytorch too. 

---
# Day 115: Classifying Footballers with their Eyes - Day 1

> Started a small project: classify footballers by just their eyes.

The idea is to turn it into a simple game—human vs AI. 

**Data Collection:**  
- Scraped ~3,000 images of 25 well-known players from google.
- Used OpenCV to automatically crop out just the eye regions from each image.
- Split the dataset into train, validation, and test sets to ensure fair evaluation.

- Small dataset size may limit accuracy.
- Some players have similar eye features, making classification harder.
- Lighting, image quality, and cropping consistency can affect results.

![alt text](10-Projects-Based-ML-DL/01-Image-Classification/day115.png) ![alt text](10-Projects-Based-ML-DL/01-Image-Classification/day115i.png)
---

# Day 116: Classifying Footballers with their Eyes – Day 2

Turned the eye classifier into an interactive game: **Human vs AI – who can recognize footballers better?**

**Key Progress:**
- Trained a ResNet18 model on the eye dataset – reached ~70% accuracy 🎯
- Built a Streamlit app for a fun player experience
- Added game mechanics: scoring, streak counter, and round tracking

**Fun Features:**
- See AI's "thought process" – shows confidence levels for different players
- Scoreboard tracking your performance vs the AI
- Clean interface focused on the eye images

![alt text](10-Projects-Based-ML-DL/01-Image-Classification/day116_demo.png)

Still refining everything, but the basic game loop works! Challenging but surprisingly addictive

---

# Day 117: YOLO (You Only Look Once)
Instead of scanning images region-by-region like traditional methods, YOLO (You Only Look Once) processes the entire image in a single pass through a CNN, predicting all object bounding boxes and class labels simultaneously. This end-to-end approach makes YOLO extremely fast.

**Network Architecture:**
![YOLO Architecture](09-State-of-the-Art-DL/images/day117_yolo_architecture.png)

- Inspired by GoogLeNet, but with a simpler design
- 24 convolutional layers followed by 2 fully connected layers
- Uses 1×1 convolutions to reduce feature map size before applying 3×3 convolutions
- The final output layer produces a 7×7×30 tensor representing bounding boxes and class probabilities

**Performance:**  
Combining Fast R-CNN with YOLO achieves a 75.0% mean Average Precision (mAP).

---
# Day 118: 




---
# Day 119: 




---
# Day 120: 




---
# Day 121: 




---
# Day 122: 




---
# Day 123: 




---
# Day 124: 




---
# Day 125: 




---
# Day 126: 




---
# Day 127: 



---
# Day 128: 




------------------------------


<div id="bottom"></div>
<div align="right">
<a href="#top">
    <img src="https://img.shields.io/badge/▲_Back_to_Top-0D1117?style=for-the-badge&logoColor=white&labelColor=161B22" alt="Back to Top" />
</a>
</div>
<!-- 
### Helping Hands
- [Kris Naik - YouTube](https://www.youtube.com/channel/UCNU_lfiiWBdtULKOw6X0Dig)
- [Statquest - YouTube](https://www.youtube.com/c/joshstarmer)
- [GitHub Repos](https://github.com/paudelsamir?tab=stars)
- [Medium Articles](https://medium.com/)
- [Machine Learning Mastery](https://machinelearningmastery.com/)
- [Kaggle Datasets - Glossary](https://www.kaggle.com/code/shivamb/data-science-glossary-on-kaggle)
- [Arxiv.org](https://arxiv.org/)
 -->


