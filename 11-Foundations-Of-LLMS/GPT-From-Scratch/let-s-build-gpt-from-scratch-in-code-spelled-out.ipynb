{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n",
    "----\n",
    "* Inspired by Andrej Karpathy's [\"Let's build GPT: from scratch, in code, spelled out.\"](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n",
    "* Supplementary links\n",
    "    - [Attention is All You Need paper](https://arxiv.org/abs/1706.03762) from Google\n",
    "    - OpenAI [GPT-3 Paper](https://arxiv.org/abs/2005.14165)\n",
    "    - OpenAI [ChatGPT blog post](https://openai.com/blog/chatgpt/)\n",
    "    - [nanoGPT](https://github.com/karpathy/ng-video-lecture/tree/master)\n",
    "    - Lambda GPU Cloud via [lambda labs](https://lambdalabs.com) provides GPU access for model training. The best and easiest way to spin up an on-demand GPU instance in the cloud is if you can ssh to: https://lambdalabs.com . If you prefer to work in notebooks, I think the easiest path today is [**Google Colab.**](https://colab.research.google.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "# Table of Contents\n",
    "------------------\n",
    "- [0. Introduction](#0)\n",
    "- [1. Baseline Bigram Language Model (LM)](#1)\n",
    "    - [1.1. Data Reading & Exploration](#101)\n",
    "    - [1.2. Tokenization & Train-Dev Split](#102)\n",
    "    - [1.3. Data Loader: Batches](#103)\n",
    "    - [1.4. Bigram LM](#104)\n",
    "    - [1.5. Training Bigram LM](#105)\n",
    "- [2. Self-Attention](#2)\n",
    "    - [2.1. V1: Averaging Past Context with `For` Loops - Weakest Form of Aggregation](#201)\n",
    "    - [2.2. Trick: Matrix Multiplication as Weighted Aggregation](#202)\n",
    "    - [2.3. V2: Matrix Multiplication](#203)\n",
    "    - [2.4. V3: Softmax](#204)\n",
    "    - [2.5. Bigram LM Code Tweaks: Robust Token Embedding Dimension](#205)\n",
    "    - [2.6. Bigram LM Code Tweaks: Positional Encoding](#206)\n",
    "    - [2.7. V4: **SELF-ATTENTION**](#207)\n",
    "    - [2.8. 6 Key Notes on Attention](#208)\n",
    "- [3. Transformers](#3)\n",
    "    - [3.1. Single Self-Attention](#301)\n",
    "    - [3.2. Multi-Head Attention (MHA)](#302)\n",
    "    - [3.3. Feed-Forward Network (FFN)](#303)\n",
    "    - [3.4. Residual Connections](#304)\n",
    "    - [3.5. Layer Normalization (`LayerNorm`)](#305)\n",
    "    - [3.6. Scaling Up the Model](#306)\n",
    "    - [3.7. Putting It All Together](#307)\n",
    "    - [3.8. Encoder vs Decoder vs Encoder-Decoder Transformers](#308)\n",
    "    - [3.9. Quick Walkthrough of `nanoGPT`](#309)\n",
    "    - [3.10. **ChatGPT, GPT-3:** pretraining vs. finetuning, **RLHF**](#310)\n",
    "- [4. Conclusion](#4)\n",
    "------\n",
    "\n",
    "# Appendix\n",
    "---------------\n",
    "## Figures\n",
    "- [A1. Query, Key, Value in Self-Attention Explained.](#a1)\n",
    "- [A2. Scaled Dot-Product Attention.](#a2)\n",
    "- [A3. Attention is All You Need - Transformer Model Architecture.](#a3)\n",
    "- [A4. Multi-Head Attention.](#a4)\n",
    "- [A5. Feed-Forward Network.](#a5)\n",
    "- [A6. Residual Block: (1)-Residual Connection on the Side of the Layer, (2)-Layer on the Side of the Residual Connection.](#a6)\n",
    "- [A7. Layer Normalization. ](#a7)\n",
    "- [A8. Dropout.](#a8)\n",
    "\n",
    "## Equations\n",
    "- [B1. Scaled Dot-Product Attention](#b1)\n",
    "- [B2. Multi-Head Attention](#b2)\n",
    "- [B3. Feed-Forward Network](#b3)\n",
    "- [B4. Residual Connections](#b4)\n",
    "- [B5. Layer Normalization](#b5)\n",
    "\n",
    "## Definitions/Explanations\n",
    "- [C1. Attention](#c0)\n",
    "- [C2. Masking](#c01)\n",
    "- [C3. Translation Invariance](#c1)\n",
    "- [C4. Self-Attention](#c101)\n",
    "- [C5. Self-Attention vs Cross Attention](#c102)\n",
    "- [C6. Residual Connections](#c2)\n",
    "- [C7. Layer Normalization](#c3)\n",
    "- [C8. Dropout](#c4)\n",
    "- [C9. Saving & Loading Model & Model Weights](#c5)\n",
    "\n",
    "## [References](#r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "<br><br><a id=\"0\"></a>\n",
    "# 0. Introduction\n",
    "---------------------------------\n",
    "We build a GPT, following the paper \"Attention is All You Need\" and OpenAI's GPT-2 / GPT-3. We talk about connections to ChatGPT, which has taken the world by storm. We watch GitHub Copilot, itself a GPT, help us write a GPT (meta :D!). We'll utilise the small \"[Tiny Shakespeare](https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt)\" dataset, which contains all of Shakespeare's work in a single file under $1$ MB, instead of a bigger chunk-sized entire internet dataset. This will tremendously reduce our parameter size from the billions. For simplicity and speed, our input tokens will be characters and not words. It's essential to watch the earlier makemore videos to get comfortable with the autoregressive language modeling framework, and basics of tensors & `PyTorch`'s **`torch.nn`**, which we take for granted in this video.\n",
    "\n",
    "**ChatGPT** is a language model (LM) developed & designed by OpenAI to understand and generate human-like text sequentially based on the input it receives. You can use it for various natural language processing tasks, such as answering questions, having conversations, generating text, and more. For the same input, it provides different outputs when it's rerun numerous times. This shows that it's a probabilistic LM.\n",
    "\n",
    "<u>Generative Pre-trained Transformer,</u> otherwise known as **GPT**, is a LM that is trained on a siginificant large size of text data to understand and generate human-like text sequentially. The \"transformer\" part refers to the model's architecture, which was introduced and inspired by the 2017 \"[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\" paper.\n",
    "\n",
    "Current implementations from **micrograd (n-grams LM)** to **makemore (MLP, CNN, RNN)** and now **GPT** follow a few key papers:\n",
    "\n",
    "- Bigram (one character predicts the next one with a lookup table of counts)\n",
    "- MLP, following [Bengio et al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "- CNN, following [DeepMind WaveNet 2016](https://arxiv.org/abs/1609.03499) (in progress...)\n",
    "- RNN, following [Mikolov et al. 2010](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)\n",
    "  - LSTM, following [Graves et al. 2014](https://arxiv.org/abs/1308.0850)\n",
    "  - GRU, following [Kyunghyun Cho et al. 2014](https://arxiv.org/abs/1409.1259)\n",
    "- Transformer, following [Vaswani et al. 2017](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:15.535328Z",
     "iopub.status.busy": "2024-06-12T15:53:15.534733Z",
     "iopub.status.idle": "2024-06-12T15:53:15.896971Z",
     "shell.execute_reply": "2024-06-12T15:53:15.896235Z",
     "shell.execute_reply.started": "2024-06-12T15:53:15.535295Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:15.899073Z",
     "iopub.status.busy": "2024-06-12T15:53:15.898706Z",
     "iopub.status.idle": "2024-06-12T15:53:19.073411Z",
     "shell.execute_reply": "2024-06-12T15:53:19.072635Z",
     "shell.execute_reply.started": "2024-06-12T15:53:15.899049Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F # this is for activation functions like ReLU\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "<br><br><a id=\"1\"></a>\n",
    "# 1. Baseline Bigram Language Model (LM)\n",
    "-----------\n",
    "\n",
    "We establish a simple bigram language model (LM) to get started as our baseline LM. We build our dataset, create our input tokens, split it into train and validation sets, create our bigram LM, train the model and then measure the model performance via cross-entropy loss.\n",
    "\n",
    "\n",
    "<a id=\"101\"></a>\n",
    "## 1.1. Data Reading & Exploration\n",
    "-----------\n",
    "\n",
    "Let's download the Tiny Shakespeare dataset, which is about a $1$ MB file, that contains all of Shakespeare's work in one single text file. We read in the text file and, upon inspection, discover it has ~$1$ million characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:19.074917Z",
     "iopub.status.busy": "2024-06-12T15:53:19.074533Z",
     "iopub.status.idle": "2024-06-12T15:53:20.347202Z",
     "shell.execute_reply": "2024-06-12T15:53:20.346277Z",
     "shell.execute_reply.started": "2024-06-12T15:53:19.074891Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  1 1089k    1 13780    0     0  34182      0  0:00:32 --:--:--  0:00:32 34278\n",
      "100 1089k  100 1089k    0     0  1783k      0 --:--:-- --:--:-- --:--:-- 1785k\n"
     ]
    }
   ],
   "source": [
    "!curl -o input.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "Since we are using character instead of word/sub-word tokens, our vocabulary will just be the different unique characters that appear in our dataset. Notice that the $1$st character is the **newline character**, `\\n`, and the $2$nd is the **space character,** `\" \"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])  # Print first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:20.348857Z",
     "iopub.status.busy": "2024-06-12T15:53:20.348561Z",
     "iopub.status.idle": "2024-06-12T15:53:20.372055Z",
     "shell.execute_reply": "2024-06-12T15:53:20.371055Z",
     "shell.execute_reply.started": "2024-06-12T15:53:20.348829Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"102\"></a>\n",
    "## 1.2.  Tokenization & Train-Dev Split\n",
    "-----------\n",
    "A **tokenizer** is a component used in natural language processing (NLP) to convert raw text of strings into some sequence of integers known as \"<u>tokens</u>\". An **encoder** allows us turn tokens represented as strings into integers, and a **decoder** allows us to turn our tokens represented as integers back into strings.\n",
    "\n",
    "We have a very simple character-level tokenizer. There are many different tokenizers, like Google's [SentencePiece](https://github.com/google/sentencepiece) schema (**a subword tokenizer**) or OpenAI's [tiktoken](https://github.com/openai/tiktoken) (**a byte pair encoding, BPE, tokenizer**). These tokenizers operate fundamentally on a sub-word level, which means their vocabulary is much larger (since there are many more permutations of subwords than characters). But the general idea remains the same, we are just turning strings into integers and vice versa.\n",
    "\n",
    "The large vocabulary size of <u>tiktoken</u>, which is $50257$, enables us to encode a string to a shorter sequence of integers as compared to our own tokenizer of size 65 which generates a longer sequence of integer tokens. The larger the vocabulary size, the shorter the sequence of integer tokens.\n",
    "\n",
    "So, once we define our encoder and decoder we can then encode our entire dataset. Once we have our encoded dataset, we perform a $90\\%:10\\%$ train-validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:20.375048Z",
     "iopub.status.busy": "2024-06-12T15:53:20.374744Z",
     "iopub.status.idle": "2024-06-12T15:53:20.383565Z",
     "shell.execute_reply": "2024-06-12T15:53:20.382731Z",
     "shell.execute_reply.started": "2024-06-12T15:53:20.375023Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "# Creating character to integer mapping\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "# Creating integer to character mapping\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Encoding function (using lambda)\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "\n",
    "# Decoding function\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "# Test\n",
    "print(encode(\"hi there\"))\n",
    "print(decode(encode(\"hi there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1115394]), torch.int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
       "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
       "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
       "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
       "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
       "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
       "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
       "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
       "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
       "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
       "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
       "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
       "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
       "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
       "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
       "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
       "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
       "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
       "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
       "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
       "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
       "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
       "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
       "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
       "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
       "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
       "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
       "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
       "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
       "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
       "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
       "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
       "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
       "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
       "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
       "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
       "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
       "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
       "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
       "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
       "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
       "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
       "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
       "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
       "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
       "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
       "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
       "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
       "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
       "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
       "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))  # 90% for training\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0), device(type='cuda', index=0))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.device, val_data.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"103\"></a>\n",
    "## 1.3.  Data Loader: Batches\n",
    "-----------\n",
    "Let's prepare the model input. We will never feed our model the entire sequence of tokens as prompt at once.\n",
    "Instead, we will feed it a **randomly drawn but consecutive sequence of tokens.** The model will then predict the next token in the sequence from this prompt.\n",
    "\n",
    ">We refer to these consecutive, size-limited input sequences of tokens as **blocks.**\n",
    "Size-limited means that blocks can have a length of up to `block_size`.\n",
    "\n",
    "When we sample our dataset, we grab a block of $8$ characters of context plus 1 final character as target. The goal is to learn from the target character during training, predict from the target during evaluation, and generate text from the target during inference.\n",
    "\n",
    "Suppose we have a `block_size` of $8$, each block actually contains 8 different examples, one for each possible sequence starting with the $1$st initial character. It is important to show our model examples with fewer than `block_size` characters, so that it can learn how to generate text with as little as one character context. Essentially, the transformer should be robust to varying context lengths (1 to `block_size`), which is essential during inference (adequate text generation during sampling with as little as context length of 1 to `block_size`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:20.699048Z",
     "iopub.status.busy": "2024-06-12T15:53:20.698497Z",
     "iopub.status.idle": "2024-06-12T15:53:20.709531Z",
     "shell.execute_reply": "2024-06-12T15:53:20.708711Z",
     "shell.execute_reply.started": "2024-06-12T15:53:20.699014Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8  # Length of the context window\n",
    "train_data[:block_size+1]\n",
    "\n",
    "# Input (block_size=8): \"Hello Wo\"\n",
    "# Target (next char): \"r\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:20.711144Z",
     "iopub.status.busy": "2024-06-12T15:53:20.710687Z",
     "iopub.status.idle": "2024-06-12T15:53:20.719520Z",
     "shell.execute_reply": "2024-06-12T15:53:20.718652Z",
     "shell.execute_reply.started": "2024-06-12T15:53:20.711119Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18], device='cuda:0') the target: 47\n",
      "when input is tensor([18, 47], device='cuda:0') the target: 56\n",
      "when input is tensor([18, 47, 56], device='cuda:0') the target: 57\n",
      "when input is tensor([18, 47, 56, 57], device='cuda:0') the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58], device='cuda:0') the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1], device='cuda:0') the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15], device='cuda:0') the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47], device='cuda:0') the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "# Let's say we have text: \"Hello Wo\"\n",
    "# x = ['H', 'e', 'l', 'l', 'o', ' ', 'W', 'o']\n",
    "# y = ['e', 'l', 'l', 'o', ' ', 'W', 'o', 'r']\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]  # Get characters up to position t+1\n",
    "    target = y[t]      # Get target character at position t\n",
    "    print(f\"when input is {context} the target: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:20.720878Z",
     "iopub.status.busy": "2024-06-12T15:53:20.720614Z",
     "iopub.status.idle": "2024-06-12T15:53:20.729444Z",
     "shell.execute_reply": "2024-06-12T15:53:20.728490Z",
     "shell.execute_reply.started": "2024-06-12T15:53:20.720855Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: First Ci\n",
      "Y: irst Cit\n",
      "F -> i\n",
      "Fi -> r\n",
      "Fir -> s\n",
      "Firs -> t\n",
      "First ->  \n",
      "First  -> C\n",
      "First C -> i\n",
      "First Ci -> t\n"
     ]
    }
   ],
   "source": [
    "print(\"X:\", decode(x.tolist()))\n",
    "print(\"Y:\", decode(y.tolist()))\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1].tolist()  # Get characters up to position t+1\n",
    "    target = y[t].tolist()      # Get target character at position t\n",
    "    print(f\"{decode(context)} -> {decode([target])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, the representation of X and y is different from our `makemore` version. In makemore, we had a **fixed input context size,** and we padded with `.` in cases where the names were not the full context length. Here, we append each subsequent character step-by-step to ensure the LM learns robustly to **varying context lengths from 1 to `block_size`.**\n",
    "\n",
    "\n",
    "Now, we feed in the dataset in **batches** of multiple chunks of text that are all stacked up like in a single tensor. This is done for efficiency and speed since GPUs are good at parallel processing/computing. The batches are processed simultaneously and independently of each other.\n",
    "\n",
    "Since we have `batch_size` 4 and `block_size` 8, one batch will contain a $4\\times8$ tensor $X$ and a $4\\times8$ tensor $Y$.\n",
    "\n",
    "* Each row, as a single sample, contains 8 different example contexts, one for each possible sequence starting with the $1$st character until the `block_size`.\n",
    "* There are 4 rows for the 4 samples in a single batch of `batch_size` 4. Each row has 8 examples, therefore there's a total of 32 training samples.\n",
    "* Each element in the 4x8 tensor Y contains a single target, each corresponding to one of the 32 examples in X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:20.731143Z",
     "iopub.status.busy": "2024-06-12T15:53:20.730797Z",
     "iopub.status.idle": "2024-06-12T15:53:20.772537Z",
     "shell.execute_reply": "2024-06-12T15:53:20.771462Z",
     "shell.execute_reply.started": "2024-06-12T15:53:20.731115Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([4, 8])\n",
      "Target batch shape: torch.Size([4, 8])\n",
      "tensor([[14, 59, 58,  1, 47, 44,  1, 63],\n",
      "        [47, 56,  1, 42, 43, 39, 58, 46],\n",
      "        [50, 42,  8,  0,  0, 19, 56, 53],\n",
      "        [52, 42,  1, 57, 53,  1, 42, 53]], device='cuda:0') \n",
      "\n",
      " tensor([[59, 58,  1, 47, 44,  1, 63, 53],\n",
      "        [56,  1, 42, 43, 39, 58, 46,  0],\n",
      "        [42,  8,  0,  0, 19, 56, 53, 53],\n",
      "        [42,  1, 57, 53,  1, 42, 53, 58]], device='cuda:0')\n",
      "Input: But if y -> Target: ut if yo\n",
      "Input: ir death -> Target: r death\n",
      "\n",
      "Input: ld.\n",
      "\n",
      "Gro -> Target: d.\n",
      "\n",
      "Groo\n",
      "Input: nd so do -> Target: d so dot\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4  # Number of sequences in a batch\n",
    "block_size = 8  # Length of the context window\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # Random starting points for each sequence\n",
    "    x = torch.stack([data[t:t+block_size] for t in ix])  # Create batch of input sequences\n",
    "    y = torch.stack([data[t+1:t+block_size+1] for t in ix])  # Create batch of target sequences\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "xb, yb = get_batch('train')\n",
    "print(\"Input batch shape:\", xb.shape)\n",
    "\n",
    "print(\"Target batch shape:\", yb.shape)\n",
    "\n",
    "print(xb,\"\\n\\n\", yb)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    print(f\"Input: {decode(xb[i].tolist())} -> Target: {decode(yb[i].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"104\"></a>\n",
    "## 1.4. Bigram LM\n",
    "-----------\n",
    "Lets start with the simplest model possible, which is a bigram language model, a character-level language model that generates the next character based on the previous one and bases its generation on the probability of two characters occurring together. \n",
    "\n",
    "### Forward pass\n",
    "Below we implement the bigram language model using an embedding with exactly `vocab_size x vocab_size`. Embedding a single integer between `0` and `vocab_size-1` would return a tensor of length `vocab_size`. This acts like a lookup table, where passing in a row index between `0` and `vocab_size-1` would return a row with length `vocab_size`. We simply initialize an embedding that maps each token to a probability distribution for the next token.\n",
    "\n",
    "If we pass in a multi-demensional vector as input, the embedding simply returns a tensor with the same dimensions, excecpt each integer gets turned into a vector of `vocab_size`. For example, if we pass in an input with dimensions `BxT`, then the output will be have dimension `BxTxC`.\n",
    "\n",
    "* `B` is the \"batch\" dimension, indicating which sequence of the batch we are in, equal to `batch_size`.\n",
    "* `T` is the \"time\" dimension, indicating our position in the sequence, equal to `block_size`.\n",
    "* `C` is the \"channel\" dimension, indicating which neuron we are talking about, equal to `vocab_size`.\n",
    "\n",
    "\n",
    "\n",
    "Ensure you pass in `logits` and `target` with the right **shape** when calling `F.cross_entropy`. The loss we expect, given a uniform distribution, to make a prediction is: <br>\n",
    "$$-ln(\\frac{1}{vocab\\_size})=-ln(\\frac{1}{65})=4.17387$$ <br>\n",
    "\n",
    "However, we get a higher loss of $\\boldsymbol{4.8786}$ which shows that initial predictions are not super diffused or evenly spread out across the entire `vocab_size` and contain a bit of entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:20.773867Z",
     "iopub.status.busy": "2024-06-12T15:53:20.773615Z",
     "iopub.status.idle": "2024-06-12T15:53:20.784268Z",
     "shell.execute_reply": "2024-06-12T15:53:20.783209Z",
     "shell.execute_reply.started": "2024-06-12T15:53:20.773839Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.6463, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)  # For reproducibility\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Create embedding table of size vocab_size x vocab_size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        self.to(device)  # Move model to GPU if available\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "    # Example shapes:\n",
    "    # idx shape: (4, 8) - batch_size=4, block_size=8\n",
    "    # targets shape: (4, 8) - same as idx\n",
    "        logits = self.token_embedding_table(idx)  # shape: (4, 8, 65)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape  # B=4, T=8, C=65\n",
    "            logits = logits.view(B*T, C)     # shape: (32, 65)\n",
    "            targets = targets.view(B*T)       # shape: (32)\n",
    "            loss = F.cross_entropy(logits, targets) # notes on how this works:\n",
    "            # logits: raw scores for each character in the vocabulary\n",
    "            # targets: actual next character indices\n",
    "            # F.cross_entropy works in this case by computing the softmax of logits and then calculating the negative log likelihood loss\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1) ... (B, T+max_new_tokens)\n",
    "        return idx\n",
    "\n",
    "bigramLM = BigramLanguageModel(vocab_size)\n",
    "logits, loss = bigramLM(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate\n",
    "Let's add the ability to generate characters to our model. To generate our output, we take the logits which would be the conditional probabilities of two characters occurring together after the last function, and extract the last token in each block because that will be the token that we will use for generating the succeeding characters. Then, we apply softmax on the last dimension which contains the output probabilities.\n",
    "\n",
    "\n",
    "`generate` takes some context and uses it to generate `max_new_tokens` more characters.\n",
    "\n",
    "For each new token up to `max_new_tokens`:\n",
    "\n",
    "* call the forward pass with the given context `idx` (without targets) to get the logits\n",
    "* \"pluck out\" the logits for just the last position in dimension `T` (since our forward pass acts on all `BxT` inputs and returns `BxTxC`)\n",
    "* apply ***softmax*** to the last position (`BxC`) to transform into probabilities\n",
    "> **Softmax** essentially amplifies the differences between the elements of the input vector, converting them into probabilities that represent the likelihood of each class or category. The softmax function transforms logits (raw scores) into probabilities that sum up to 1, and each probability represents the likelihood of a particular class. The distribution of these probabilities depends on the distribution of the logits themselves.\n",
    "* sample from the probability distribution to generate the next character\n",
    "* append generated character to context and \"shift\" the context window\n",
    "* repeat\n",
    "\n",
    "See that `self(idx)` calls the `forward` function of the model. `forward` is adapted accordingly above to also take a call with just `idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:20.850003Z",
     "iopub.status.busy": "2024-06-12T15:53:20.849721Z",
     "iopub.status.idle": "2024-06-12T15:53:20.903376Z",
     "shell.execute_reply": "2024-06-12T15:53:20.902454Z",
     "shell.execute_reply.started": "2024-06-12T15:53:20.849962Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pYCXxfRkRZd\n",
      "wc'wfNfT;OLlTEeC K\n",
      "jxqPToTb?bXAUG:C-SGJO-33SM:C?YI3a\n",
      "hs:LVXJFhXeNuwqhObxZ.tSVrddXlaSZaNe\n"
     ]
    }
   ],
   "source": [
    "# initial context is just a 0 (new line character) with shape 1x1 (1 character, 1 batch)\n",
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "\n",
    "# generate 100 new tokens\n",
    "res = bigramLM.generate(idx, max_new_tokens=100)\n",
    "\n",
    "# since generate returns a batch of sequences, we just take the first one\n",
    "res0 = res[0]\n",
    "\n",
    "# decode the sequence of indices into characters\n",
    "print(decode(res0.tolist()))\n",
    "# print(decode(bigramLM.generate(\n",
    "#     torch.zeros((1, 1), dtype=torch.long), \n",
    "#     max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"105\"></a>\n",
    "## 1.5. Training Bigram LM\n",
    "-----------\n",
    "Let's train our model. Let's setup our optimization routine. We will use the AdamW optimizer.\n",
    "\n",
    "* **SGD** (Stochastic Gradient Descent): A fundamental optimization algorithm used in machine learning and deep learning. It updates model parameters by computing gradients using randomly selected small batches of data, making it \"stochastic.\" It's widely used for training neural networks and other machine learning models.\n",
    "\n",
    "* **Adam** (Adaptive Moment Estimation): A popular optimization algorithm that improves convergence and training speed compared to traditional SGD. It maintains moving averages of gradients and adapts learning rates for each parameter. It's known for its efficiency in practice.\n",
    "\n",
    "* **AdamW**: A modification of the Adam optimizer designed to handle weight decay (L2 regularization) more effectively. It separates weight decay from the optimization process, making it better at controlling overfitting during the training of deep neural networks. It's a preferred choice for tasks where regularization is important.\n",
    "\n",
    "We set the learning rate to `1e-3` which is a decent setting for small networks. We estimate the loss after every $200$ steps by taking the average to prevent a noisy plot and get a more respresentative, smoother plot. We print out the estimated loss value after every $500$ steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:20.904679Z",
     "iopub.status.busy": "2024-06-12T15:53:20.904403Z",
     "iopub.status.idle": "2024-06-12T15:53:22.079391Z",
     "shell.execute_reply": "2024-06-12T15:53:22.078608Z",
     "shell.execute_reply.started": "2024-06-12T15:53:20.904653Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(bigramLM.parameters(), lr=1e-3)  # typical bigger sized NNs: Lr=3e-4\n",
    "\n",
    "# batch_size = 32\n",
    "# for steps in range(10000): # increase number of steps for good results...\n",
    "\n",
    "#     # sample a batch of data\n",
    "#     xb, yb = get_batch('train')\n",
    "\n",
    "#     # evaluate the loss\n",
    "#     logits, loss = bigramLM(xb, yb)              # forward pass\n",
    "#     optimizer.zero_grad(set_to_none=True)        # clear accumulated gradients\n",
    "#     loss.backward()                              # backward pass (backprop: to get gradients)\n",
    "#     optimizer.step()                             # update parameters\n",
    "\n",
    "# print(loss.item())\n",
    "# print('\\n')\n",
    "# print(decode(bigramLM.generate(\n",
    "#     idx = torch.zeros((1, 1), dtype=torch.long),\n",
    "#     max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:31.361806Z",
     "iopub.status.busy": "2024-06-12T15:53:31.361514Z",
     "iopub.status.idle": "2024-06-12T15:53:31.942706Z",
     "shell.execute_reply": "2024-06-12T15:53:31.941804Z",
     "shell.execute_reply.started": "2024-06-12T15:53:31.361780Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.7303, val loss 4.7214\n",
      "step 500: train loss 4.3569, val loss 4.3665\n",
      "step 1000: train loss 4.0330, val loss 4.0354\n",
      "step 1500: train loss 3.7740, val loss 3.7857\n",
      "step 2000: train loss 3.5565, val loss 3.5578\n",
      "step 2500: train loss 3.3645, val loss 3.3706\n",
      "step 3000: train loss 3.2245, val loss 3.2159\n",
      "step 3500: train loss 3.0658, val loss 3.0859\n",
      "step 4000: train loss 2.9548, val loss 2.9686\n",
      "step 4500: train loss 2.8989, val loss 2.8907\n",
      "step 5000: train loss 2.8049, val loss 2.8030\n",
      "step 5500: train loss 2.7427, val loss 2.7692\n",
      "step 6000: train loss 2.7283, val loss 2.7427\n",
      "step 6500: train loss 2.6716, val loss 2.6751\n",
      "step 7000: train loss 2.6455, val loss 2.6618\n",
      "step 7500: train loss 2.6209, val loss 2.6329\n",
      "step 8000: train loss 2.6162, val loss 2.6158\n",
      "step 8500: train loss 2.5656, val loss 2.5624\n",
      "step 9000: train loss 2.5820, val loss 2.5590\n",
      "step 9500: train loss 2.5498, val loss 2.5883\n",
      "step 10000: train loss 2.5426, val loss 2.5403\n",
      "step 10500: train loss 2.5438, val loss 2.4980\n",
      "step 11000: train loss 2.5047, val loss 2.5238\n",
      "step 11500: train loss 2.5045, val loss 2.5418\n",
      "step 12000: train loss 2.5155, val loss 2.5257\n",
      "step 12500: train loss 2.5158, val loss 2.5286\n",
      "step 13000: train loss 2.4999, val loss 2.4888\n",
      "step 13500: train loss 2.4943, val loss 2.5138\n",
      "step 14000: train loss 2.4903, val loss 2.5097\n",
      "step 14500: train loss 2.4809, val loss 2.5022\n",
      "step 15000: train loss 2.4619, val loss 2.4888\n",
      "step 15500: train loss 2.4733, val loss 2.4682\n",
      "step 16000: train loss 2.4607, val loss 2.4839\n",
      "step 16500: train loss 2.4981, val loss 2.4929\n",
      "step 17000: train loss 2.4770, val loss 2.4793\n",
      "step 17500: train loss 2.4674, val loss 2.4742\n",
      "step 18000: train loss 2.4720, val loss 2.4776\n",
      "step 18500: train loss 2.4614, val loss 2.4612\n",
      "step 19000: train loss 2.4742, val loss 2.5050\n",
      "step 19500: train loss 2.4493, val loss 2.5013\n",
      "\n",
      "Wawice my.\n",
      "\n",
      "HDEdarom oroup\n",
      "Yowhthetof isth ble mil ndill, ath iree sengmin lat Heriliovets, and Win nghir.\n",
      "Swanousel lind me l.\n",
      "HAule ce hiry:\n",
      "Supr aisspllw y.\n",
      "Hentoul noroopetelaves\n",
      "MPOLI l, d mothakleo Windo whth eisbys wie m dourive we higend t so mower; te\n",
      "\n",
      "AN ad nterupt f s ar igr Whe:\n",
      "\n",
      "Thiny aleronth,\n",
      "Mad\n",
      "RD:\n",
      "\n",
      "WISo myr f-bube!\n",
      "KENoby ak\n",
      "Sadsal thes ghesthidin couk ay aney Iry ts I fr y ce.\n",
      "JMOn pand, bemary.\n",
      "Yof 'sour menm sora anghy t-wenomes twe men.\n",
      "Wand thot sulin s th llety od, wiourc\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "eval_iters = 200      # Number of iterations for loss estimation\n",
    "max_iters = 20000     # Total training iterations\n",
    "eval_interval = 500   # Evaluate model every 500 iterations\n",
    "\n",
    "@torch.no_grad()  # Decorator to disable gradient tracking for efficiency during evaluation\n",
    "def estimate_loss(model):\n",
    "    \"\"\"\n",
    "    Estimates the average loss on train and validation sets\n",
    "    Example output: {'train': 1.234, 'val': 1.345}\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()  # Switch model to evaluation mode (affects dropout, batchnorm etc.)\n",
    "    \n",
    "    # Evaluate on both training and validation splits\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)  # Store losses for averaging\n",
    "        for k in range(eval_iters):\n",
    "            # Get a random batch\n",
    "            # Example X shape: (batch_size=4, block_size=8)\n",
    "            # Example Y shape: (batch_size=4, block_size=8) \n",
    "            X, Y = get_batch(split)\n",
    "            \n",
    "            # Forward pass to get loss\n",
    "            # Example logits shape: (4, 8, vocab_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()  # Store scalar loss value\n",
    "            \n",
    "        out[split] = losses.mean()  # Calculate average loss\n",
    "    model.train()  # Switch back to training mode\n",
    "    return out\n",
    "\n",
    "# Lists to track metrics over time\n",
    "train_losses = []  # Store training losses\n",
    "val_losses = []    # Store validation losses \n",
    "epochs = []        # Store iteration numbers\n",
    "\n",
    "# Main training loop\n",
    "for iter in range(max_iters):  # Example: 0 to 19999\n",
    "\n",
    "    # Evaluate model every eval_interval steps\n",
    "    if iter % eval_interval == 0:  # Example: at steps 0, 500, 1000, ...\n",
    "        losses = estimate_loss(bigramLM)\n",
    "        # Example losses: {'train': 2.1, 'val': 2.3}\n",
    "        \n",
    "        # Store metrics for plotting\n",
    "        train_losses.append(losses['train'].item())\n",
    "        val_losses.append(losses['val'].item())\n",
    "        epochs.append(iter)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Training step:\n",
    "    # 1. Get a random batch\n",
    "    xb, yb = get_batch('train')  # Example shapes: (4, 8) each\n",
    "    \n",
    "    # 2. Forward pass\n",
    "    logits, loss = bigramLM(xb, yb)  # Example loss: tensor(2.1)\n",
    "    \n",
    "    # 3. Backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)  # Clear previous gradients\n",
    "    loss.backward()                        # Compute gradients\n",
    "    optimizer.step()                       # Update model parameters\n",
    "\n",
    "# After training, generate text:\n",
    "# Start with a single token (newline character) as context\n",
    "context = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "# Generate 500 new tokens and decode them to text\n",
    "generated_text = decode(bigramLM.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a figure and axes\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the training and validation losses\n",
    "ax.plot(epochs, train_losses, label='Training Loss')\n",
    "ax.plot(epochs, val_losses, label='Validation Loss')\n",
    "\n",
    "# Set the x-axis and y-axis labels\n",
    "ax.set_xlabel('Iterations', fontsize=14)\n",
    "ax.set_ylabel('Loss', fontsize=14)\n",
    "\n",
    "# Set the x-axis ticks and labels\n",
    "ax.set_xticks(epochs)\n",
    "ax.set_xticklabels(epochs, rotation=67.5, fontsize=12)\n",
    "\n",
    "# Set the y-axis tick format\n",
    "ax.yaxis.set_major_formatter(plt.FormatStrFormatter('%.1f'))\n",
    "\n",
    "# Set the title\n",
    "ax.set_title('Training and Validation Losses', fontsize=16)\n",
    "\n",
    "# Add a legend\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(linestyle='--', alpha=0.5)\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase the number of epochs, our generation begins to resemble Shakespeare format but the text is stil nonsensical.\n",
    "\n",
    "**What are the drawbacks of this model?**\n",
    "\n",
    "Even though it can generate characters based on previous characters, it only takes into account the previous character as the context. It cannot understand the context of grammar rules that emerge over words and sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "<br><br><a id=\"2\"></a>\n",
    "# 2. Self-Attention\n",
    "-----------\n",
    "<a id='c0'></a>\n",
    "**Attention** is a communication mechanism that allows models to focus on different parts of the input data when making predictions. This concept is especially important in sequence-based NLP tasks such as machine translation and text summarization, and image processing tasks such as image captioning. It helps models pick out the important bits from a lot of information and focus on them to make smarter decisions. It overcomes the long-range dependency limitations of RNNs & LSTMs by allowing the model to weigh the importance of different elements in the sequence. Instead of processing each element sequentially, attention enables the model to look at all elements simultaneously and decide which ones are more relevant to the current task.\n",
    "\n",
    "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the **query, keys, values**, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
    "\n",
    "Overall, attention is good for capturing long-range dependencies, parallel processing, and making model decisions more interpretable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"201\"></a>\n",
    "## 2.1. V1: Averaging Past Context with `For` Loops - Weakest Form of Aggregation\n",
    "-----------\n",
    "We want our tokens to talk to each other. Tokens must talk only with the previous tokens. Since we are predicting the next token, we need to consider the previous tokens only (5th token communicates with 1st, 2nd, 3rd & 4th tokens)\n",
    "\n",
    "The easiset way to make them communicate is by averaging the previous tokens embeddings. This is a weak form of interaction, it is extremely lossy since we are losing the spatial information of the token arrangements and positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:31.944165Z",
     "iopub.status.busy": "2024-06-12T15:53:31.943868Z",
     "iopub.status.idle": "2024-06-12T15:53:31.957232Z",
     "shell.execute_reply": "2024-06-12T15:53:31.956349Z",
     "shell.execute_reply.started": "2024-06-12T15:53:31.944141Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2143, -0.5001],\n",
       "         [ 0.4922,  1.2006],\n",
       "         [-0.8338, -0.8490],\n",
       "         [-0.7448,  1.7622],\n",
       "         [-0.7925, -1.1531],\n",
       "         [ 1.6754,  0.2774],\n",
       "         [ 0.1732, -0.1147],\n",
       "         [-1.1392,  0.3428]],\n",
       "\n",
       "        [[-0.4025, -1.0747],\n",
       "         [ 0.7761,  1.5219],\n",
       "         [ 0.5268,  0.2607],\n",
       "         [ 1.5801,  1.0655],\n",
       "         [-1.4747, -0.4344],\n",
       "         [ 0.2294,  0.5581],\n",
       "         [ 1.2166, -0.8566],\n",
       "         [ 0.9931, -0.7224]],\n",
       "\n",
       "        [[-0.0047,  0.2919],\n",
       "         [ 0.9026, -0.2179],\n",
       "         [ 1.8337, -0.9827],\n",
       "         [ 0.2086, -0.3206],\n",
       "         [ 0.3625, -1.8386],\n",
       "         [-0.5833, -0.5692],\n",
       "         [-1.3405, -1.3709],\n",
       "         [ 1.0179,  0.6573]],\n",
       "\n",
       "        [[ 0.2693, -2.4841],\n",
       "         [ 1.3780,  1.2509],\n",
       "         [ 0.4769,  0.0726],\n",
       "         [ 0.2418, -0.4994],\n",
       "         [ 0.6281, -0.3364],\n",
       "         [-0.2053,  1.3117],\n",
       "         [-1.3234, -2.5067],\n",
       "         [-1.4071, -0.0138]]], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)  # For reproducibility\n",
    "B, T, C = 4, 8, 2  # Example dimensions: batch_size=4, block_size=8, vocab_size=2\n",
    "x = torch.randn(B, T, C, device=device)  # Random input tensor\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [0]:\n",
      " tensor([[-0.2143, -0.5001],\n",
      "        [ 0.4922,  1.2006],\n",
      "        [-0.8338, -0.8490],\n",
      "        [-0.7448,  1.7622],\n",
      "        [-0.7925, -1.1531],\n",
      "        [ 1.6754,  0.2774],\n",
      "        [ 0.1732, -0.1147],\n",
      "        [-1.1392,  0.3428]], device='cuda:0') \n",
      "\n",
      "Running Averages:\n",
      " tensor([[-0.2143, -0.5001],\n",
      "        [ 0.1390,  0.3503],\n",
      "        [-0.1853, -0.0495],\n",
      "        [-0.3252,  0.4034],\n",
      "        [-0.4186,  0.0921],\n",
      "        [-0.0696,  0.1230],\n",
      "        [-0.0349,  0.0891],\n",
      "        [-0.1730,  0.1208]])\n"
     ]
    }
   ],
   "source": [
    "# We want x[b, t] = mean_{i <= t} x[b, i]\n",
    "xbow = torch.zeros((B, T, C))               # Create tensor of zeros of shape (B, T, C) (bag of words representation of the input)\n",
    "for b in range(B):                          # For all batches\n",
    "    for t in range(T):                      # For all tokens in the batch\n",
    "        xprev = x[b, :t+1]                  # Get all tokens up to and including the current token (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)   # Calculate the mean of the tokens up to and including the current token\n",
    "\n",
    "print('Batch [0]:\\n', x[0], \"\\n\")     # First batch of 8 tokens, each of size 2\n",
    "print('Running Averages:\\n', xbow[0]) # Running averages of the first batch of 8 tokens, each of size 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each column, we have vertically averaged at each step from the first step until that current step. We can make this much more efficient using matrix multiplication and removing the `for` loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='202'></a>\n",
    "## 2.2. Trick: Matrix Multiplication as Weighted Aggregation\n",
    "-----\n",
    "For each column, we have vertically averaged at each step from the first step until that current step by using `torch.tril` and matrix multiplication `a @ b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:31.958660Z",
     "iopub.status.busy": "2024-06-12T15:53:31.958388Z",
     "iopub.status.idle": "2024-06-12T15:53:31.980263Z",
     "shell.execute_reply": "2024-06-12T15:53:31.979395Z",
     "shell.execute_reply.started": "2024-06-12T15:53:31.958636Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))            # Lower triangular matrix of ones\n",
    "a = a / a.sum(dim=1, keepdim=True)          # Normalize the matrix by dividing along each row\n",
    "b = torch.randint(0, 10, (3, 2)).float()    # 3x2 matrix of random integers between 0 and 9\n",
    "c = a @ b                                   # Matrix multiplication of a and b\n",
    "\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='203'></a>\n",
    "## 2.3. V2: Matrix Multiplication\n",
    "-----\n",
    "Now we can use matrix multiplication and the mathematics trick to implement a weighted aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:31.981855Z",
     "iopub.status.busy": "2024-06-12T15:53:31.981509Z",
     "iopub.status.idle": "2024-06-12T15:53:31.991621Z",
     "shell.execute_reply": "2024-06-12T15:53:31.990610Z",
     "shell.execute_reply.started": "2024-06-12T15:53:31.981826Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "wei = wei.to(device)\n",
    "\n",
    "wei = wei.view(1, T, T)  # (1, T, T)\n",
    "\n",
    "x = x.to(device)         # make sure x is on same device\n",
    "xbow = xbow.to(device)   # same for xbow if needed\n",
    "\n",
    "xbow2 = wei @ x          # (1, T, T) @ (B, T, C) --> (B, T, C)\n",
    "\n",
    "torch.allclose(xbow, xbow2, atol=1e-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='204'></a>\n",
    "## 2.4. V3: Softmax\n",
    "-----\n",
    "Now let's use softmax to perform weighted aggregation of the preceding tokens. The Softmax approach is preferred due to its accurate representation of the context. By setting upper triangular indices to `-inf`, we have defined that, the future cannot communicate with the past. We can use a lower triangular matrix to perform weighted aggregation of the past elements.\n",
    "\n",
    "<a id='c01'></a>\n",
    "**Masking**\n",
    "\n",
    "In a decoder, we implement masked attention in which a token cannot access the weights of future tokens. It can be connected with itself or with past tokens. We cannot aggregate any information from the future tokens.\n",
    "To implement masking, we use PyTorch's function `tril` which only returns the <u>lower diagonal of a tensor while the upper diagonal is filled with zeros.</u> With the help of this tensor, <u>we fill the positions that were filled with $0$ with $-\\infty$. When we apply softmax across rows to normalize the values, the $-\\infty$ is stored as $0$, preserving the masking.</u> Then weights are multiplied by values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:31.993307Z",
     "iopub.status.busy": "2024-06-12T15:53:31.993056Z",
     "iopub.status.idle": "2024-06-12T15:53:32.002128Z",
     "shell.execute_reply": "2024-06-12T15:53:32.001147Z",
     "shell.execute_reply.started": "2024-06-12T15:53:31.993285Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:32.005157Z",
     "iopub.status.busy": "2024-06-12T15:53:32.003156Z",
     "iopub.status.idle": "2024-06-12T15:53:32.013461Z",
     "shell.execute_reply": "2024-06-12T15:53:32.012598Z",
     "shell.execute_reply.started": "2024-06-12T15:53:32.005130Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:32.015077Z",
     "iopub.status.busy": "2024-06-12T15:53:32.014538Z",
     "iopub.status.idle": "2024-06-12T15:53:32.023153Z",
     "shell.execute_reply": "2024-06-12T15:53:32.022206Z",
     "shell.execute_reply.started": "2024-06-12T15:53:32.015048Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.zeros((T,T))                            \n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))       \n",
    "wei = F.softmax(wei, dim=-1)                       \n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:32.024932Z",
     "iopub.status.busy": "2024-06-12T15:53:32.024388Z",
     "iopub.status.idle": "2024-06-12T15:53:32.034125Z",
     "shell.execute_reply": "2024-06-12T15:53:32.033227Z",
     "shell.execute_reply.started": "2024-06-12T15:53:32.024899Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei = wei.to(device)  \n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3, atol=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='205'></a>\n",
    "## 2.5. Bigram LM Code Tweaks: Robust Token Embedding Dimension\n",
    "-----\n",
    "Now, we can make a few changes and cleanups to our script. Let's introduce a new `n_embd` variable to the embedding table. Our embedding lengths might differ from the `vocab_size`. However, now the embedding table won't give us `logits` directly and generate tokens. We must introduce an **intermediary linear layer** to go from `embeddings` to the `logits`.\n",
    "<br><br>\n",
    "\n",
    "<a id='206'></a>\n",
    "## 2.6. Bigram LM Code Tweaks: Positional Encoding\n",
    "-----\n",
    "Until now, we only defined identity embeddings. Embeddings do not change with the position. However, positional information is also important. Thus, we will define a position embedding table. However, in a bigram model, this means little, due to the small context size.\n",
    "\n",
    "\n",
    "\n",
    "In Sections [$2.5$](#205) & $2.6$, we've implemented multiple changes to our `BigramLanguageModel(nn.Module)` class.\n",
    "\n",
    "\n",
    "- `__init__` now takes no arguments (`vocab_size` is now a global variable)\n",
    "- Intermediate phase added before logit embedding:\n",
    "    - Changed embedding layer dimensions from `(vocab_size, vocab_size)` to `(vocab_size, n_embd)`\n",
    "        - `n_embd` is the now arbitrary size of the vector into which the token is embedded\n",
    "    - Added a linear layer `self.lm_head` for the logit embedding of dimensions `(n_embd, vocab_size)` so that we can do the weighted aggregation of the past tokens with size `(n_embd, n_embd)`\n",
    "- Added positional embeddings\n",
    "    - positional embedding layer of dimensions `(block_size, n_embd)`, embeds the token position in the sequence\n",
    "    - `pos_emb` is the positional embedding of the current token, which is added (+) to the embedding of the current token information `tok_emb`\n",
    "    - this sum is then passed to the linear layer to get the `logits`\n",
    "\n",
    "So at this point linear layer input holds not just the token identities, but the positions at which these tokens occur.This is currently not that useful because of course, we just have a simple bigram model.\n",
    "So it doesn't matter if you're in the $5\\text{th}$ position, the $2\\text{nd}$ position or wherever, it's all **translation invariant** at this stage. So this information currently wouldn't help. But as we work on the self-attention block, we'll see that this starts to matter.\n",
    "\n",
    "In summary, these minor code changes do *not yet* affect anything, because we are still missing the self-attention block. However, the model works still.\n",
    "\n",
    "<a id='c1'></a>\n",
    "<u>Aside:</u> ***translation invariance*** means that the system can recognize an object or pattern in an image regardless of its position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='207'></a>\n",
    "## 2.7. V4: **SELF-ATTENTION**\n",
    "-----\n",
    "Now let's use self-attention to perform weighted aggregation of the preceding/past tokens. We'll build a self-attention unit for a single head to understand how attention works in the simplest case.\n",
    "\n",
    "<a id='c101'></a><u>Self-attention</u> is the part of the model where **tokens interact with each other.** Each token \"looks\" at other tokens in the sentence with an attention mechanism, gathers context, and updates the previous representation of \"self\". Let's look at the illustration below (Note that in practice, this happens in parallel.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Query, Key, and Value in Self-Attention\n",
    "\n",
    "Formally, this intuition is implemented with a **query-key-value** attention. Each input token in self-attention receives three representations corresponding to the roles it can play:\n",
    "\n",
    "*   **Query** - asking for information;\n",
    "*   **Key** - saying that it has some information;\n",
    "*   **Value** - giving the information.\n",
    "\n",
    "The **query** is used when a token looks at others - it's seeking the information to understand itself better. The **key** is responding to a query's request: it is used to compute attention weights. The **value** is used to compute attention output: it gives information to the tokens which \"say\" they need it (i.e. assigned large weights to this token).\n",
    "\n",
    "<a id=\"a1\"></a>\n",
    "<center>\n",
    "<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/transformer/qkv_explained-min.png\" alt=\"Query, Key, Value\" width=\"750\"/>\n",
    "</center>\n",
    "\n",
    "**Figure 1: Query, Key, Value in Self-Attention Explained.** ([Source](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#self_attention))<br><br>\n",
    "\n",
    "\n",
    "We call our particular attention **Scaled Dot-Product Attention.** The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.\n",
    "\n",
    "<a id=\"a2\"></a>\n",
    "<center>\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/35184258-10f5-4cd0-8de3-bd9bc8f88dc3.png\" alt=\"Scaled Dot-Product Attention Formula\" width=\"350\"/>\n",
    "</center>\n",
    "\n",
    "**Figure 2: Scaled Dot-Product Attention.** ([Source](https://paperswithcode.com/method/scaled))<br><br>\n",
    "\n",
    "<a id=\"b1\"></a>\n",
    "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$. The keys and values are also packed together into matrices $K$ and $V$. The softmax outputs are known as the **attention weights or affinity matrix.** We compute the matrix of attention outputs as:\n",
    "\n",
    "$$ {\\text{Attention}}(Q, K, V) = \\text{softmax}(\\frac{QK^{T}}{\\sqrt{d_k}})V $$\n",
    "\n",
    "\n",
    "\n",
    "According to [The Annotated Transformer article](https://nlp.seas.harvard.edu/annotated-transformer/#background), the two most commonly used attention functions are **additive,** and **dot-product (multiplicative)** attention. In our case, we employed a scaled version (scaling factor of $\\frac{1}{d_k}$) of dot-product attention. Both methods are similar in theretical complexity, however dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
    "\n",
    "While for small values of ${d_k}$ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of ${d_k}$. We suspect that for large values of ${d_k}$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. Let's illustrate why the dot products get large. If we assume that $q$ and $k$ are ${d_k}$-dimensional vectors whose components are independent random variables with mean $0$ and variance $1$. Then their dot product, $q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i$, has mean $0$ and variance $d_k$. To counteract this effect for large values of ${d_k}$ (since we would prefer these values to have variance $1$), we scale the dot-product by $\\frac{1}{\\sqrt{d_k}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a self-attention block for a single, individual head for simplicity. To do that, we'll continue with our 'running average' trick from before. I already hinted at it: We don't want the probabilities of `wei` to be row-wise uniform.<br>\n",
    "Different tokens should find other tokens more or less important/interesting, and this should be learned by the model.\n",
    "\n",
    "> Gather information from the past, but do so in a data-dependent way and improve based on training.\n",
    "\n",
    "With Self-Attention, every single token in the batch emits two vectors: `query` and `key`:\n",
    "- The **`query` vector** is the token-specific \"What am I looking for?\" information\n",
    "- The **`key` vector** is the token-specific \"What do I contain?\" information\n",
    "\n",
    "To establish **affinity** (high interrelation and high influence on the sampling decision) between tokens of the batch, we calculate the dot product of the `query` and `key` vectors of each token with each other token in the batch.<br>\n",
    "**This is the `affinity` matrix or in our case `wei`.**\n",
    "\n",
    "If during dot product calculation the `key` and the `query` turn out to be well aligned or similar, the affinity will be high. If they are not, the affinity will be low.\n",
    "\n",
    "Let's build the individual head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:32.045370Z",
     "iopub.status.busy": "2024-06-12T15:53:32.045073Z",
     "iopub.status.idle": "2024-06-12T15:53:32.063228Z",
     "shell.execute_reply": "2024-06-12T15:53:32.062395Z",
     "shell.execute_reply.started": "2024-06-12T15:53:32.045342Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)  # For reproducibility\n",
    "\n",
    "B, T, C = 4, 8, 2  # Example dimensions: batch_size=4, block_size=8, vocab_size=2\n",
    "x = torch.randn(B, T, C, device=device)  # (4, 8, 2) and all values are random\n",
    "\n",
    "head_size = 16  # Number of attention heads\n",
    "key = nn.Linear(C, head_size, bias=False).to(device)  # Linear layer for keys\n",
    "query = nn.Linear(C, head_size, bias=False).to(device)  # Linear layer for queries\n",
    "value = nn.Linear(C, head_size, bias=False).to(device)  # Linear layer for values\n",
    "\n",
    "k = key(x)  # (B, T, head_size)\n",
    "q = query(x)  # (B, T, head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
    "tril = torch.tril(torch.ones(T,T)).to(device)  # Lower triangular matrix\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # Mask upper triangular part\n",
    "\n",
    "wei = F.softmax(wei, dim=2)  # Apply softmax to get attention weights\n",
    "\n",
    "v = value(x)  # (B, T, head_size)\n",
    "out = wei @ v  # (B, T, T) @ (B, T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:32.064793Z",
     "iopub.status.busy": "2024-06-12T15:53:32.064300Z",
     "iopub.status.idle": "2024-06-12T15:53:32.071874Z",
     "shell.execute_reply": "2024-06-12T15:53:32.070884Z",
     "shell.execute_reply.started": "2024-06-12T15:53:32.064762Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape  # Output shape should be (B, T, head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0354,  0.3436,  0.0126,  0.1749, -0.2145,  0.2312,  0.1707,  0.0484,\n",
       "         -0.1768,  0.1090, -0.1404,  0.2775,  0.1142, -0.1171, -0.0497, -0.2314],\n",
       "        [-0.0533, -0.4246, -0.0197, -0.2116,  0.2751, -0.2880, -0.2199, -0.0595,\n",
       "          0.2140, -0.1371,  0.1725, -0.3514, -0.1344,  0.1572,  0.0620,  0.2854],\n",
       "        [-0.1384,  0.5931, -0.0650,  0.3966, -0.1582,  0.3530,  0.1050,  0.0900,\n",
       "         -0.3987,  0.1379, -0.2638,  0.3011,  0.3377,  0.0572, -0.0736, -0.4111],\n",
       "        [-0.1071,  0.0876, -0.0473,  0.0997,  0.0687,  0.0321, -0.0668,  0.0161,\n",
       "         -0.0996, -0.0015, -0.0483, -0.0328,  0.1109,  0.1212, -0.0056, -0.0658],\n",
       "        [-0.4884,  0.3536, -0.2154,  0.4291,  0.3367,  0.1166, -0.3228,  0.0668,\n",
       "         -0.4283, -0.0200, -0.2009, -0.1823,  0.4872,  0.5618, -0.0192, -0.2690],\n",
       "        [ 0.6959, -1.0941,  0.3117, -0.9408, -0.1761, -0.5493,  0.2247, -0.1802,\n",
       "          0.9424, -0.1434,  0.5340, -0.1627, -0.9332, -0.6784,  0.1091,  0.7843],\n",
       "        [-0.0525, -0.0597, -0.0223, -0.0084,  0.0865, -0.0509, -0.0736, -0.0069,\n",
       "          0.0090, -0.0306,  0.0194, -0.0895,  0.0128,  0.0806,  0.0114,  0.0375],\n",
       "        [-0.2518,  0.4538, -0.1132,  0.3727,  0.0339,  0.2363, -0.0582,  0.0736,\n",
       "         -0.3736,  0.0687, -0.2175,  0.1003,  0.3611,  0.2335, -0.0475, -0.3231]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='208'></a>\n",
    "## 2.8. $6$ Key Notes on Attention\n",
    "-----\n",
    "Notes:\n",
    "- Attention is a **communication mechanism**. It can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "  - It contains a directed graph and each node aggregates information via a weighted sum of the nodes pointed towards it. This is done in a data-dependent manner.\n",
    "- Attention **does not have a notion of space.** It simply acts over a set of vectors. This is <u>why we need to positionally encode tokens.</u> \n",
    "  - Thus, we have to encode the node positionally, which was done through a positional encoding. In contrast, convolutional mechanisms have a concrete layout of information in the space. There is no notion of space. \n",
    "-  There is **no communication across batch dimension.**\n",
    "  - <u>Each example</u> across batch dimension is of course **processed completely independently and never \"talk\" to each other.**\n",
    "- In an **\"encoder\"** attention block, it justs delete the single line that does masking with `tril`, allowing all tokens to communicate. The masking block here is called a **\"decoder\"** attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling. \n",
    "  - In some cases, we must allow every node to talk to each other. (Ex: Sentiment analysis, the process of analyzing digital text to determine the emotional tone of a message). Thus, we will use an **encoder block** of self-attention. It simply deletes the lower triangular mask step and lets all the nodes communicate. In **decoder blocks**, future nodes will never communicate with the current node as masking step is active.\n",
    "- <a id='c102'></a>**\"self-attention\"** just means that the keys and values are produced from the same source as queries. In **\"cross-attention,\"** the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "  - Our implementation is called **self-attention** since both keys, queries, and values are coming from the same source, thus the nodes are self-attending. In **cross-attention**, keys and values might be generated from a different set of nodes.\n",
    "- **\"Scaled\" self-attention** additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n",
    "  - In the **Attention is all you need** paper, the attention weights are divided by $\\sqrt{head\\_size}$. This will allow `wei` to have a unit variance. This is important since wei is fed to a softmax layer. If `wei` have large positive or negative numbers during the initialization, the softmax will be converged to one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:32.073443Z",
     "iopub.status.busy": "2024-06-12T15:53:32.073172Z",
     "iopub.status.idle": "2024-06-12T15:53:32.083063Z",
     "shell.execute_reply": "2024-06-12T15:53:32.082197Z",
     "shell.execute_reply.started": "2024-06-12T15:53:32.073411Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unscaled Dot-Product Self-Attention\n",
      "1.0657\n",
      "1.0719\n",
      "16.3892\n",
      "\n",
      "Scaled Dot-Product Self-Attention\n",
      "1.0657\n",
      "1.0719\n",
      "1.0243\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "\n",
    "print(\"Unscaled Dot-Product Self-Attention\")\n",
    "print(k.var().item().__format__('.4f'))              \n",
    "print(q.var().item().__format__('.4f'))  \n",
    "print(wei.var().item().__format__('.4f')) # The variance is larger\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) * (head_size ** -0.5) # This is the scaled attention, avoiding exploding variance which would sharpen the softmax distributions (and thus make the attention more deterministic)\n",
    "\n",
    "print(\"\\nScaled Dot-Product Self-Attention\")\n",
    "print(k.var().item().__format__('.4f'))   # The variance is like before\n",
    "print(q.var().item().__format__('.4f'))   # The variance is like before\n",
    "print(wei.var().item().__format__('.4f')) # The variance is now much smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:32.084523Z",
     "iopub.status.busy": "2024-06-12T15:53:32.084222Z",
     "iopub.status.idle": "2024-06-12T15:53:32.091809Z",
     "shell.execute_reply": "2024-06-12T15:53:32.090736Z",
     "shell.execute_reply.started": "2024-06-12T15:53:32.084492Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:32.094562Z",
     "iopub.status.busy": "2024-06-12T15:53:32.093663Z",
     "iopub.status.idle": "2024-06-12T15:53:32.100727Z",
     "shell.execute_reply": "2024-06-12T15:53:32.099937Z",
     "shell.execute_reply.started": "2024-06-12T15:53:32.094535Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "<br><br><a id=\"3\"></a>\n",
    "# 3. Transformers\n",
    "-----------\n",
    "\n",
    "The transformer model has these main parts:\n",
    "\n",
    "1.  Self-attention\n",
    "2.  Multi-head attention\n",
    "3.  Feed Forward Neural Network\n",
    "4.  Residual Connections\n",
    "5.  Layer Normalization\n",
    "\n",
    "<a id=\"a3\"></a>\n",
    "![attention is all you need architecture](https://lena-voita.github.io/resources/lectures/seq2seq/transformer/model-min.png)\n",
    "\n",
    "**Figure 3: Attention is All You Need - Transformer Model Architecture.** ([Source](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#transformer_model_architecture))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='301'></a>\n",
    "## 3.1. Single Self-Attention\n",
    "-----\n",
    "Now let's build a single self-attention block. During initialization, just like we discussed in self-attention, we initialize the key, query, and value vector to have the dimensions of `n_embd` embeddings and `head_size`\n",
    "\n",
    "So the input vectors would be inputted with `B, T, C` dimensions:<br> \n",
    ">_[Batch, Time (block size) and Channels (embedding dimensions)]_<br> \n",
    "\n",
    "and their key, query, and value vectors would be taking in the `C `dimension (`n_embd`) and multiplying with `head_size.`\n",
    "\n",
    "In `self.register_buffer(torch.tril(torch.ones(block_size, block_size)))` we use `register_buffer` to register a tensor as part of a module's state, but not as a parameter to be optimized during training. This is typically used for tensors that you want to keep track of but don't want to optimize. The tril vector will be used for masking.\n",
    "\n",
    "The `nn.Dropout` randomly drops some neurons in a layer which means it initializes them as zero during every forward and backward pass to prevent overfitting of data and building an ensemble of neural networks.\n",
    "\n",
    "In the `forward` function, we calculate attention similar to how we did in the previous section. The only addition is that we scale the weights by dividing them by the square root of the dimension of the key vector, to implement **scaled dot product attention**. Scaling it with the square root ensures that it has a unit Gaussian distribution which makes training easier.\n",
    "\n",
    "The rest should be recognizable from the Section [2.7](#207).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4           # how many independent sequences will we process in parallel?\n",
    "block_size = 32          # what is the maximum context length for predictions?\n",
    "max_iters = 1000      \n",
    "eval_interval = 50   \n",
    "learning_rate = 1e-3  #1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 20\n",
    "n_embd = 24  \n",
    "            # (n_head * batch_size)\n",
    "n_head = 6                # (n_embd/batch_size)\n",
    "n_layer = 2 \n",
    "dropout = 0.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout layer for regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, C = x.shape  # B=batch_size, T=block_size, C=n_embd\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * (head_size ** -0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v  # (B, T, T) @ (B,\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='302'></a>\n",
    "## 3.2. Multi-Head Attention (MHA)\n",
    "-----\n",
    "**Multi-head attention:**<br><a id=\"b2\"></a>\n",
    "This is applying multiple attentions in parallel\n",
    "and concatenating the results.It's basically just multiple attentions in parallel. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
    "\n",
    "  > $$\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, \\text{...}, \\text{head}_h )W^O$$\n",
    "  $$\\text{where }\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "  \n",
    "Where the projections are parameter matrices:\n",
    "> $$W_i^Q \\in \\mathbb{R}^{ d_{model} \\times\\ {d_K} }$$\n",
    " $$W_i^K \\in \\mathbb{R}^{ d_{model} \\times\\ {d_K} }$$\n",
    " $$W_i^V \\in \\mathbb{R}^{ d_{model} \\times\\ {d_V} }$$\n",
    " $$W_i^O \\in \\mathbb{R}^{ hd_v \\times\\ d_{model}}$$\n",
    " $$ \\text{with } {d_k} = {d_v} = d_{model}/h $$\n",
    "\n",
    "<a id=\"a4\"></a>\n",
    "<center>\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png\" alt=\"multi-head attention\" width=\"350\"/>\n",
    "</center>\n",
    "\n",
    "**Figure 4: Multi-Head Attention.** ([Source](https://paperswithcode.com/method/multi-head-attention))<br><br>\n",
    "\n",
    "\n",
    "Let's implement a multi-head attention block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:32.112928Z",
     "iopub.status.busy": "2024-06-12T15:53:32.112656Z",
     "iopub.status.idle": "2024-06-12T15:53:32.124029Z",
     "shell.execute_reply": "2024-06-12T15:53:32.123223Z",
     "shell.execute_reply.started": "2024-06-12T15:53:32.112907Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MULTI-HEAD ATTENTION FLOW ===\n",
      "Input shape: torch.Size([4, 8, 24])\n",
      "\n",
      "1. INDIVIDUAL HEAD OUTPUTS:\n",
      "Head 0 output shape: torch.Size([4, 8, 4])\n",
      "Head 1 output shape: torch.Size([4, 8, 4])\n",
      "Head 2 output shape: torch.Size([4, 8, 4])\n",
      "Head 3 output shape: torch.Size([4, 8, 4])\n",
      "Head 4 output shape: torch.Size([4, 8, 4])\n",
      "Head 5 output shape: torch.Size([4, 8, 4])\n",
      "\n",
      "2. CONCATENATION:\n",
      "Concatenated shape: torch.Size([4, 8, 24])\n",
      "Expected: (batch_size=4, block_size=8, n_head*head_size=24)\n",
      "\n",
      "3. PROJECTION:\n",
      "After projection shape: torch.Size([4, 8, 24])\n",
      "Projects from 24 back to 24\n",
      "\n",
      "4. DROPOUT:\n",
      "Final output shape: torch.Size([4, 8, 24])\n",
      "\n",
      "Complete forward pass output shape: torch.Size([4, 8, 24])\n"
     ]
    }
   ],
   "source": [
    "# Example configuration\n",
    "n_head = 6\n",
    "head_size = 4  # n_embd / n_head = 24 / 6 = 4\n",
    "n_embd = 24\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "# Input tensor\n",
    "x = torch.randn(batch_size, block_size, n_embd)  # (4, 8, 24)\n",
    "\n",
    "print(\"=== MULTI-HEAD ATTENTION FLOW ===\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Create multi-head attention\n",
    "mha = MultiHeadAttention(n_head, head_size)\n",
    "\n",
    "# Step-by-step breakdown:\n",
    "print(\"\\n1. INDIVIDUAL HEAD OUTPUTS:\")\n",
    "head_outputs = []\n",
    "for i, head in enumerate(mha.heads):\n",
    "    head_out = head(x)  # Each head outputs (4, 8, 4)\n",
    "    head_outputs.append(head_out)\n",
    "    print(f\"Head {i} output shape: {head_out.shape}\")\n",
    "\n",
    "print(f\"\\n2. CONCATENATION:\")\n",
    "concatenated = torch.cat(head_outputs, dim=-1)  # (4, 8, 24)\n",
    "print(f\"Concatenated shape: {concatenated.shape}\")\n",
    "print(f\"Expected: (batch_size={batch_size}, block_size={block_size}, n_head*head_size={n_head*head_size})\")\n",
    "\n",
    "print(f\"\\n3. PROJECTION:\")\n",
    "projected = mha.proj(concatenated)  # (4, 8, 24)\n",
    "print(f\"After projection shape: {projected.shape}\")\n",
    "print(f\"Projects from {n_head * head_size} back to {n_embd}\")\n",
    "\n",
    "print(f\"\\n4. DROPOUT:\")\n",
    "final_output = mha.dropout(projected)  # (4, 8, 24)\n",
    "print(f\"Final output shape: {final_output.shape}\")\n",
    "\n",
    "# Complete forward pass\n",
    "full_output = mha(x)\n",
    "print(f\"\\nComplete forward pass output shape: {full_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='303'></a>\n",
    "## 3.3. Feed-Forward Network (FFN)\n",
    "-----\n",
    "**Feed-forward network (FFN):**<br><a id=\"b3\"></a>\n",
    "This is basically two linear transformations with a ReLU activation in between.\n",
    ">$\\text{FFN}(x) =  \\text{max}(0, xW_1 + b_1)W_2 + b_2$\n",
    "\n",
    "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as $2$ convolutions with kernel size $1$. \n",
    "\n",
    "<a id=\"a5\"></a>\n",
    "<center>\n",
    "<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/transformer/ffn-min.png\" alt=\"feed-forward network\" width=\"350\"/>\n",
    "</center>\n",
    "\n",
    "**Figure 5: Feed-Forward Network.** ([Source](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#transformer_model_architecture))<br><br>\n",
    "\n",
    "Next, let's add a feed-forward network, which is basically a simple multilayer perceptron network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:32.126433Z",
     "iopub.status.busy": "2024-06-12T15:53:32.125173Z",
     "iopub.status.idle": "2024-06-12T15:53:32.133023Z",
     "shell.execute_reply": "2024-06-12T15:53:32.132281Z",
     "shell.execute_reply.started": "2024-06-12T15:53:32.126407Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__ (self, n_embd): # n_embed eg: 24 means the input and output dimensions are 24 comes from the embedding size\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # Increase dimensions\n",
    "            nn.ReLU(),                       # Non-linearity\n",
    "            nn.Linear(4 * n_embd, n_embd),  # Project back to original dimensions\n",
    "            nn.Dropout(dropout)              # Dropout for regularization\n",
    "        )\n",
    "\n",
    "\n",
    "# The 4x expansion gives the network more \"thinking space\"\n",
    "# More parameters = more capacity to learn complex patterns\n",
    "\n",
    "\n",
    "\n",
    "# Think of it this way:\n",
    "# 1. Attention: Tokens communicate and gather information\n",
    "# 2. FFN: Each token individually processes what it learned\n",
    "\n",
    "# Without FFN:\n",
    "# Token: \"I gathered info from other tokens, but I can't process it deeply\"\n",
    "\n",
    "# With FFN:\n",
    "# Token: \"I gathered info AND now I can think about it in a richer way\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "We were too fast to calculate the logits. With multi-head-attention, we allowed the nodes to collect all the information they needed but we did not allow them to **\"think.\"** Thus, the feed-forward network layer allows the model tokens to **\"think\"/process** the data individually & independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='304'></a>\n",
    "## 3.4. Residual Connections\n",
    "-----\n",
    "We have already implemented most of all the necessary building blocks for the transformer architecture. Now it is time to collect them together. Now we can define a `block` module, which intersperses all the communications and computations. <u>Communication</u> between the nodes will be done via the **multi-head self-attention** and <u>token-level computations</u> will be done using the **feed-forward network.** This implementation does not significantly improve the performance. The neural network has become deeper. However, deeper neural networks suffer from optimization issues.\n",
    "\n",
    "The first solution for the issue is using residual connections.\n",
    "\n",
    "<a id=\"c2\"></a>\n",
    "**[Residual Connections:](https://paperswithcode.com/method/residual-block)**<br>\n",
    "Residual Connections are a type of <u>skip-connection</u> that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. They are very simple (add a block's input to its output), but at the same time are very useful: they ease the gradient flow through a network and allow stacking of multiple layers. The intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers. Having skip connections allows the network to more easily learn identity-like mappings.\n",
    "<a id=\"b4\"></a>\n",
    ">Formally, denoting the desired underlying mapping as \n",
    ", let the stacked nonlinear layers fit another mapping of $F(x) := H(x) - x = \\text{output} - \\text{input}$<br><br>\n",
    "The original mapping is recast into $F(x) + x$.<br> $F(x)$ acts like a residual, hence the name **\"residual block.\"**\n",
    "\n",
    "According to [an article](https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec) on residual blocks, our residual block is overall trying to learn the true output, _H(x)_. If you look closely at the image above, you will realize that since we have an identity connection coming from _x_, the layers are actually trying to learn the residual, _F(x)_. So to summarize, <u>the layers in a traditional network are learning the true output (_H(x)_), whereas the layers in a residual network are learning the residual (_F(x)_).</u> Hence, the name: **_Residual Block_.**\n",
    "\n",
    "\n",
    "<a id='a6'></a>\n",
    "![residual connection](https://pbs.twimg.com/media/ESnE4IvUYAAopRf?format=jpg&name=large)\n",
    "\n",
    "**Figure 6: Residual Block: (1)-Residual Connection on the Side of the Layer, (2)-Layer on the Side of the Residual Connection.** ([Source](https://x.com/karpathy/status/1236737502200791041))<br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the $2$nd figure above on the right, the computation follows a residual pathway with addition operations connecting the inputs to the targets. The residual pathway  allows intermediate computations to fork off and rejoin through addition. This residual design allows gradients during backpropagation to flow unimpeded from the loss all the way to the inputs via the addition operations, creating a **\"gradient superhighway.\"** The residual blocks are initially initialized to contribute very little, essentially bypassing them at first. However, during training, these residual blocks gradually come online (into play) and start contributing to the computation & improving the optimization. This architecture helps optimize the model effectively by enabling direct and unobstructed gradient flow initially, while allowing the residual blocks to adapt and contribute over time. The combination of the residual pathway and the gradual incorporation of residual blocks dramatically improves the optimization process.\n",
    "\n",
    "Another [article](https://medium.com/@adachoudhry26/deep-dive-into-ai-building-gpt-from-scratch-aff87c804117) states that **residual connection** is when we add identity mapping in addition to the output before passing it to the next layer. This is another way of saying that we add the input to the output before passing it onto the next layer. **This solves the problem of exploding or vanishing gradients seen in feed-forward neural networks** because in these networks the path length for output is proportional to the number of layers. On adding more layers, the gradient explodes because the resultant output is huge compared to the outputs of neurons in the initial layers. This makes the network during backpropagation unstable.\n",
    "\n",
    "Next, let's construct our `block` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:32.134240Z",
     "iopub.status.busy": "2024-06-12T15:53:32.133942Z",
     "iopub.status.idle": "2024-06-12T15:53:32.145119Z",
     "shell.execute_reply": "2024-06-12T15:53:32.144232Z",
     "shell.execute_reply.started": "2024-06-12T15:53:32.134214Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" one block of the transformer: multi-head self-attention followed by feed-forward network \"\"\"\n",
    "\n",
    "    def __init__ (self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd//n_head  \n",
    "        self.sa = MultiHeadAttention(n_head, head_size) \n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # Residual connection 1: x + self_attention(x)\n",
    "        x = x + self.ffwd(self.ln2(x)) # residual connection 2: x + FFN output\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Without residuals:\n",
    "# Input  Layer1  Layer2  Layer3  ...  LayerN  Output\n",
    "# Gradients must flow through ALL layers during backprop\n",
    "# Gradients get smaller and smaller (vanishing gradient problem)\n",
    "\n",
    "# With residuals:\n",
    "# Input  (+)  Output\n",
    "#    Layer1  Layer2  Layer3  ...  LayerN \n",
    "    \n",
    "# Mathematical insight:\n",
    "# output = F(x) + x\n",
    "# Where F(x) is what the layer learns (residual)\n",
    "# If F(x) = 0, then output = x (perfect identity)\n",
    "# Layer only needs to learn the \"residual\" or \"differenc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='305'></a>\n",
    "## 3.5. Layer Normalization (`LayerNorm`)\n",
    "-----\n",
    "<a id='c3'></a>\n",
    "`LayerNorm` helps us to optimize our network even more via normalizing the activations of the neurons withing a layer. It is similar to `batchNorm` which we've encountered previously. Unlike `batchNorm` which normalizes across the batch dimension (columns), `LayerNorm` normalizes across the features in a layer (rows).\n",
    "\n",
    "Unlike batch normalization, Layer Normalization directly estimates the normalization statistics from all of the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks.\n",
    "\n",
    "<a id=\"b5\"></a>\n",
    "We compute the layer normalization statistics over all the hidden units in the same layer as follows:\n",
    "$$\\mu^l = \\frac{1}{H}\\sum_{i=1}^H a_i^l$$\n",
    "$$ $$\n",
    "$$\\sigma^l = \\sqrt{\\frac{1}{H}\\sum_{i=1}^H (a_i^l - \\mu^l)^2}$$\n",
    "\n",
    "<a id='a7'></a>\n",
    "![layer normalization](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-19_at_4.24.42_PM.png)\n",
    "\n",
    "**Figure 7: Layer Normalization.** ([Source](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-19_at_4.24.42_PM.png))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:32.146629Z",
     "iopub.status.busy": "2024-06-12T15:53:32.146356Z",
     "iopub.status.idle": "2024-06-12T15:53:32.164652Z",
     "shell.execute_reply": "2024-06-12T15:53:32.163745Z",
     "shell.execute_reply.started": "2024-06-12T15:53:32.146588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Simple LayerNorm implementation\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))   # Learnable scale\n",
    "        self.beta = nn.Parameter(torch.zeros(dim))   # Learnable shift\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, features)\n",
    "        mean = x.mean(dim=-1, keepdim=True)     # Mean across features\n",
    "        var = x.var(dim=-1, keepdim=True)       # Variance across features\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)  # Normalize\n",
    "        return self.gamma * x_norm + self.beta  # Scale and shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch Normalization down the columns\n",
      "tensor(7.4506e-09) tensor(1.0000)\n",
      "\n",
      "Batch Normalization across the rows\n",
      "tensor(0.0411) tensor(1.0431) \n",
      "\n",
      "tensor([ 0.0468, -0.1209, -0.1358,  0.6035, -0.0515])\n",
      "torch.Size([32, 100])\n"
     ]
    }
   ],
   "source": [
    "# Makemore 3's BatchNorm1d\n",
    "class BatchNorm1d:\n",
    "  \n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps                        # Epsilon set to PyTorch default, you may change it\n",
    "        self.momentum = momentum              # Momemtum set to PyTorch default, you may change it\n",
    "        self.training = True\n",
    "        # Initialize Parameters (trained with backprop)\n",
    "        # (bngain -> gamma, bnbias -> beta)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # Initialize Buffers \n",
    "        # (Trained with a running 'momentum update')\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Forward-Pass\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True) # Batch mean\n",
    "            xvar = x.var(0, keepdim=True)   # Batch variance\n",
    "        else:\n",
    "            xmean = self.running_mean # Using the running mean as basis\n",
    "            xvar = self.running_var   # Using the running variance as basis\n",
    "\n",
    "        # Normalize to unit variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta  # Apply batch gain and bias\n",
    "\n",
    "        # Update the running buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta] # return layer's tensors\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = BatchNorm1d(100)\n",
    "x = torch.randn(32, 100)           # Batch size 32, 100 features (batch size 32 of 100-dimensional vectors)\n",
    "x = module(x)                      # Forward pass\n",
    "print(\"\\nBatch Normalization down the columns\")\n",
    "print(x[:,0].mean(), x[:,0].std()) # mean and standard deviation of the 1st feature across all batch inputs (columns) \n",
    "print(\"\\nBatch Normalization across the rows\")\n",
    "print(x[0,:].mean(), x[0,:].std(),'\\n') # mean and standard deviation of a single input (1st one) from the batch (rows)\n",
    "print(x[:5,0])                     # See how the feature indicates the normalization feature-wise across the batch, not sample-wise across the features\n",
    "print(x.shape)                     # Output shape should is the same as input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmean = x.mean(1, keepdim=True) # Layer mean\n",
    "xvar = x.var(1, keepdim=True)   # Layer variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "To implement `LayerNorm`, we need to normalize the samples across the features (rows). So we change the dimension of the mean and standard deviation for `xmean` and `xvar` respectively in `batchNorm` above:\n",
    "```\n",
    "xmean = x.mean(1, keepdim=True) # Layer mean\n",
    "xvar = x.var(1, keepdim=True)   # Layer variance\n",
    "```\n",
    "Because the calculations inside `LayerNorm` don't span across the batch dimension, we can remove the buffers `running_mean` and `running_var`. There also is no distinction between training and eval mode anymore. We can remove the `train` parameter. The training process will determine the deviation from the unit gaussian distribution as seen fit by the optimizer.<br>\n",
    "With the changes in place, we can now add `LayerNorm` to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:32.166044Z",
     "iopub.status.busy": "2024-06-12T15:53:32.165724Z",
     "iopub.status.idle": "2024-06-12T15:53:32.178377Z",
     "shell.execute_reply": "2024-06-12T15:53:32.177434Z",
     "shell.execute_reply.started": "2024-06-12T15:53:32.166018Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([32, 100])\n",
      "\n",
      "Layer Normalization down the columns\n",
      "tensor(0.1469) tensor(0.8803)\n",
      "\n",
      "Layer Normalization across the rows\n",
      "tensor(-9.5367e-09) tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        xmean = x.mean(1, keepdim=True) # batch mean\n",
    "        xvar = x.var(1, keepdim=True) # batch variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "print(\"x:\", x.shape)\n",
    "print(\"\\nLayer Normalization down the columns\")\n",
    "print(x[:,0].mean(), x[:,0].std()) # mean,std of one feature across all batch inputs\n",
    "print(\"\\nLayer Normalization across the rows\")\n",
    "print(x[0,:].mean(), x[0,:].std()) # mean,std of a single input from the batch, of its features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Note that we deviate from the original paper** in our implementation of `LayerNorm` in our network. <br>In the \"attention is all you need\" paper, `Add+Norm` is applied after the transformation (**FFN, MHA**). However recently, it is a bit more common to apply the `LayerNorm` before the transformation, instead of after. This is because the `LayerNorm` is more expensive than the transformation. We want to apply the `LayerNorm` as early as possible, so that <u>we can skip it in the residual connection if the transformation is skipped.</u> This is called **\"pre-norm\" formulation.**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "give two three sentences per key point explaining the point as per the excerpt and knowledge\n",
    "give two three sentences per key point explaining the point as per the excerpt and knowledge\n",
    "<a id='306'></a>\n",
    "## 3.6. Scaling Up the Model\n",
    "-----\n",
    "<a id='c4'></a>\n",
    "Let's add a **dropout** layer right before the residual connection. Dropout is a regularization technique for neural networks that drops a unit (along with connections) at training time with a specified probability $p$ (a common value is $p$ = 0.5). At test time, all units are present, but with weights scaled by $p$ (i.e. $w$ becomes $pw$).\n",
    "\n",
    "The idea is to prevent co-adaptation, where the neural network becomes too reliant on particular connections, as this could be symptomatic of overfitting. Intuitively, dropout can be thought of as creating an implicit ensemble of sub-neural networks.\n",
    "\n",
    "<a id='a8'></a>\n",
    "![dropout](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_6.19.24_PM.png)\n",
    "\n",
    "**Figure 8: Dropout.** ([Source](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_6.19.24_PM.png))<br><br>\n",
    "\n",
    "\n",
    "Now we have a pretty complete transformer, but it is a **decoder-only transformer** unlike the one in the Attention paper. It is a decoder-only transformer because we implemented triangular masking on our affinity weights prior to applying softmax on them to ensure auto-regressive property. Also, our transformer block has only self-attention and no cross-attention.\n",
    "\n",
    "Let's increase some of the hyperparameters since we're running the model on a GPU.\n",
    "\n",
    "```\n",
    "# hyperparameters\n",
    "batch_size = 64           # how many independent sequences will we process in parallel?\n",
    "block_size = 256          # what is the maximum context length for predictions?\n",
    "max_iters = 5000      \n",
    "eval_interval = 500   \n",
    "learning_rate = 3e-4 #1e-3  #1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384              # (n_head * batch_size)\n",
    "n_head = 6                # (n_embd/batch_size)\n",
    "n_layer = 6 \n",
    "dropout = 0.2 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='307'></a>\n",
    "## 3.7. Putting It All Together\n",
    "-----\n",
    "Let's put all the code together to run as one cohesive transformer language model (GPT). The validation loss reduces from $\\boldsymbol{2.5903}$ in the **simple baseline bigram** language model (LM) to $\\boldsymbol{1.4856}$ in the **GPT** language model. The generated text is now fully structured as a Shakespearan work (script dialogue format with text broken up into different character's dialogue parts). Most of the words and punctuations are consistent with proper English. There are still some nonsensical words and phrases but overall it's better than the generated text from the simple bigram LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64           # increased from 16 to process more data in parallel\n",
    "block_size = 256         # increased back to 256 for longer context\n",
    "max_iters = 3000        # moderate increase from 2000 for better training\n",
    "eval_interval = 200     # kept same\n",
    "learning_rate = 3e-4    # kept same for stable training\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 192            # increased from 128 for more expressiveness \n",
    "n_head = 6              # increased from 4 for better attention\n",
    "n_layer = 6             # increased from 4 for deeper model\n",
    "dropout = 0.2           # kept same to maintain regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE CLEARING CACHE ===\n",
      "Memory allocated: 75.5 MB\n",
      "Memory cached: 502.0 MB\n",
      "\n",
      "=== AFTER CLEARING CACHE ===\n",
      "Memory allocated: 58.1 MB\n",
      "Memory cached: 260.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU cache before training\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(\"=== BEFORE CLEARING CACHE ===\")\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.1f} MB\")\n",
    "print(f\"Memory cached: {torch.cuda.memory_reserved(0) / 1024**2:.1f} MB\")\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n=== AFTER CLEARING CACHE ===\")\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.1f} MB\")\n",
    "print(f\"Memory cached: {torch.cuda.memory_reserved(0) / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)   # randomly prevent some of the nodes from communication\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T15:53:32.179785Z",
     "iopub.status.busy": "2024-06-12T15:53:32.179476Z",
     "iopub.status.idle": "2024-06-12T15:53:32.189914Z",
     "shell.execute_reply": "2024-06-12T15:53:32.189104Z",
     "shell.execute_reply.started": "2024-06-12T15:53:32.179756Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU...\n",
      "\n",
      "2.740289 M parameters\n",
      "step 0: train loss 4.3293, val loss 4.3350\n",
      "step 200: train loss 2.4767, val loss 2.4911\n",
      "step 400: train loss 2.4008, val loss 2.4196\n",
      "step 600: train loss 2.2327, val loss 2.2623\n",
      "step 800: train loss 2.0836, val loss 2.1390\n",
      "step 1000: train loss 1.9705, val loss 2.0530\n",
      "step 1200: train loss 1.8641, val loss 1.9817\n",
      "step 1400: train loss 1.7736, val loss 1.9090\n",
      "step 1600: train loss 1.7165, val loss 1.8648\n",
      "step 1800: train loss 1.6546, val loss 1.8098\n",
      "step 2000: train loss 1.6098, val loss 1.7797\n",
      "step 2200: train loss 1.5782, val loss 1.7548\n",
      "step 2400: train loss 1.5421, val loss 1.7266\n",
      "step 2600: train loss 1.5148, val loss 1.7011\n",
      "step 2800: train loss 1.4894, val loss 1.6886\n",
      "step 2999: train loss 1.4668, val loss 1.6605\n",
      "\n",
      "Wheth house let of I go, well wifect.\n",
      "\n",
      "Prit, shrich you, and away to strup with land!\n",
      "\n",
      "RICHARS:\n",
      "Whance you days resconques of thy cromter word,\n",
      "And, love casse to fly my face,\n",
      "That my father's come thee were proun seecul.\n",
      "Thinks in my loud their juneubles my groust;\n",
      "Tumblies their toogRBom ruing teempules of for\n",
      "With malw. Your got winds the rease sland the too.\n",
      "\n",
      "Szoungman:\n",
      "Tut he exease other not made to crrivens in coutize\n",
      "Good you our wright, per it chereite: hell be firtund,\n",
      "And not vow it that the act so:\n",
      "Tis I new--wamb: litering; our seeming,\n",
      "Or discragame at Macts ifforgerty.\n",
      "\n",
      "KING ELIV:\n",
      "More were but not was prople grown malt of\n",
      "ane rebthalles, where stays but bear any an ream; 'tis last:\n",
      "I to was frande itus by poor a is ploul and him.\n",
      "But troubdes, is the haste you this; and not where:\n",
      "You from you, time thy fit the prounishess courful\n",
      "Or busospor, would war within were a shn\n",
      "secry for you have; 'fight 'tiss to fin'd fulish deel?\n",
      "\n",
      "WARICHUS IIV:\n",
      "What. Is dis here no reirs.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "I so, undery, thou art it soldien drange as tonume:\n",
      "And morrow men I will his him pit, my tears\n",
      "That most the it than them our corgue.\n",
      "\n",
      "LUCEONIUS:\n",
      "We min men.\n",
      "\n",
      "BESHOpiniagus:\n",
      "All time own\n",
      "Not not nor yoursellow; laying, and speak the claive in\n",
      "now fair inizon bate eyess camxert! if not! a spain\n",
      "ra s shall betworn one the old strength. Let if honouse tear\n",
      "Of Endward and faace's her shall sairiour,\n",
      "And the Claugets truber dood, me and head\n",
      "recutursed and conform art,\n",
      "And gold reades: on'er oanish,\n",
      "But not so you divistir his our word,\n",
      "Ere that a way?\n",
      "O hold for, most to thing, the morer little the nurth\n",
      "is come his. O, will sighalt---broy winks andy:\n",
      "And peaks, now? what we defeass you our twat now:\n",
      "Away, sube Por trentius spiraton.\n",
      "Clad, merr I glord, dividang some, here,\n",
      "Hones conter rudes statiton, tur\n",
      "As arms?\n",
      "\n",
      "QUEERSA:\n",
      "He's you chare your one compermon more, that to Them fuls;\n",
      "It sapet meries.\n",
      "\n",
      "HANTIS:\n",
      "I am you at den.\n",
      "\n",
      "LUCHIO:\n",
      "I che I medio retine of baute,\n",
      "And that caught, it my brame on\n",
      "Witing hank tid knot, and subs so, lettlemon no.\n",
      "\n",
      "QUCEEN ELIZABETH:\n",
      "What pretiosed, yet right, yet my sounce are\n",
      "That thortery service Edward to songuin. Barke, horth\n",
      "A daugge his con thee, son, left iboun mean!--\n",
      "\n",
      "Third grave:\n",
      "Or if less ands thou his statution be one my do oundendly\n",
      "In his sub? a my tong life, your Lord Bickoward,\n",
      "Are readfit unto the kin the caple the tto as tearms\n",
      "have were as suchster-hooding with hence.\n",
      "\n",
      "COMILLEO:\n",
      "Lent you?\n",
      "\n",
      "Servan thest from upor Aumer whoper itked,\n",
      "And I is show ingot they Vouch\n",
      "Praventor:\n",
      "It ase itseend in with we or foul a s reposion their\n",
      "fr yram lowhere\n",
      "of you the repove mork will things fare.\n",
      "Nor you pusiton the audoun her withousable flece.\n",
      "\n",
      "KING HENRY III:\n",
      "'But, whe is news save will awhat.\n",
      "\n",
      "HARTINGS:\n",
      "Ux with sit contal owh the win straigl?\n",
      "Be the duk thy hour fiends hars; affer!\n",
      "You too? metch wofuld wildre most tain;\n",
      "And therear of thee spukes a our is,\n",
      "Firstand hed in lattly toger how have a pitpeak,\n",
      "For 'greal to his trun ress man.\n",
      "\n",
      "First Sercord Captarue:\n",
      "Most caully have go that thou comestle: Hor\n",
      "ly meare: who have disch'd dion suchall, aminded feer\n",
      "Drences not weren one ply of his sown.\n",
      "\n",
      "ELTHORUCHIOLADIUS:\n",
      "That good ager thun so mover, I'll has I cold?\n",
      "\n",
      "Nourses:\n",
      "Think, me I cannot, you right.\n",
      "\n",
      "DUCHESS OF York My BERCKJOHNS:?\n",
      "\n",
      "KING MONGS:\n",
      "Man:\n",
      "Suppt to Clarence, which been it botherit touch,\n",
      "Where that our but wile comfortune; well itle I cage!\n",
      "If sill, my could tad it it you there,\n",
      "Orrainy!\n",
      "\n",
      "ESCAMULILLO:\n",
      "But and teate I\n",
      "To the litter.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Our them sweell neque, till put\n",
      "My loverien forther temas,\n",
      "Undell fack antwights, to honourse of the rab;\n",
      "And I hatche asise it to apporniobe. Will,\n",
      "'Tis shaling to thou her eneme his to more here;\n",
      "I murnry upon our amouran plase mimm,\n",
      "Tis my him tear, hastese grieves:\n",
      "Let Carece, which! do sirstir, I suns.\n",
      "\n",
      "Nurse:\n",
      "Cwrow tomaid I: Get is no\n",
      "alle a toged crue to give dise, aside thee prince and now\n",
      "DUp with in the dead-weaed love.\n",
      "Is the sound have's that them\n",
      "Thou it, you. Marry your with, those or tears on ribly tild\n",
      "Wart uponcorteran war, lordes even in my lord,\n",
      "Un pay most as than; but you smy dish's disgue.\n",
      "I'll us brishre enpecrous one it than tau hust seveng of you\n",
      "Jou, and fear this duit remy. Let sak you lites\n",
      "To the nank to is righard of thou, kneight, comet\n",
      "But.\n",
      "\n",
      "\n",
      "BUCKINGHARD:\n",
      "Nor nobble Cutiolatous! why have eyeldings!\n",
      "\n",
      "WARFICW:\n",
      "Arh, hearr, I be, my his noble Camis acter\n",
      "Of arend to Not ampe merness must.\n",
      "\n",
      "COWISHABELLAND:\n",
      "Ind all with and thee; or in Cly\n",
      "But tongue follow'd what aitome fave your moust.\n",
      "I pracele.\n",
      "\n",
      "CORIOLANUS:\n",
      "What like is is pleace of your songes; his no saing;\n",
      "Take I phampisoncel with betteen the here;\n",
      "When if thee seight, of nor erwards catell--boy,\n",
      "Or judgher give an\n",
      "Your that you sighn's rusterfid; choul. There say you,--\n",
      "\n",
      "Shithal stop for this: you she that's not this of their carry\n",
      "Deside to-d shrink, lord; were with rendece:\n",
      "He learing the stays, that I deeks they heards;\n",
      "When you di\n"
     ]
    }
   ],
   "source": [
    "# Check if we are using GPU or CPU\n",
    "device_type = 'GPU' if torch.cuda.is_available() else 'CPU'\n",
    "print(f'Training on {device_type}...\\n')\n",
    "torch.manual_seed(1337)\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=5000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = [4.3293, 2.4767, 2.4008, 2.2327, 2.0836, 1.9705, 1.8641, 1.7736, 1.7165, 1.6546, 1.6098, 1.5782, 1.5421, 1.5148, 1.4894, 1.4668]\n",
    "val_loss = [4.3350, 2.4911, 2.4196, 2.2623, 2.1390, 2.0530, 1.9817, 1.9090, 1.8648, 1.8098, 1.7797, 1.7548, 1.7266, 1.7011, 1.6886, 1.6605]\n",
    "steps = [0, 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000, 2200, 2400, 2600, 2800, 2999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a figure and axes\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the training and validation losses\n",
    "ax.plot(steps, train_loss, label='Training Loss')\n",
    "ax.plot(steps, val_loss, label='Validation Loss')\n",
    "\n",
    "# Set the x-axis and y-axis labels\n",
    "ax.set_xlabel('Iterations', fontsize=14)\n",
    "ax.set_ylabel('Loss', fontsize=14)\n",
    "\n",
    "# Set the x-axis ticks and labels\n",
    "ax.set_xticks(steps)\n",
    "ax.set_xticklabels(steps, rotation=45, fontsize=12)\n",
    "\n",
    "# Set the y-axis tick format\n",
    "ax.yaxis.set_major_formatter(plt.FormatStrFormatter('%.1f'))\n",
    "\n",
    "# Set the title\n",
    "ax.set_title('Training and Validation Losses', fontsize=16)\n",
    "\n",
    "# Add a legend\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(linestyle='--', alpha=0.5)\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='c5'></a>\n",
    "#### Saving and Loading the Model Weights\n",
    "----\n",
    "\n",
    "PyTorch models store the learned parameters in an internal state dictionary, called `state_dict`. These can be persisted via the `torch.save` method. To load model weights, you need to create an instance of the same model first, and then load the parameters using `load_state_dict()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTLanguageModel(\n",
       "  (token_embedding_table): Embedding(65, 192)\n",
       "  (position_embedding_table): Embedding(256, 192)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=192, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model weights\n",
    "torch.save(model.state_dict(), 'model_5000.pth')\n",
    "\n",
    "# Load the model architecture\n",
    "model = GPTLanguageModel()\n",
    "model.load_state_dict(torch.load(\"model_5000.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T16:25:33.450356Z",
     "iopub.status.busy": "2024-06-12T16:25:33.450105Z",
     "iopub.status.idle": "2024-06-12T16:25:33.776363Z",
     "shell.execute_reply": "2024-06-12T16:25:33.775456Z",
     "shell.execute_reply.started": "2024-06-12T16:25:33.450334Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTLanguageModel(\n",
       "  (token_embedding_table): Embedding(65, 192)\n",
       "  (position_embedding_table): Embedding(256, 192)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=192, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=192, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the entire model (including architecture and weights)\n",
    "torch.save(model, 'model.pth')\n",
    "\n",
    "# Load the entire model\n",
    "# Load the entire model with weights_only=False\n",
    "model = torch.load('model.pth', weights_only=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and Loading the Full Model (Weights + Shapes)\n",
    "----\n",
    "\n",
    "When loading model weights, we needed to instantiate the model class first, because the class defines the structure of a network. We might want to save the structure of this class together with the model, in which case we can pass `model` (and not `model.state_dict()`) to the saving function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "<p>be sure to call <code>model.eval()</code> method before inferencing to set the dropout and batch normalization layers to evaluation mode. Failing to do this will yield inconsistent inference results.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='308'></a>\n",
    "## 3.8. Encoder vs Decoder vs Encoder-Decoder Transformers\n",
    "-----\n",
    "So far we've implemented a decoder-only transformer similar to the Generative Pre-trained Transformer (GPT) model but different from the full transformer architecture that includes an encoder and cross-attention blocks. Our model only has self-attention and feed-forward layers, using a triangular mask for autoregressive text generation. This decoder-only setup utilizes a triangular mask for self-attention, enabling autoregressive text generation capabilities. This setup is suitable for unconditioned text generation, like GPT as it can be used to predict the next word in a sequence, and for language modeling, where the input sequence is not needed. This is useful for tasks like language translation, text summarization, and chatbots. The decoder-only transformer is trained on a large corpus of text, such as the internet, and can generate text that is similar in style and structure to the training data.\n",
    "\n",
    "In contrast, the original Transformer paper introduced an encoder-decoder architecture specifically designed for machine translation tasks, where the encoder encodes/processes the input sequence/tokens (e.g., French), and the decoder generates the output sequence/tokens (e.g., English translation) while attending to the encoded input representations with cross-attention linking the two.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='309'></a>\n",
    "## 3.9. Quick Walkthrough of `nanoGPT`\n",
    "-----\n",
    "The `nanoGPT` implementation available on [Karpathy's GitHub](https://github.com/karpathy/nanoGPT) closely follows the decoder-only transformer we've covered so far with some minor differences. It consists of two key files: `train.py` for training and `model.py` for the model. The training script handles more complex tasks like checkpointing, learning rate decay, and distributed training. The model file closely resembles our implementation but **optimizes multi-headed attention for efficiency.** In summary, the code includes the transformer model, the training loop, and the data loading code, and can be used to train a chatGPT model. However, training large language models like GPT-3 involves a two-stage process: **pre-training and fine-tuning.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='310'></a>\n",
    "## 3.10. **ChatGPT, GPT-3:** pretraining vs. finetuning, **RLHF**\n",
    "-----\n",
    "\n",
    "#### Pre-training stage of training a chatGPT/GPT-3 model\n",
    "Pre-training involves training a massive decoder-only transformer on a vast corpus of internet data, with models like GPT-3 utilizing up to 175 billion parameters and being trained on 300 billion tokens. The pre-training stage of training a chatGPT model involves training a large model on a large corpus of text, such as the internet. This stage is used to train the model to predict the next word in a sequence. \n",
    "\n",
    "#### Fine-tuning stage involves aligning the model to be an assistant\n",
    "The fine-tuning stage is crucial for aligning the pre-trained model for specific tasks, such as question-answering, as demonstrated by ChatGPT. For ChatGPT, fine-tuning involves multiple steps, including supervised fine-tuning on question-answer pairs, training a reward model to evaluate response quality, and using reinforcement learning (PPO) to align the model's responses to score highly on the reward model. \n",
    "\n",
    "\n",
    "The fine-tuning stage involves aligning the model to be an assistant by collecting training data that looks like what an assistant would do. This data includes questions and answers, and is used to train the model to respond to specific questions and tasks. The fine-tuning stage also involves training a reward model and using **PPO (Proximal Policy Optimization)** to align the model with respect to the reward model.\n",
    "\n",
    "#### From Pre-training to Fine-tuning for ChatGPT\n",
    "Training ChatGPT involves two main stages:\n",
    "\n",
    "1. <u>Pre-training:</u> Similar to our work but on a much larger scale, using massive datasets and computational resources.\n",
    "2. <u>Fine-tuning:</u> Aligns the model to be a helpful assistant. This involves collecting specific question-answer training data, training a reward model based on human preferences, and using reinforcement learning to fine-tune the model's responses. This process where human preferences are used to guide the training of a model is known as **RLHF (Reinforcement Learning from Human Feedback).**<br><br>\n",
    "\n",
    ">#### Reinforcement Learning from Human Feedback (RLHF)\n",
    "RLHF is a process where human preferences are used to guide the training of a model.<br> It involves:\n",
    ">* <u>Collecting Human Feedback:</u> Human evaluators rank different model outputs to create a dataset of preferences.\n",
    ">* <u>Training a Reward Model:</u> This model learns to predict the quality of responses based on the human feedback.\n",
    ">* <u>Reinforcement Learning:</u> The main model is fine-tuned using a reinforcement learning algorithm, such as Proximal Policy Optimization (PPO), to maximize the predicted reward from the reward model.<br>\n",
    ">This approach helps align the model's outputs with human expectations and improves the overall quality and relevance of its responses.\n",
    "\n",
    ">#### Proximal Policy Optimization (PPO)\n",
    ">PPO stands for Proximal Policy Optimization, which is a popular reinforcement learning algorithm used in deep learning. It was developed by OpenAI and published in a research paper in 2017.\n",
    "PPO is a model-free, on-policy algorithm that aims to optimize a policy (a mapping from states to actions) to maximize a cumulative reward signal. It's designed to be more stable and reliable than other reinforcement learning algorithms, such as Q-learning or policy gradient methods.\n",
    "The key features of PPO are:\n",
    ">1. Proximal: PPO tries to stay close to the current policy, rather than making large updates that might lead to instability.\n",
    ">2. Policy Optimization: PPO optimizes the policy directly, rather than learning a value function (like Q-learning).\n",
    ">3. On-policy: PPO learns from experiences gathered without exploration noise, which makes it more efficient than off-policy methods.<br>\n",
    "PPO has been successfully applied to various tasks, including:\n",
    ">* Continuous control problems (e.g., robotics, autonomous driving)\n",
    ">* Discrete action spaces (e.g., game playing, recommendation systems)\n",
    ">* Multi-agent environments\n",
    "\n",
    "\n",
    "PPO is used to fine-tune the chatGPT model to align it with a reward model, which is trained to predict the utility or quality of the output. This process helps the model generate more informative and relevant responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "<br><br><a id=\"4\"></a>\n",
    "# 4. Conclusion\n",
    "-----------\n",
    "\n",
    "We trained a small decoder-only transformer inspired by GPT, demonstrating its capability on a tiny Shakespeare dataset. While our model is much smaller, the principles are the same as those used in large-scale models like GPT-3. Furthermore, while the tutorial focused on the pre-training stage, it did not cover the fine-tuning stages required for task-specific or aligned models like ChatGPT. For practical applications beyond text generation, further fine-tuning stages are necessary.\n",
    "\n",
    "### Next Steps\n",
    "* Further fine-tuning stages for specific tasks or alignment\n",
    ">Further fine-tuning stages can be used to adapt the chat GPT model to specific tasks or alignment goals. For example, the model can be fine-tuned for sentiment analysis, question answering, or text classification. These fine-tuning stages can be done using supervised learning methods, such as labeled data, or reinforcement learning methods, such as reward models.\n",
    "* Simple supervised fine-tuning or more fancy methods like training a reward model and using PPO\n",
    ">Simple supervised fine-tuning involves training the model on labeled data to adapt it to a specific task. More advanced methods, such as training a reward model and using PPO, can be used to fine-tune the model for more complex tasks or alignment goals. These methods involve training a separate model to predict the reward or utility of the output, and using this reward model to guide the fine-tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "<br><br><a id=\"r1\"></a>\n",
    "# References\n",
    "----\n",
    "1. \"<u>Let's build GPT: from scratch, in code, spelled out.</u>\" [youtube video](https://www.youtube.com/watch?v=kCc8FmEb1nY), Jan 2023.\n",
    "2. Andrej Karpathy **GPT** [github repo](https://github.com/karpathy/ng-video-lecture/tree/master).\n",
    "3. Twitter: \"<u>Which style of drawing residual networks is semantically superior?</u>\" - Andrej Karpathy, [tweet](https://x.com/karpathy/status/1236737502200791041), Mar 2020.\n",
    "4. Article: \"<u>GPT with Andrej Karpathy</u>\" - Kavishka Abeywardana, Pt [1](https://medium.com/@kdwa2404/gpt-with-andrej-karpathy-part-1-865bec6fbcce), [2](https://medium.com/@kdwa2404/gpt-with-andrej-karpathy-part-2-f8653926272f), [3](https://medium.com/@kdwa2404/gpt-with-andrej-karpathy-part-3-a42313db1421), [4](https://medium.com/@kdwa2404/gpt-with-andrej-karpathy-part-4-319365968713), [5](https://medium.com/@kdwa2404/gpt-with-andrej-karpathy-part-5-d5c0cbfec7de), March 2024.\n",
    "5. \"<u>Deep dive into AI: Building GPT from Scratch!</u>\" - Ada Choudhry, [article](https://medium.com/@adachoudhry26/deep-dive-into-ai-building-gpt-from-scratch-aff87c804117), Apr 2024.\n",
    "6. \"<u>Residual blocks  Building blocks of ResNet</u>\" - Sabyasachi Sahoo, [blog](https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec), Nov 2018.\n",
    "7. \"<u>Sequence to Sequence (seq2seq) and Attention</u>\" - Elena Voita, [article](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html), Nov 2023.\n",
    "8. \"<u>The Annotated Transformer</u>\" - Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak, and Stella Biderman., [blog](https://nlp.seas.harvard.edu/annotated-transformer), 2022. ([Original Version](https://nlp.seas.harvard.edu/2018/04/03/attention.html): Sasha Rush, Apr 2018)\n",
    "9. \"<u>Attention Is All You Need</u>\" - Vaswani, Ashish et. al., [Academic Paper](https://arxiv.org/abs/1706.03762), _Neural Information Processing Systems (NeurIPS)_, Vol 3. Jun 2017.\n",
    "10. \"<u>Layer Normalization</u>\" - Ba, Jimmy et al., [Academic Paper](https://arxiv.org/abs/1607.06450v1), _ArXiv_. Jul 2016.\n",
    "11. \"<u>Deep Residual Learning for Image Recognition</u>\" - He, Kaiming et al., [Academic Paper](https://arxiv.org/abs/1512.03385), _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 770-778. Dec 2015.\n",
    "12. \"<u>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</u>\" - Srivastava, Nitish et al., [Academic Paper](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf), _J.Mach.Learn.Res._ Vol 15. 1929-1958. June 2014. \n",
    "13. PyTorch Resources: [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) \n",
    "14. Papers With Code: [Scaled Dot-Product Attention](https://paperswithcode.com/method/scaled), [Multi-Head Attention](https://paperswithcode.com/method/multi-head-attention), [Dropout](https://paperswithcode.com/method/dropout), [Layer Normalization](https://paperswithcode.com/method/layer-normalization), [Residual Block](https://paperswithcode.com/method/residual-block)\n",
    "------"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
